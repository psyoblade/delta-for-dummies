{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d657b2f3-464b-46c8-96af-76521af7c5ab",
   "metadata": {},
   "source": [
    "# Writing to a Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de619a5e-2b8b-4294-81b0-8ca435db8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "from delta import *\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "warehouse_dir = f\"{work_dir}/spark-warehouse\"\n",
    "\n",
    "# Create spark session with hive enabled\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"pyspark-notebook\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_dir)\n",
    "    .enableHiveSupport()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5046ad9-8423-49f0-93d6-b650da098fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 델타 레이크 생성시에 반드시 `configure_spark_with_delta_pip` 구성을 통해 실행되어야 정상적인 델타 의존성이 로딩됩니다\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a25c297-9f0c-4b52-8560-c598f181cfe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.30.99.141:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f73a9d06e20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a7aba2-cbd8-412b-ad2f-18c73531c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql(queries, num_rows=20):\n",
    "    for query in queries.split(\";\"):\n",
    "        spark.sql(query).show(num_rows, truncate=False)\n",
    "\n",
    "def ls(command):\n",
    "    !ls -al {command}\n",
    "\n",
    "def cat(filename):\n",
    "    !cat {filename}\n",
    "\n",
    "def grep(keyword, filename):\n",
    "    !grep -i {keyword} {filename}\n",
    "\n",
    "def grep_and_json(keyword, filename):\n",
    "    !grep {keyword} {filename} | python -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42922ae-d6e3-42ac-a2db-c9429d88004b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>taxidb</td><td>greentaxis</td><td>false</td></tr>\n",
       "<tr><td>taxidb</td><td>yellowtaxis</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+-----------+-----------+\n",
       "|namespace|  tableName|isTemporary|\n",
       "+---------+-----------+-----------+\n",
       "|   taxidb| greentaxis|      false|\n",
       "|   taxidb|yellowtaxis|      false|\n",
       "+---------+-----------+-----------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use taxidb\")\n",
    "spark.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5da3ae-6840-4466-90c4-f324c5516b07",
   "metadata": {},
   "source": [
    "### Cleaning Out the YellowTaxis Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c460dda8-7ec0-462d-8d9e-7a4875d5eaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(1)</th></tr>\n",
       "<tr><td>450627</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "|  450627|\n",
       "+--------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from taxidb.greentaxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e42c515-eff7-4cfb-a24a-5a1c9b582c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>AverageFare</th></tr>\n",
       "<tr><td>null</td><td>28.830541636814186</td></tr>\n",
       "<tr><td>1</td><td>13.217967034800125</td></tr>\n",
       "<tr><td>2</td><td>12.054380250700097</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+------------------+\n",
       "|VendorID|       AverageFare|\n",
       "+--------+------------------+\n",
       "|    null|28.830541636814186|\n",
       "|       1|13.217967034800125|\n",
       "|       2|12.054380250700097|\n",
       "+--------+------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT VendorID, AVG(fare_amount) AS AverageFare\n",
    "FROM taxidb.greentaxis\n",
    "GROUP BY VendorID\n",
    "HAVING AVG(fare_amount) > 10\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9939a9-d726-4a9a-9191-f3a744b3bd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 450,627\n"
     ]
    }
   ],
   "source": [
    "# Reading a Table with PySpark\n",
    "df = spark.read.format(\"delta\").table(\"taxidb.greentaxis\")\n",
    "print(f\"Number of records: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb0e6f53-dbd6-47d2-914e-10453a1d184c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=None, AverageFare=28.830541636814186),\n",
       " Row(VendorID=1, AverageFare=13.217967034800125),\n",
       " Row(VendorID=2, AverageFare=12.054380250700097)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = (\n",
    "    df.groupBy(\"VendorID\")\n",
    "    .agg(avg(\"fare_amount\").alias(\"AverageFare\"))\n",
    "    .filter(col(\"AverageFare\") > 10)\n",
    "    .sort(col(\"AverageFare\").desc())\n",
    ")\n",
    "res.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcdbc6c3-ba76-4b0a-a533-4b69c7599cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|2       |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql(\"select count(1) from taxidb.YellowTaxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31c5b804-c413-4b51-bef7-3b817ad7d47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxidb.YellowTaxis (\n",
    "    RideId INT,\n",
    "    VendorId INT,\n",
    "    PickupTime TIMESTAMP,\n",
    "    DropTime TIMESTAMP,\n",
    "    PickupLocationId INT,\n",
    "    DropLocationId INT,\n",
    "    CabNumber STRING,\n",
    "    DriverLicenseNumber STRING,\n",
    "    PassengerCount INT,\n",
    "    TripDistance DOUBLE,\n",
    "    RatecodeId INT,\n",
    "    PaymentType INT,\n",
    "    TotalAmount DOUBLE,\n",
    "    FareAmount DOUBLE,\n",
    "    Extra DOUBLE,\n",
    "    MtaTax DOUBLE,\n",
    "    TipAmount DOUBLE,\n",
    "    TollsAmount DOUBLE,\n",
    "    ImprovementSurcharge DOUBLE\n",
    ") USING DELTA\n",
    "LOCATION \"{work_dir}/data/yellowTaxis.delta\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39a6aa88-9fc1-487c-82c8-85dfc7283f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count</th></tr>\n",
       "<tr><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|count|\n",
       "+-----+\n",
       "|    2|\n",
       "+-----+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO taxidb.yellowtaxis (RideId, VendorId, PickupTime, DropTime, PickupLocationId,\n",
    "    DropLocationId, CabNumber, DriverLicenseNumber, PassengerCount, TripDistance,\n",
    "    RatecodeId, PaymentType, TotalAmount, FareAmount, Extra, \n",
    "    MtaTax, TipAmount, TollsAmount, ImprovementSurcharge)\n",
    "VALUES (9999995, 1, '2019-11-01T00:00:00.000Z', '2019-11-01T00:02:23.573Z', 65, \n",
    "    71, 'TAC304', '453987', 2, 4.5, \n",
    "    1, 1, 20.34, 15.0, 0.5, \n",
    "    0.4, 2.0, 2.0, 1.1)\n",
    "\"\"\")\n",
    "spark.sql(\"SELECT count(RideId) AS count FROM taxidb.YellowTaxis WHERE RideId = 9999995\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68b655-010f-4322-84a0-42eb56d0ed1a",
   "metadata": {},
   "source": [
    "### Appending a DataFrame to a Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d0afbcf-b1d8-4b8e-b81f-92ab74ccff73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count</th></tr>\n",
       "<tr><td>1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|count|\n",
       "+-----+\n",
       "|    1|\n",
       "+-----+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테이블 생성 즉시 경로가 생김\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxidb.YellowTaxis_append (\n",
    "    RideId INT,\n",
    "    VendorId INT,\n",
    "    PickupTime TIMESTAMP,\n",
    "    DropTime TIMESTAMP,\n",
    "    PickupLocationId INT,\n",
    "    DropLocationId INT,\n",
    "    CabNumber STRING,\n",
    "    DriverLicenseNumber STRING,\n",
    "    PassengerCount INT,\n",
    "    TripDistance DOUBLE,\n",
    "    RatecodeId INT,\n",
    "    PaymentType INT,\n",
    "    TotalAmount DOUBLE,\n",
    "    FareAmount DOUBLE,\n",
    "    Extra DOUBLE,\n",
    "    MtaTax DOUBLE,\n",
    "    TipAmount DOUBLE,\n",
    "    TollsAmount DOUBLE,\n",
    "    ImprovementSurcharge DOUBLE\n",
    ") USING DELTA\n",
    "LOCATION \"{work_dir}/data/yellowTaxis_append.delta\"\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO taxidb.yellowtaxis_append (RideId, VendorId, PickupTime, DropTime, PickupLocationId,\n",
    "    DropLocationId, CabNumber, DriverLicenseNumber, PassengerCount, TripDistance,\n",
    "    RatecodeId, PaymentType, TotalAmount, FareAmount, Extra, \n",
    "    MtaTax, TipAmount, TollsAmount, ImprovementSurcharge)\n",
    "VALUES (9999995, 2, '2019-11-01T00:00:00.000Z', '2019-11-01T00:02:23.573Z', 65, \n",
    "    71, 'TAC304', '453987', 2, 4.5, \n",
    "    1, 1, 20.34, 15.0, 0.5, \n",
    "    0.4, 2.0, 2.0, 1.1)\n",
    "\"\"\")\n",
    "spark.sql(\"SELECT count(RideId) AS count FROM taxidb.yellowtaxis_append WHERE VendorId = 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb4ff673-88e0-440e-aba2-e652abfa91ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RideId: integer (nullable = true)\n",
      " |-- VendorId: integer (nullable = true)\n",
      " |-- PickupTime: timestamp (nullable = true)\n",
      " |-- DropTime: timestamp (nullable = true)\n",
      " |-- PickupLocationId: integer (nullable = true)\n",
      " |-- DropLocationId: integer (nullable = true)\n",
      " |-- CabNumber: string (nullable = true)\n",
      " |-- DriverLicenseNumber: string (nullable = true)\n",
      " |-- PassengerCount: integer (nullable = true)\n",
      " |-- TripDistance: double (nullable = true)\n",
      " |-- RatecodeId: integer (nullable = true)\n",
      " |-- PaymentType: integer (nullable = true)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      " |-- FareAmount: double (nullable = true)\n",
      " |-- Extra: double (nullable = true)\n",
      " |-- MtaTax: double (nullable = true)\n",
      " |-- TipAmount: double (nullable = true)\n",
      " |-- TollsAmount: double (nullable = true)\n",
      " |-- ImprovementSurcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_for_append = spark.read.format(\"delta\").table(\"taxidb.yellowtaxis_append\")\n",
    "df_for_append.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6746f966-5dbd-4be3-b635-b39dc1eaffc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_append.write.mode(\"append\").format(\"delta\").save(f\"{work_dir}/data/yellowTaxis.delta\")\n",
    "spark.read.format(\"delta\").table(\"taxidb.yellowtaxis\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ddcd76e-c670-4a62-802e-31487ce31c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-----------------------+----------------+--------------+---------+-------------------+--------------+------------+----------+-----------+-----------+----------+-----+------+---------+-----------+--------------------+\n",
      "|RideId |VendorId|PickupTime         |DropTime               |PickupLocationId|DropLocationId|CabNumber|DriverLicenseNumber|PassengerCount|TripDistance|RatecodeId|PaymentType|TotalAmount|FareAmount|Extra|MtaTax|TipAmount|TollsAmount|ImprovementSurcharge|\n",
      "+-------+--------+-------------------+-----------------------+----------------+--------------+---------+-------------------+--------------+------------+----------+-----------+-----------+----------+-----+------+---------+-----------+--------------------+\n",
      "|9999995|1       |2019-11-01 09:00:00|2019-11-01 09:02:23.573|65              |71            |TAC304   |453987             |2             |4.5         |1         |1          |20.34      |15.0      |0.5  |0.4   |2.0      |2.0        |1.1                 |\n",
      "|9999995|1       |2019-11-01 09:00:00|2019-11-01 09:02:23.573|65              |71            |TAC304   |453987             |2             |4.5         |1         |1          |20.34      |15.0      |0.5  |0.4   |2.0      |2.0        |1.1                 |\n",
      "|9999995|2       |2019-11-01 09:00:00|2019-11-01 09:02:23.573|65              |71            |TAC304   |453987             |2             |4.5         |1         |1          |20.34      |15.0      |0.5  |0.4   |2.0      |2.0        |1.1                 |\n",
      "|9999995|2       |2019-11-01 09:00:00|2019-11-01 09:02:23.573|65              |71            |TAC304   |453987             |2             |4.5         |1         |1          |20.34      |15.0      |0.5  |0.4   |2.0      |2.0        |1.1                 |\n",
      "+-------+--------+-------------------+-----------------------+----------------+--------------+---------+-------------------+--------------+------------+----------+-----------+-----------+----------+-----+------+---------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql(\"select * from taxidb.yellowtaxis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d41ed3-06f7-443b-8bd9-a040c2e0cad2",
   "metadata": {},
   "source": [
    "### Using the OverWrite Mode When Writing to a Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5ab07-2417-48fc-b913-8056f3c725a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 코드는 데이터 다운로드가 필요하여 추후에 테스트 하기로 함\n",
    "\n",
    "# Appending a DataFrame to a Table @ https://drive.google.com/file/d/1--SfboYb-KyEug4U89m2pnjq0MyUWbvJ/view?usp=sharing\n",
    "yellowTaxi = spark.read.format(\"delta\").table(\"taxidb.YellowTaxis\")\n",
    "yellowTaxiSchema = yellowTaxi.schema\n",
    "print(yellowTaxiSchema)\n",
    "\n",
    "df_for_append = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .parquet(\"/mnt/datalake/book/data files/YellowTaxis_append.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b9ed57a-4319-4920-bb7d-5a934872d710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(f\"{work_dir}/data/yellowTaxis.parquet\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71357f77-5af5-4f3f-91b1-9bb88387fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|1       |2021-01-01 09:30:10 |2021-01-01 09:36:12  |1.0            |2.1          |1.0       |N                 |142         |43          |2           |8.0        |3.0  |0.5    |0.0       |0.0         |0.3                  |11.8        |2.5                 |null       |\n",
      "|1       |2021-01-01 09:51:20 |2021-01-01 09:52:19  |1.0            |0.2          |1.0       |N                 |238         |151         |2           |3.0        |0.5  |0.5    |0.0       |0.0         |0.3                  |4.3         |0.0                 |null       |\n",
      "|1       |2021-01-01 09:43:30 |2021-01-01 10:11:06  |1.0            |14.7         |1.0       |N                 |132         |165         |1           |42.0       |0.5  |0.5    |8.65      |0.0         |0.3                  |51.95       |0.0                 |null       |\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6acad0eb-e9ff-48c9-a72d-ba528181eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = df.where(expr(\"VendorID = 1 and PULocationID = 142 and DOLocationID = 43\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6268436-03b5-45d9-a8b1-0eeaf3fa015a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|1       |2021-01-01 09:30:10 |2021-01-01 09:36:12  |1.0            |2.1          |1.0       |N                 |142         |43          |2           |8.0        |3.0  |0.5    |0.0       |0.0         |0.3                  |11.8        |2.5                 |null       |\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>payment_type</th><th>count</th></tr>\n",
       "<tr><td>4</td><td>3</td></tr>\n",
       "<tr><td>0</td><td>19</td></tr>\n",
       "<tr><td>3</td><td>8</td></tr>\n",
       "<tr><td>2</td><td>143</td></tr>\n",
       "<tr><td>1</td><td>604</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+-----+\n",
       "|payment_type|count|\n",
       "+------------+-----+\n",
       "|           4|    3|\n",
       "|           0|   19|\n",
       "|           3|    8|\n",
       "|           2|  143|\n",
       "|           1|  604|\n",
       "+------------+-----+"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.show(1, truncate=False)\n",
    "filtered.groupBy(\"payment_type\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4545f6d8-a2de-439c-b29d-2de0a0608241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2021-01-11 01:25:02|  2021-01-11 01:26:18|            1.0|          0.4|       1.0|                 N|         142|          43|           3|        3.5|  3.5|    0.5|       0.0|         0.0|                  0.3|         7.8|                 2.5|       null|\n",
      "|       1| 2021-01-18 18:59:07|  2021-01-18 19:10:04|            1.0|          2.2|       1.0|                 N|         142|          43|           3|       10.5|  2.5|    0.5|       0.0|         0.0|                  0.3|        13.8|                 2.5|       null|\n",
      "|       1| 2021-01-27 23:21:12|  2021-01-27 23:24:39|            1.0|          0.6|       1.0|                 N|         142|          43|           3|        4.5|  2.5|    0.5|       0.0|         0.0|                  0.3|         7.8|                 2.5|       null|\n",
      "|       1| 2021-01-28 02:07:53|  2021-01-28 02:13:59|            1.0|          1.9|       1.0|                 N|         142|          43|           3|        7.5|  3.5|    0.5|       0.0|         0.0|                  0.3|        11.8|                 2.5|       null|\n",
      "|       1| 2021-01-28 21:40:08|  2021-01-28 21:48:42|            0.0|          2.1|       1.0|                 N|         142|          43|           3|        8.5|  2.5|    0.5|       0.0|         0.0|                  0.3|        11.8|                 2.5|       null|\n",
      "|       1| 2021-01-29 21:56:04|  2021-01-29 21:59:19|            1.0|          1.0|       1.0|                 N|         142|          43|           3|        5.0|  2.5|    0.5|       0.0|         0.0|                  0.3|         8.3|                 2.5|       null|\n",
      "|       1| 2022-01-01 10:45:01|  2022-01-01 10:50:55|            2.0|          0.8|       1.0|                 N|         142|          43|           3|        6.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         9.8|                 2.5|        0.0|\n",
      "|       1| 2022-01-11 03:04:00|  2022-01-11 03:04:41|            1.0|          0.2|       2.0|                 N|         142|          43|           3|       52.0|  7.0|    0.5|       0.0|         0.0|                  0.3|        59.8|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered.where(expr(\"payment_type = 3\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61276b40-5cef-4532-9ff7-5ca1f5b6de0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: integer (nullable = false)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withColumn = filtered.where(expr(\"payment_type = 3\")).withColumn(\"payment_type\", lit(30))\n",
    "withColumn.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd038094-a9b8-4870-b443-e9caa32ee97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2021-01-11 01:25:02|  2021-01-11 01:26:18|            1.0|          0.4|       1.0|                 N|         142|          43|          30|        3.5|  3.5|    0.5|       0.0|         0.0|                  0.3|         7.8|                 2.5|       null|\n",
      "|       1| 2021-01-18 18:59:07|  2021-01-18 19:10:04|            1.0|          2.2|       1.0|                 N|         142|          43|          30|       10.5|  2.5|    0.5|       0.0|         0.0|                  0.3|        13.8|                 2.5|       null|\n",
      "|       1| 2021-01-27 23:21:12|  2021-01-27 23:24:39|            1.0|          0.6|       1.0|                 N|         142|          43|          30|        4.5|  2.5|    0.5|       0.0|         0.0|                  0.3|         7.8|                 2.5|       null|\n",
      "|       1| 2021-01-28 02:07:53|  2021-01-28 02:13:59|            1.0|          1.9|       1.0|                 N|         142|          43|          30|        7.5|  3.5|    0.5|       0.0|         0.0|                  0.3|        11.8|                 2.5|       null|\n",
      "|       1| 2021-01-28 21:40:08|  2021-01-28 21:48:42|            0.0|          2.1|       1.0|                 N|         142|          43|          30|        8.5|  2.5|    0.5|       0.0|         0.0|                  0.3|        11.8|                 2.5|       null|\n",
      "|       1| 2021-01-29 21:56:04|  2021-01-29 21:59:19|            1.0|          1.0|       1.0|                 N|         142|          43|          30|        5.0|  2.5|    0.5|       0.0|         0.0|                  0.3|         8.3|                 2.5|       null|\n",
      "|       1| 2022-01-01 10:45:01|  2022-01-01 10:50:55|            2.0|          0.8|       1.0|                 N|         142|          43|          30|        6.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         9.8|                 2.5|        0.0|\n",
      "|       1| 2022-01-11 03:04:00|  2022-01-11 03:04:41|            1.0|          0.2|       2.0|                 N|         142|          43|          30|       52.0|  7.0|    0.5|       0.0|         0.0|                  0.3|        59.8|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withColumn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b40e4-b929-4d40-9262-8df5e946f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting Data with the SQL COPY INTO Command\n",
    "spark.sql(\"\"\"\n",
    "COPY INTO taxidb.yellowtaxis\n",
    "FROM (\n",
    "    SELECT RideId::Int\n",
    "    , VendorId::Int\n",
    "    , PickupTime::Timestamp\n",
    "    , DropTime::Timestamp\n",
    "    , PickupLocationId::Int\n",
    "    , DropLocationId::Int\n",
    "    , CabNumber::String\n",
    "    , DriverLicenseNumber::String\n",
    "    , PassengerCount::Int\n",
    "    , TripDistance::Double\n",
    "    , RateCodeId::Int\n",
    "    , PaymentType::Int\n",
    "    , TotalAmount::Double\n",
    "    , FareAmount::Double\n",
    "    , Extra::Double\n",
    "    , MtaTax::Double\n",
    "    , TipAmount::Double\n",
    "    , TollsAmount::Double\n",
    "    , ImprovementSurcharge::Double\n",
    ") FROM '/mnt/datalake/book/DataFiles/YellowTaxisLargeAppend.csv'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS (\"header\" = \"true\")\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) FROM taxidb.YellowTaxis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db86d60e-f179-4634-ab66-e1a1eb28961b",
   "metadata": {},
   "source": [
    "### Partitions\n",
    "> These use cases lend themselves well to partitioning. Partitioning your data to align\n",
    "with your query patterns can dramatically speed up query performance, especially\n",
    "when combined with other performance optimizations, such as Z-ordering.7 A Delta\n",
    "table partition is composed of a folder with a subset of data rows that share the same\n",
    "value for one or more column(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efdde85-2161-4721-b30d-cb4f8059af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파티션 구성된 델타 테이블을 사전에 생성해 두고\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE taxidb.YellowTaxisPartitioned\n",
    "    RideId INT,\n",
    "    VendorId INT,\n",
    "    PickupTime TIMESTAMP,\n",
    "    DropTime TIMESTAMP,\n",
    "    PickupLocationId INT,\n",
    "    DropLocationId INT,\n",
    "    CabNumber STRING,\n",
    "    DriverLicenseNumber STRING,\n",
    "    PassengerCount INT,\n",
    "    TripDistance DOUBLE,\n",
    "    RatecodeId INT,\n",
    "    PaymentType INT,\n",
    "    TotalAmount DOUBLE,\n",
    "    FareAmount DOUBLE,\n",
    "    Extra DOUBLE,\n",
    "    MtaTax DOUBLE,\n",
    "    TipAmount DOUBLE,\n",
    "    TollsAmount DOUBLE,\n",
    "    ImprovementSurcharge DOUBLE\n",
    ") USING DELTA\n",
    "PARTITIONED BY(VendorId)\n",
    "LOCATION \"/mnt/datalake/book/chapter03/YellowTaxisDeltaPartitioned\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229176a8-cfae-4cbf-9a4a-e99ee98bbd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존에 생성되어 있는 관리 테이블을 읽어서\n",
    "df = spark.read.format(\"delta\").table(\"taxidb.YellowTaxis\")\n",
    "# 델타 테이블에 덮어쓰기를 한다\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/datalake/book/chapter03/YellowTaxisDeltaPartitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9ffd7-0c08-4dac-91c4-d53af24266a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT DISTINCT(VendorId) FROM taxidb.YellowTaxisPartitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc10bcf-0790-45b5-9e4a-3866ce7b3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning by multiple columns -- Partition by VendorId AND rateCodeId\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE taxidb.YellowTaxisPartitioned\n",
    "    RideId INT,\n",
    "    VendorId INT,\n",
    "    PickupTime TIMESTAMP,\n",
    "    DropTime TIMESTAMP,\n",
    "    PickupLocationId INT,\n",
    "    DropLocationId INT,\n",
    "    CabNumber STRING,\n",
    "    DriverLicenseNumber STRING,\n",
    "    PassengerCount INT,\n",
    "    TripDistance DOUBLE,\n",
    "    RatecodeId INT,\n",
    "    PaymentType INT,\n",
    "    TotalAmount DOUBLE,\n",
    "    FareAmount DOUBLE,\n",
    "    Extra DOUBLE,\n",
    "    MtaTax DOUBLE,\n",
    "    TipAmount DOUBLE,\n",
    "    TollsAmount DOUBLE,\n",
    "    ImprovementSurcharge DOUBLE\n",
    ") USING DELTA\n",
    "PARTITIONED BY(VendorId, RatecodeId)\n",
    "LOCATION \"/mnt/datalake/book/chapter03/YellowTaxisDeltaPartitioned\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e89b85-46ca-4891-8f89-8aa4d61164ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) > 0 AS `Partition exists` FROM taxidb.YellowTaxisPartitioned WHERE VendorId = 2 AND RateCodeId = 99\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3eb239-5eb9-4bc2-ad51-cd55cea951dd",
   "metadata": {},
   "source": [
    "### Selectively updating Delta partitions with replaceWhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4175d44-b23f-440a-9f0b-4d112c600011",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT RideId, VendorId, PaymentType FROM taxidb.yellowtaxispartitioned WHERE VendorID = 1 AND RatecodeId = 99 LIMIT 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f001b5d6-775c-43d2-9baf-6fbc35dbd4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s assume that we have a business reason that states that all PaymentTypes for\n",
    "# VendorId = 1 and RatecodeId = 9 should be 3. We can use the following PySpark\n",
    "# expression with replaceWhere to achieve that result\n",
    "\n",
    "# 임의의 조건에 해당하는 데이터 집합을 사전에 생성해 두고 (PaymentType 컬럼은 기 존재하는 컬럼)\n",
    "from pyspark.sql.functions import *\n",
    "businessCondition = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"/mnt/datalake/book/chapter03/YellowTaxisDeltaPartitioned\")\n",
    "    .where((col(\"VendorId\") == 1) & (col(\"RatecodeId\") == 99))\n",
    "    .withColumn(\"PaymentType\", lit(3))\n",
    ")\n",
    "\n",
    "# 해당 데이터프레임 기준으로 존재하는 컬럼에 교체하는 연산을 수행\n",
    "replaceWhere = (\n",
    "    businessCondition.write.format(\"delta\")\n",
    "    .option(\"replaceWhere\", \"VendorId = 1 AND RateCodeId = 99\")\n",
    "    .mode(\"overwrite\")\n",
    ")\n",
    "replaceWhere.save(\"/mnt/datalake/book/chapter03/YellowTaxisDeltaPartitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f89929-381b-4af0-8744-e2406e887a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT(PaymentType)\n",
    "FROM taxidb.yellowtaxispartitioned\n",
    "WHERE VendorID = 1 AND RatecodeId = 99\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb9e48-6954-4b50-a2aa-d124594d73ea",
   "metadata": {},
   "source": [
    "### User-Defined Metadata\n",
    "> Let’s look at the SparkSession configuration first. Assume that we have an INSERT\n",
    "operation, to which we want to assign a GDPR tag for auditing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc35957-30be-4a7e-8e23-0e16c1eeff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SparkSession to Set Custom Metadata\n",
    "spark.sql(f\"\"\"\n",
    "SET spark.databricks.delta.commitInfo.userMetadata=my-custom-metadata= { \"GDPR\": \"INSERT Request 1x965383\" }\n",
    "\"\"\")\n",
    "\n",
    "# The GDPR tag will automatically be applied to the commit info in the transaction log.\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO taxidb.yellowtaxisPartitioned (RideId, VendorId, PickupTime, DropTime,\n",
    "    PickupLocationId, DropLocationId, CabNumber,\n",
    "    DriverLicenseNumber, PassengerCount, TripDistance,\n",
    "    RatecodeId, PaymentType, TotalAmount,\n",
    "    FareAmount, Extra, MtaTax, TipAmount,\n",
    "    TollsAmount, ImprovementSurcharge)\n",
    "VALUES(10000000, 3, '2019-11-01T00:00:00.000Z',\n",
    "    '2019-11-01T00:02:23.573Z', 65, 71, 'TAC304',\n",
    "    '453987', 2, 4.5, 1, 1, 20.34, 15.0, 0.5,\n",
    "    0.4, 2.0, 2.0, 1.1)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e260aa-f073-491e-8d1d-54aaa0f7764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the DataFrameWriter to Set Custom Metadata\n",
    "customMetadata = df.write.mode(\"append\").format(\"delta\").option(\"userMetadata\", '{\"PII\": \"Confidential XYZ\"}')\n",
    "customMetadata.save(\"/mnt/datalake/book/chapter03/YellowTaxisDeltaPartitioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96346b53-951b-47ef-bcff-ab28807c8695",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "> 기본적인 연산 수행 시에, 다양한 방식(SQL DDL/DML, DataFrameWriter API, DeltaTable Builder API 등)으로 수행이 가능하며\n",
    "테이블 생성 시에도 경로를 직접 명시하여 비관리 테이블로 사용할 수도 있고, 테이블 수준으로 관리 테이블로 활용이 가능하다\n",
    "파티셔닝을 통해 관리 효율성 및 성능 향상을 기대할 수 있으며, replaceWhere 같은 구문을 통해서 특정 파티션 혹은 조건의 데이터를 효과적으로 업데이트 할 수 있다\n",
    "또한 이용자 정의 메타데이터를 활용하여 특정 태그를 가진 statements 혹은 operations 목록을 확인할 수 있고, 또한 감사 혹은 규제를 목적으로 활용할 수도 있다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc98ed-804e-4aa5-9145-260d31c9fa9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
