{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16654264-22b9-4d66-bfdd-7b17357e28b7",
   "metadata": {},
   "source": [
    "## 5-3. Liquid Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9879ddf1-1276-49d0-89b8-eec2b8df8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-11-openjdk-amd64\n",
      "/usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986ad10a-b2b6-4543-812e-b0f09f85b72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "from delta import *\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "warehouse_dir = f\"{work_dir}/spark-warehouse\"\n",
    "\n",
    "# Create spark session with hive enabled\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"pyspark-notebook\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_dir)\n",
    "    .enableHiveSupport()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5b931d-1ee5-4394-960f-33f131d4d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 델타 레이크 생성시에 반드시 `configure_spark_with_delta_pip` 구성을 통해 실행되어야 정상적인 델타 의존성이 로딩됩니다\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39abe029-89d4-473d-9f8c-e44146492879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.31.95.65:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbe05a22340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark.conf.set(\"spark.sql.decimalOperations.allowPrecisionLoss\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57b61f7-1ee2-4a9a-bb31-3663bca1953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(queries, num_rows = 20):\n",
    "    for query in queries.split(\";\"):\n",
    "        spark.sql(query).show(num_rows, truncate=False)\n",
    "\n",
    "def sql(query):\n",
    "    return spark.sql(query)\n",
    "\n",
    "def history(dbName, tableName):\n",
    "    return spark.sql(\"describe history {}.{}\".format(dbName, tableName))\n",
    "\n",
    "def table(dbName, tableName):\n",
    "    return spark.read.format(\"delta\").table(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "def describe(dbName, tableName, extended = True, num_rows = 20):\n",
    "    if extended:\n",
    "        show(\"describe extended {}.{}\".format(dbName, tableName), num_rows)\n",
    "    else:\n",
    "        show(\"describe {}.{}\".format(dbName, tableName), num_rows)\n",
    "\n",
    "def ls(target):\n",
    "    !ls -al {target}\n",
    "\n",
    "def ls_and_head(target, lineno):\n",
    "    !ls -al {target} | grep -v 'crc' | head -{lineno}\n",
    "\n",
    "def cat(filename):\n",
    "    !cat {filename}\n",
    "\n",
    "def grep(keyword, filename):\n",
    "    !grep -i {keyword} {filename}\n",
    "\n",
    "def grep_and_json(keyword, filename):\n",
    "    !grep {keyword} {filename} | python -m json.tool\n",
    "\n",
    "def grep_sed_json(keyword, lineno, filename):\n",
    "    !grep {keyword} {filename} | sed -n {lineno}p | python -m json.tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28e9b69-edc2-4936-9131-2a6952722f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "def dropAndRemoveTable(dbName, tableName):\n",
    "    location=\"/home/jovyan/work/spark-warehouse/{}\".format(tableName)\n",
    "    !rm -rf {location}\n",
    "    sql(\"DROP TABLE IF EXISTS {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70834e6-06d3-4f34-b209-3b110d6566de",
   "metadata": {},
   "source": [
    "### Q1. 파티션 없는 users (id, firstName, lastName) 테이블의 에서 (middleName) 컬럼을 추가한 상태에서 저장 시에 스키마는 어떻게 되는가?\n",
    "```python\n",
    "AnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: 76246dde-b128-43df-8fca-605c2dbef282).\n",
    "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
    "'.option(\"mergeSchema\", \"true\")'.\n",
    "For other operations, set the session configuration\n",
    "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
    "specific to the operation for details.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d82a93bb-ae40-4ef4-9aff-d5a5ffd79f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|id                          |int                                                             |       |\n",
      "|firstName                   |string                                                          |       |\n",
      "|lastName                    |string                                                          |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Not partitioned             |                                                                |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.users                                                   |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/users                    |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropAndRemoveTable(dbName, tableName)\n",
    "\n",
    "schema_v1 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True)\n",
    "])\n",
    "rows_v1 = []\n",
    "rows_v1.append(Row(1, \"suhyuk\", \"park\"))\n",
    "rows_v1.append(Row(2, \"youngmi\", \"kim\"))\n",
    "\n",
    "df_v1 = spark.createDataFrame(rows_v1, schema_v1)\n",
    "df_v1.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "sql(\"show tables\")\n",
    "show(\"describe extended {}.{}\".format(dbName, tableName), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d62cde40-d143-4770-826d-d44356dc5532",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 76246dde-b128-43df-8fca-605c2dbef282).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- id: integer (nullable = true)\n-- firstName: string (nullable = true)\n-- lastName: string (nullable = true)\n\n\nData schema:\nroot\n-- id: integer (nullable = true)\n-- firstName: string (nullable = true)\n-- middleName: string (nullable = true)\n-- lastName: string (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m rows_v2\u001b[38;5;241m.\u001b[39mappend(Row(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msowon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meva\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpark\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      9\u001b[0m df_v2 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(rows_v2, schema_v2)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mdf_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow tables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m show(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescribe extended \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dbName, tableName), \u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:806\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m--> 806\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 76246dde-b128-43df-8fca-605c2dbef282).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- id: integer (nullable = true)\n-- firstName: string (nullable = true)\n-- lastName: string (nullable = true)\n\n\nData schema:\nroot\n-- id: integer (nullable = true)\n-- firstName: string (nullable = true)\n-- middleName: string (nullable = true)\n-- lastName: string (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "schema_v2 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"middleName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True)\n",
    "])\n",
    "rows_v2 = []\n",
    "rows_v2.append(Row(3, \"sowon\", \"eva\", \"park\"))\n",
    "df_v2 = spark.createDataFrame(rows_v2, schema_v2)\n",
    "df_v2.write.format(\"delta\").mode(\"append\").saveAsTable(\"{}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b207c-d0b4-4a11-97f4-07d1bc6131b6",
   "metadata": {},
   "source": [
    "### Q2. `mergeSchema` 옵션을 주고 저장하면 어떻게 되는가?\n",
    "> mergeSchema : 스키마가 추가되지만 기존 컬럼의 가장 마지막에 컬럼이 추가된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ca80ae3-d617-4c43-bc17-940520381bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|id                          |int                                                             |       |\n",
      "|firstName                   |string                                                          |       |\n",
      "|lastName                    |string                                                          |       |\n",
      "|middleName                  |string                                                          |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Not partitioned             |                                                                |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.users                                                   |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/users                    |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v2.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", True).saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "sql(\"show tables\")\n",
    "show(\"describe extended {}.{}\".format(dbName, tableName), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bcda87-8dad-4737-9805-aab02337739b",
   "metadata": {},
   "source": [
    "### Q3. `overwriteSchema` 옵션을 주고 저장하면 어떻게 되는가?\n",
    "> overwriteSchema : 덮어쓰기 모드(overwrite)에서만 사용할 수 있으며 기존 데이터가 모두 사라짐에 주의해야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "200fb277-647b-4dc2-94b5-5de52b373240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>lastName</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>park</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kim</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+--------+\n",
       "| id|firstName|lastName|\n",
       "+---+---------+--------+\n",
       "|  1|   suhyuk|    park|\n",
       "|  2|  youngmi|     kim|\n",
       "+---+---------+--------+"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"select * from {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b93f4eae-d02a-4e28-9d2d-e34c1591068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|id                          |int                                                             |       |\n",
      "|firstName                   |string                                                          |       |\n",
      "|middleName                  |string                                                          |       |\n",
      "|lastName                    |string                                                          |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Not partitioned             |                                                                |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.users                                                   |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/users                    |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1 예제를 다시 실행하고\n",
    "\n",
    "schema_v3 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"middleName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True)\n",
    "])\n",
    "rows_v3 = []\n",
    "rows_v3.append(Row(3, \"sowon\", \"eva\", \"park\"))\n",
    "rows_v3.append(Row(4, \"sihun\", \"sean\", \"park\"))\n",
    "df_v3 = spark.createDataFrame(rows_v3, schema_v3)\n",
    "df_v3.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "sql(\"show tables\")\n",
    "show(\"describe extended {}.{}\".format(dbName, tableName), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96e00dc3-1c78-45c3-bbdf-65645d2f3902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th></tr>\n",
       "<tr><td>4</td><td>sihun</td><td>sean</td><td>park</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+\n",
       "| id|firstName|middleName|lastName|\n",
       "+---+---------+----------+--------+\n",
       "|  4|    sihun|      sean|    park|\n",
       "|  3|    sowon|       eva|    park|\n",
       "+---+---------+----------+--------+"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"select * from {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c284f-07a2-4984-87d7-d00df42271f1",
   "metadata": {},
   "source": [
    "### Q4. 파티션이 존재하는 상태에서 users 테이블에 다른 파티션에만 middleName 컬럼이 추가되는 경우?\n",
    "> 어차피 델타 테이블의 경우 파티션 단위로 파일이 저장될 뿐, 특정 경로를 읽어내는 경우는 없으며, 스키마 또한 통합되어 관리되기 때문에 overwrite 혹은 merge 둘 중에 하나만 고민하면 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1c4fed4-b514-4ef7-ad7d-fcefcdaa07a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|id                          |int                                                             |       |\n",
      "|firstName                   |string                                                          |       |\n",
      "|lastName                    |string                                                          |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Part 0                      |lastName                                                        |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.users                                                   |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/users                    |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>lastName</th></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kim</td></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>park</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+--------+\n",
       "| id|firstName|lastName|\n",
       "+---+---------+--------+\n",
       "|  2|  youngmi|     kim|\n",
       "|  1|   suhyuk|    park|\n",
       "+---+---------+--------+"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropAndRemoveTable(dbName, tableName)\n",
    "\n",
    "schema_v4 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True)\n",
    "])\n",
    "rows_v4 = []\n",
    "rows_v4.append(Row(1, \"suhyuk\", \"park\"))\n",
    "rows_v4.append(Row(3, \"sowon\", \"park\"))\n",
    "rows_v4.append(Row(4, \"sean\", \"park\"))\n",
    "\n",
    "df_v4 = spark.createDataFrame(rows_v1, schema_v1)\n",
    "df_v4.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"lastName\").saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "show(\"describe extended {}.{}\".format(dbName, tableName), 100)\n",
    "sql(\"select * from {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13a31e23-b4df-432d-842c-77c46b0ce743",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_v4a = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"middleName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True)\n",
    "])\n",
    "rows_v4a = []\n",
    "rows_v4a.append(Row(2, \"youngmi\", \"kiki\", \"kim\"))\n",
    "df_v4a = spark.createDataFrame(rows_v4a, schema_v4a)\n",
    "df_v4a.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", True).partitionBy(\"lastName\").saveAsTable(\"{}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0027ea19-96a4-4a7e-9f6d-39d5f92d3a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|id                          |int                                                             |       |\n",
      "|firstName                   |string                                                          |       |\n",
      "|lastName                    |string                                                          |       |\n",
      "|middleName                  |string                                                          |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Part 0                      |lastName                                                        |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.users                                                   |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/users                    |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show(\"describe extended {}.{}\".format(dbName, tableName), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49362107-600a-4fd3-86ae-300a50be7819",
   "metadata": {},
   "source": [
    "### Q5. 데이터 변경이 존재하는 변경 시에 dataChange = false 주게 되는 경우?\n",
    "> 최초 테이블 생성 시에 저장되는 히스토리 정보 확인 후, append 이후에 히스토리 내역을 보면 `islocationLevel` 이 `Serializable` 에서 `SnapshotIsolation` 으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bdacd1b-a46c-47d3-8b56-172a7d92d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|id                          |int                                                             |       |\n",
      "|firstName                   |string                                                          |       |\n",
      "|lastName                    |string                                                          |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Part 0                      |lastName                                                        |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.users                                                   |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/users                    |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>0</td><td>2024-10-28 14:30:08.489</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;lastName&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 716}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|              timestamp|userId|userName|                        operation|                                                                    operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                          operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      0|2024-10-28 14:30:08.489|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"lastName\"], properties -> {}}|null|    null|     null|       null|  Serializable|        false|{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 716}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropAndRemoveTable(dbName, tableName)\n",
    "\n",
    "schema_v5 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True)\n",
    "])\n",
    "rows_v5 = []\n",
    "rows_v5.append(Row(1, \"suhyuk\", \"park\"))\n",
    "\n",
    "df_v5 = spark.createDataFrame(rows_v5, schema_v5)\n",
    "df_v5.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"lastName\").saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "show(\"describe extended {}.{}\".format(dbName, tableName), 100)\n",
    "history(dbName, tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edda5b44-642c-4aeb-869f-412dfb6f0daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>1</td><td>2024-10-28 14:30:14.594</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 723}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>0</td><td>2024-10-28 14:30:08.489</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;lastName&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 716}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|              timestamp|userId|userName|                        operation|                                                                    operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                          operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      1|2024-10-28 14:30:14.594|  null|    null|                            WRITE|                                                    {mode -> Append, partitionBy -> []}|null|    null|     null|          0|  Serializable|         true|{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 723}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      0|2024-10-28 14:30:08.489|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"lastName\"], properties -> {}}|null|    null|     null|       null|  Serializable|        false|{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 716}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_v5a = []\n",
    "rows_v5a.append(Row(2, \"youngmi\", \"kim\"))\n",
    "df_v5a = spark.createDataFrame(rows_v5a, schema_v5)\n",
    "df_v5a.write.format(\"delta\").mode(\"append\").option(\"dataChange\", True).partitionBy(\"lastName\").saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "sql(\"select * from {}.{}\".format(dbName, tableName))\n",
    "history(dbName, tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e286553f-b673-476a-b5cf-6f0f93a26b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>2</td><td>2024-10-28 14:30:25.965</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>1</td><td>SnapshotIsolation</td><td>true</td><td>{numFiles -&gt; 2, numOutputRows -&gt; 2, numOutputBytes -&gt; 1411}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>1</td><td>2024-10-28 14:30:14.594</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 723}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>0</td><td>2024-10-28 14:30:08.489</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;lastName&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 716}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|              timestamp|userId|userName|                        operation|                                                                    operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      2|2024-10-28 14:30:25.965|  null|    null|                            WRITE|                                                    {mode -> Append, partitionBy -> []}|null|    null|     null|          1|SnapshotIsolation|         true|{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 1411}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      1|2024-10-28 14:30:14.594|  null|    null|                            WRITE|                                                    {mode -> Append, partitionBy -> []}|null|    null|     null|          0|     Serializable|         true| {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 723}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      0|2024-10-28 14:30:08.489|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"lastName\"], properties -> {}}|null|    null|     null|       null|     Serializable|        false| {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 716}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_v5b = []\n",
    "rows_v5b.append(Row(3, \"sowon\", \"park\"))\n",
    "rows_v5b.append(Row(4, \"sean\", \"park\"))\n",
    "df_v5b = spark.createDataFrame(rows_v5b, schema_v5)\n",
    "df_v5b.write.format(\"delta\").mode(\"append\").option(\"dataChange\", False).partitionBy(\"lastName\").saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "sql(\"select * from {}.{}\".format(dbName, tableName))\n",
    "history(dbName, tableName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65193f8-097f-48ee-a372-3f9edde8125e",
   "metadata": {},
   "source": [
    "### Q6. 데이터 변경이 존재하지 않는 경우에 dataChange = false 주는 경우?\n",
    "> 최초 테이블 생성 시에 저장되는 히스토리 정보 확인 후, 마찬가지로 `islocationLevel` 이 `Serializable` 에서 `SnapshotIsolation` 으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aaa47704-c86b-4775-8d51-fb701d25bbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      "\n",
      "+---+---------+--------+\n",
      "|id |firstName|lastName|\n",
      "+---+---------+--------+\n",
      "|2  |youngmi  |kim     |\n",
      "|1  |suhyuk   |park    |\n",
      "|3  |sowon    |park    |\n",
      "|4  |sean     |park    |\n",
      "+---+---------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>2</td><td>2024-10-28 14:30:25.965</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>1</td><td>SnapshotIsolation</td><td>true</td><td>{numFiles -&gt; 2, numOutputRows -&gt; 2, numOutputBytes -&gt; 1411}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>1</td><td>2024-10-28 14:30:14.594</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 723}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>0</td><td>2024-10-28 14:30:08.489</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;lastName&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 716}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|              timestamp|userId|userName|                        operation|                                                                    operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      2|2024-10-28 14:30:25.965|  null|    null|                            WRITE|                                                    {mode -> Append, partitionBy -> []}|null|    null|     null|          1|SnapshotIsolation|         true|{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 1411}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      1|2024-10-28 14:30:14.594|  null|    null|                            WRITE|                                                    {mode -> Append, partitionBy -> []}|null|    null|     null|          0|     Serializable|         true| {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 723}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      0|2024-10-28 14:30:08.489|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"lastName\"], properties -> {}}|null|    null|     null|       null|     Serializable|        false| {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 716}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v6 = table(dbName, tableName)\n",
    "df_v6.printSchema()\n",
    "df_v6.show(10, truncate=False)\n",
    "history(dbName, tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23c865-35ec-4739-bc3c-15657480183e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>3</td><td>2024-10-28 14:36:58.225</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;lastName&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 2, numOutputRows -&gt; 4, numOutputBytes -&gt; 1443}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>2</td><td>2024-10-28 14:30:25.965</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>1</td><td>SnapshotIsolation</td><td>true</td><td>{numFiles -&gt; 2, numOutputRows -&gt; 2, numOutputBytes -&gt; 1411}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>1</td><td>2024-10-28 14:30:14.594</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 723}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>0</td><td>2024-10-28 14:30:08.489</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;lastName&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 716}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|              timestamp|userId|userName|                        operation|                                                                    operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      3|2024-10-28 14:36:58.225|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"lastName\"], properties -> {}}|null|    null|     null|          2|     Serializable|        false|{numFiles -> 2, numOutputRows -> 4, numOutputBytes -> 1443}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      2|2024-10-28 14:30:25.965|  null|    null|                            WRITE|                                                    {mode -> Append, partitionBy -> []}|null|    null|     null|          1|SnapshotIsolation|         true|{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 1411}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      1|2024-10-28 14:30:14.594|  null|    null|                            WRITE|                                                    {mode -> Append, partitionBy -> []}|null|    null|     null|          0|     Serializable|         true| {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 723}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      0|2024-10-28 14:30:08.489|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"lastName\"], properties -> {}}|null|    null|     null|       null|     Serializable|        false| {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 716}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v6.repartition(1).write.option(\"dataChange\", True).format(\"delta\").mode(\"overwrite\").saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "history(dbName, tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4bfc5d7b-ff6f-4e57-b19b-f1bf75094590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>4</td><td>2024-10-28 14:39:56.626</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;lastName&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>3</td><td>SnapshotIsolation</td><td>false</td><td>{numFiles -&gt; 4, numOutputRows -&gt; 4, numOutputBytes -&gt; 2850}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>3</td><td>2024-10-28 14:36:58.225</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;lastName&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 2, numOutputRows -&gt; 4, numOutputBytes -&gt; 1443}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>2</td><td>2024-10-28 14:30:25.965</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>1</td><td>SnapshotIsolation</td><td>true</td><td>{numFiles -&gt; 2, numOutputRows -&gt; 2, numOutputBytes -&gt; 1411}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>1</td><td>2024-10-28 14:30:14.594</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 723}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>0</td><td>2024-10-28 14:30:08.489</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;lastName&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 716}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|              timestamp|userId|userName|                        operation|                                                                    operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      4|2024-10-28 14:39:56.626|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"lastName\"], properties -> {}}|null|    null|     null|          3|SnapshotIsolation|        false|{numFiles -> 4, numOutputRows -> 4, numOutputBytes -> 2850}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      3|2024-10-28 14:36:58.225|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"lastName\"], properties -> {}}|null|    null|     null|          2|     Serializable|        false|{numFiles -> 2, numOutputRows -> 4, numOutputBytes -> 1443}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      2|2024-10-28 14:30:25.965|  null|    null|                            WRITE|                                                    {mode -> Append, partitionBy -> []}|null|    null|     null|          1|SnapshotIsolation|         true|{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 1411}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      1|2024-10-28 14:30:14.594|  null|    null|                            WRITE|                                                    {mode -> Append, partitionBy -> []}|null|    null|     null|          0|     Serializable|         true| {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 723}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      0|2024-10-28 14:30:08.489|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"lastName\"], properties -> {}}|null|    null|     null|       null|     Serializable|        false| {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 716}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-----------------------+------+--------+---------------------------------+---------------------------------------------------------------------------------------+----+--------+---------+-----------+-----------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v6.repartition(4).write.option(\"dataChange\", False).format(\"delta\").mode(\"overwrite\").saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "history(dbName, tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72e4cbf9-f8fd-4821-8760-564e75000c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbName=\"default\"\n",
    "tableName=\"pusan_popular_cluster_by\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f1d0e5-81aa-4f7d-8824-328e843d3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "pusan_popular_trip = spark.read.format(\"parquet\").load(\"data/pusan_popular_trip\")\n",
    "pusan_popular_trip.write.option(\"overwrite\", True).format(\"delta\").saveAsTable(\"{}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea7f2e50-8ed7-47fd-bfdb-be4978fa0089",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\nno viable alternative at input 'ALTER TABLE default.pusan_popular_cluster_by CLUSTER'(line 1, pos 45)\n\n== SQL ==\nALTER TABLE default.pusan_popular_cluster_by CLUSTER BY (id, name)\n---------------------------------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALTER TABLE \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m CLUSTER BY (id, name)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m describe(dbName, tableName)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36msql\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(query):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \nno viable alternative at input 'ALTER TABLE default.pusan_popular_cluster_by CLUSTER'(line 1, pos 45)\n\n== SQL ==\nALTER TABLE default.pusan_popular_cluster_by CLUSTER BY (id, name)\n---------------------------------------------^^^\n"
     ]
    }
   ],
   "source": [
    "sql(\"ALTER TABLE {}.{} CLUSTER BY (id, name)\".format(dbName, tableName))\n",
    "describe(dbName, tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897d93c4-6298-429c-8771-2481355051b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropAndRemoveTable(dbName, tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "709841db-6c5d-42cb-8bc7-ae90f5f01784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1956"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pusan_popular_trip = sql(\"select * from default.pusan_popular_trip\")\n",
    "pusan_popular_trip.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b02dab4a-c216-490f-a4e9-f11aa41c3f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|tableName           |isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|default  |delta_v1            |false      |\n",
      "|default  |delta_v2            |false      |\n",
      "|default  |family              |false      |\n",
      "|default  |pusan_popular_trip  |false      |\n",
      "|default  |pusan_popular_zorder|false      |\n",
      "|default  |users               |false      |\n",
      "+---------+--------------------+-----------+\n",
      "\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|category                    |int                                                             |       |\n",
      "|id                          |int                                                             |       |\n",
      "|name                        |string                                                          |       |\n",
      "|address                     |string                                                          |       |\n",
      "|naddress                    |string                                                          |       |\n",
      "|tel                         |string                                                          |       |\n",
      "|tag                         |string                                                          |       |\n",
      "|exp                         |decimal(38,0)                                                   |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Not partitioned             |                                                                |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.pusan_popular_trip                                      |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/pusan_popular_trip       |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show(\"show tables\")\n",
    "describe(\"default\", \"pusan_popular_trip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfbe304e-bd2f-4afc-9244-af75675fa497",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameWriter' object has no attribute 'clusterBy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pusan_popular_trip \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault.pusan_popular_trip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpusan_popular_trip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclusterBy\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid,name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dbName, tableName))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameWriter' object has no attribute 'clusterBy'"
     ]
    }
   ],
   "source": [
    "pusan_popular_trip = spark.read.table(\"default.pusan_popular_trip\")\n",
    "pusan_popular_trip.write.format(\"delta\").mode(\"overwrite\").clusterBy(\"id,name\").saveAsTable(\"{}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "501c98ce-3b6f-43a0-819a-c00a578a0a13",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeltaTableBuilder' object has no attribute 'clusterBy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# cluster-by 구문은 delta lake 3.x 부터 지원하며 현재는 2.x 임\u001b[39;00m\n\u001b[1;32m      2\u001b[0m createTable \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mDeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtableName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maddress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnaddress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecimal(38,0)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclusterBy\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DeltaTableBuilder' object has no attribute 'clusterBy'"
     ]
    }
   ],
   "source": [
    "# cluster-by 구문은 delta lake 3.x 부터 지원하며 현재는 2.x 임\n",
    "createTable = (\n",
    "    DeltaTable.create()\n",
    "    .tableName(\"{}.{}\".format(dbName, tableName))\n",
    "    .addColumn(\"category\", dataType = \"int\")\n",
    "    .addColumn(\"id\", dataType = \"int\")\n",
    "    .addColumn(\"name\", dataType = \"string\")\n",
    "    .addColumn(\"address\", dataType = \"string\")\n",
    "    .addColumn(\"naddress\", dataType = \"string\")\n",
    "    .addColumn(\"tel\", dataType = \"string\")\n",
    "    .addColumn(\"tag\", dataType = \"string\")\n",
    "    .addColumn(\"exp\", dataType = \"decimal(38,0)\")\n",
    "    .clusterBy(\"id\", \"name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1ed93-387e-4de1-877b-3584d936534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "createTable.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41660d5-3c7b-4af6-90f4-1accb5b11881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
