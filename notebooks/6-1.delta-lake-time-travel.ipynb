{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa727ab-1c39-48f7-b13c-582deb136463",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Using Time Travel\n",
    "\n",
    "### 델타 레이크 리텐션 관련 안전장치\n",
    "1. 델타 레이크 엔진의 경우, 로그 및 데이터의 최근 7일 이내의 로그 및 데이터는 삭제는 할 수 없다\n",
    "1. 만일, 7일 이내의 기간에 대한 로그 및 데이터 삭제를 위해서는 'spark.databricks.delta.retentionDurationCheck.enabled' 설정을 false 로 해야만 한다\n",
    "1. 그 이상의 기간에 대해서는 설정으로 변경 시에는 언제든지 변경이 가능하다\n",
    "1. 생성 및 수정을 통한 `TBLPROPERTIES ('delta.logRetentionDuration'='10 minutes')` 변경은 초 단위까지 가능하지만\n",
    "1. 명령어 `VACUUM` 실행 시에는 `VACUUM tableName RETAIN 0 HOURS;` 와 같이 시간 단위로만 변경이 가능하다\n",
    "\n",
    "### 데이터 리텐션의 특징\n",
    "1. 편의상 리텐션 테스트를 위해 `retentionDurationCheck.enabled` 값을 false 설정으로 했다고 가정 합니다\n",
    "1. 데이터 파일의 경우 `TBLPROPERTIES('delta.deletedFileRetentionDuration' = '30 seconds')` 와 같이 설정 후, `VACUUM` 수행 시에 즉각적으로 삭제가 된다\n",
    "\n",
    "### 로그 리텐션의 특징\n",
    "1. 아카이브 로그의 기본 설정 상, 10회에 1번씩 체크포인팅이 되기 때문에 그 와중에 발생한 커밋에 대한 시간 여행이 불가능합니다\n",
    "1. 또한 SQL 수준에서 명시적으로 `checkpont` 실행은 어렵기 때문에 API 통해서 별도 프로세스를 통해서 수행하거나, 자동 실행을 기대해야 합니다\n",
    "1. 로그의 경우 해당 로그가 생성되는 시점에 `logRetentionDuration` 값에 따라서 로그가 삭제되며, 해당 정보는 `TBLPROPERTIES` 정보를 활용합니다\n",
    "1. 기본 로그 삭제 설정이 30일이므로 생성시에 수정하거나, 명시적으로 변경해 주어야 운영관리에 문제가 발생하지 않습니다\n",
    "   - 특히 스트리밍 애플리케이션의 경우 로그 파일이 상당히 많아질 수 있고, 하이브 테이블로 생성된 경우 조회 성능에 영향을 줄 수 있습니다\n",
    "1. 2024년 10월 30일 현재 구현상 [MetadataCleanup.scala](https://github.com/delta-io/delta/blob/master/connectors/standalone/src/main/scala/io/delta/standalone/internal/MetadataCleanup.scala#L50) 구현상, 즉각적인 로그 삭제는 불가능합니다\n",
    "   - 구현 내역에 따르면 현재시간을 기준으로 리텐션 시간을 뺀 시간에서 GMT 기준 0시 기준으로 일자로 이전일까지만 삭제 대상으로 지정합니다\n",
    "   - 현재 체크포인트가 생성되는 시점에 로그 리텐션이 0시간 이라고 하더라도, 오늘 새벽 0시 이전의 로그만 삭제된다고 보면 됩니다\n",
    "1. 정리하면 아카이브 로그의 삭제는 아래의 제약 조건이 만족해야 로그 삭제 대상으로 선정된다\n",
    "   - 기본 로그 리텐션은 30일이므로 테이블 생성 시에 명시적으로 지정해 주어야 한다\n",
    "   - 아카이브 로그 삭제는 개별 커밋로그 생성 시점이 아니라, 체크포인트 생성 시점이므로 트랜잭션이 없다면 지연이 발생할 수 있으며 필요하다면 명시적인 체크포인팅 수행이 필요합니다\n",
    "   - 모든 조건이 만족하더라도 현재 구현상 한국시간 기준 오전 9시(GMT 기준 0시) 정각에 체크포인팅이 발생하는 경우 리텐션 시간 이전 로그가 삭제됩니다\n",
    "\n",
    "### 스트리밍 애플리케이션의 적절한 로그와 데이터 관리 정책\n",
    "1. 스트리밍 애플리케이션의 경우 로그 생성량에 따라 조정해야 하지만, 1분 이내 여러 트랜잭션이 발생하는 경우 로그 리텐션 조정 검토가 필요함\n",
    "1. 특히 델타 레이크 커넥터를 통한 하이브 서비스를 고려한다면, 로그의 수에 따른 성능 저하가 예상되므로 생성 시점에 1일 이하로 리텐션을 검토할 것\n",
    "1. 데이터 리텐션의 경우는 로그 리텐션 보다 길 수 없으므로 동일하게 조정하는 것이 적절합니다\n",
    "1. 외에도 스트리밍 처리는 자동 압축이나, 최적화 및 컴팩션 등의 배치 처리를 별도 스케줄로 운영해야 합니다\n",
    "\n",
    "### 로그/데이터 리텐션 강제 실행이 가능한가?\n",
    "* 데이터는 가능하지만, 로그는 어렵다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9879ddf1-1276-49d0-89b8-eec2b8df8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-11-openjdk-amd64\n",
      "/usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986ad10a-b2b6-4543-812e-b0f09f85b72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "from delta import *\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "warehouse_dir = f\"{work_dir}/spark-warehouse\"\n",
    "\n",
    "# Create spark session with hive enabled\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"pyspark-notebook\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_dir)\n",
    "    .enableHiveSupport()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5b931d-1ee5-4394-960f-33f131d4d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 델타 레이크 생성시에 반드시 `configure_spark_with_delta_pip` 구성을 통해 실행되어야 정상적인 델타 의존성이 로딩됩니다\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39abe029-89d4-473d-9f8c-e44146492879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.31.95.65:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb780e083d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark.conf.set(\"spark.sql.decimalOperations.allowPrecisionLoss\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e57b61f7-1ee2-4a9a-bb31-3663bca1953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(queries, num_rows = 20):\n",
    "    for query in queries.split(\";\"):\n",
    "        spark.sql(query).show(num_rows, truncate=False)\n",
    "\n",
    "def sql(query):\n",
    "    return spark.sql(query)\n",
    "\n",
    "def history(dbName, tableName):\n",
    "    return spark.sql(\"describe history {}.{}\".format(dbName, tableName))\n",
    "\n",
    "def table(dbName, tableName):\n",
    "    return spark.read.format(\"delta\").table(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "def describe(dbName, tableName, extended = True, num_rows = 20):\n",
    "    if extended:\n",
    "        show(\"describe extended {}.{}\".format(dbName, tableName), num_rows)\n",
    "    else:\n",
    "        show(\"describe {}.{}\".format(dbName, tableName), num_rows)\n",
    "\n",
    "def ls(target):\n",
    "    !ls -al {target}\n",
    "\n",
    "def ls_and_head(target, lineno):\n",
    "    !ls -al {target} | grep -v 'crc' | head -{lineno}\n",
    "\n",
    "def cat(filename):\n",
    "    !cat {filename}\n",
    "\n",
    "def grep(keyword, filename):\n",
    "    !grep -i {keyword} {filename}\n",
    "\n",
    "def grep_and_json(keyword, filename):\n",
    "    !grep {keyword} {filename} | python -m json.tool\n",
    "\n",
    "def grep_sed_json(keyword, lineno, filename):\n",
    "    !grep {keyword} {filename} | sed -n {lineno}p | python -m json.tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e28e9b69-edc2-4936-9131-2a6952722f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>delta_v1</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>delta_v2</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>family</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>users</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---------+-----------+\n",
       "|namespace|tableName|isTemporary|\n",
       "+---------+---------+-----------+\n",
       "|  default| delta_v1|      false|\n",
       "|  default| delta_v2|      false|\n",
       "|  default|   family|      false|\n",
       "|  default|    users|      false|\n",
       "+---------+---------+-----------+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8602f-731e-4e82-9051-9d2e42c0eb00",
   "metadata": {},
   "source": [
    "\n",
    "### Q1. 예제 테이블 없는 경우 데이터 리텐션 강제 실행방법?\n",
    "* 테이블 생성시 부터 retentionDurationCheck false, deletedFileRetentionDuration 1분 설정\n",
    "* 임의의 데이터 저장 후, 일부 데이터를 즉시 삭제\n",
    "* 약 1분 대기 후에, retain 0 hours 설정으로 vacuum 실행\n",
    "* 1분 이전의 삭제됨을 확인\n",
    "\n",
    "```sql\n",
    "CREATE TABLE delta_v1 (id INT, data STRING) \n",
    "USING delta \n",
    "TBLPROPERTIES (\n",
    "  'delta.deletedFileRetentionDuration' = '1 minutes',\n",
    "  'spark.databricks.delta.retentionDurationCheck.enabled' = 'false'\n",
    ");\n",
    "\n",
    "INSERT INTO delta_v1 VALUES (1, 'first'), (2, 'second');\n",
    "DELETE FROM delta_v1 WHERE id = 1;\n",
    "\n",
    "-- wait 1 minute\n",
    "VACUUM delta_v1;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "906749a5-79ff-4be4-8bda-f937ccf1831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "def dropAndRemoveTable(dbName, tableName):\n",
    "    location=\"/home/jovyan/work/spark-warehouse/{}\".format(tableName)\n",
    "    !rm -rf {location}\n",
    "    sql(\"DROP TABLE IF EXISTS {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d6f63e-5c0f-4451-9bb4-43a0ceb0ebbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbName=\"default\"\n",
    "tableName=\"delta_v1\"\n",
    "dropAndRemoveTable(dbName, tableName)\n",
    "\n",
    "sql(\"\"\"\n",
    "CREATE TABLE {} (id INT, data STRING) \n",
    "USING delta \n",
    "TBLPROPERTIES (\n",
    "  'delta.deletedFileRetentionDuration' = '1 minutes',\n",
    "  'spark.databricks.delta.retentionDurationCheck.enabled' = 'false'\n",
    ")\"\"\".format(tableName))\n",
    "# spark.databricks.delta.retentionDurationCheck.enabled\n",
    "\n",
    "sql(\"INSERT INTO {} VALUES (1, 'first'), (2, 'second')\".format(tableName))\n",
    "sql(\"DELETE FROM {} WHERE id = 1\".format(tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19d150b3-d17d-4728-98fa-9519cd3330ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwxrwxrwx 1 jovyan 1000 512 Oct 29 04:56 .\n",
      "drwxrwxrwx 1 jovyan 1000 512 Oct 29 04:55 ..\n",
      "drwxrwxrwx 1 jovyan 1000 512 Oct 29 04:56 _delta_log\n",
      "-rwxrwxrwx 1 jovyan 1000 382 Oct 29 04:56 part-00000-0e13db32-fe48-4793-826e-df135f4e0762-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  12 Oct 29 04:56 .part-00000-0e13db32-fe48-4793-826e-df135f4e0762-c000.snappy.parquet.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 694 Oct 29 04:56 part-00000-4a98b7b9-9d2f-470d-a138-8d1ca4d5a8dc-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  16 Oct 29 04:56 .part-00000-4a98b7b9-9d2f-470d-a138-8d1ca4d5a8dc-c000.snappy.parquet.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 701 Oct 29 04:56 part-00001-e66155f1-5323-4090-ab29-c3c571193ed3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  16 Oct 29 04:56 .part-00001-e66155f1-5323-4090-ab29-c3c571193ed3-c000.snappy.parquet.crc\n"
     ]
    }
   ],
   "source": [
    "ls(\"./spark-warehouse/delta_v1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b6fa223-b229-46ee-ab41-37c3d89c5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- wait 1 minute\n",
    "import time\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cc29942-42b5-431e-bbf1-77105d040a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th></tr>\n",
       "<tr><td>file:/home/jovyan/work/spark-warehouse/delta_v1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------------------------------------+\n",
       "|                                           path|\n",
       "+-----------------------------------------------+\n",
       "|file:/home/jovyan/work/spark-warehouse/delta_v1|\n",
       "+-----------------------------------------------+"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"VACUUM {}\".format(tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b646e31f-7333-4800-9eba-b9f8c457d42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\n",
      "drwxrwxrwx 1 jovyan 1000 512 Oct 29  2024 .\n",
      "drwxrwxrwx 1 jovyan 1000 512 Oct 29 04:55 ..\n",
      "drwxrwxrwx 1 jovyan 1000 512 Oct 29 04:56 _delta_log\n",
      "-rwxrwxrwx 1 jovyan 1000 382 Oct 29 04:56 part-00000-0e13db32-fe48-4793-826e-df135f4e0762-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  12 Oct 29 04:56 .part-00000-0e13db32-fe48-4793-826e-df135f4e0762-c000.snappy.parquet.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 701 Oct 29 04:56 part-00001-e66155f1-5323-4090-ab29-c3c571193ed3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  16 Oct 29 04:56 .part-00001-e66155f1-5323-4090-ab29-c3c571193ed3-c000.snappy.parquet.crc\n"
     ]
    }
   ],
   "source": [
    "ls(\"./spark-warehouse/{}/\".format(tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc3ab874-ced1-4483-b987-11274065f753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>data</th></tr>\n",
       "<tr><td>2</td><td>second</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+------+\n",
       "| id|  data|\n",
       "+---+------+\n",
       "|  2|second|\n",
       "+---+------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"select * from {}\".format(tableName))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad26be9-a446-49ed-81a4-0bf805bdae81",
   "metadata": {},
   "source": [
    "### Q2. 예제 테이블 없는 경우 로그 리텐션 강제 실행방법?\n",
    "* 테이블 생성시 부터 retentionDurationCheck false 설정, logRetentionDuration 1분 설정\n",
    "* 임의의 데이터를 저장하되 10회 미만의 커밋이 발생하도록 한다 (예 5회, 총 6개의 커밋 발생)\n",
    "* 약 1분 대기 후에, 명시적으로 checkpoint 메서드를 실행\n",
    "* 1분 이전에 수행된 로그가 삭제 되었는지 확인\n",
    "\n",
    "```sql\n",
    "CREATE TABLE delta_v2 (id INT, data STRING) \n",
    "USING delta \n",
    "TBLPROPERTIES (\n",
    "  'delta.logRetentionDuration' = '1 minutes',\n",
    "  'spark.databricks.delta.retentionDurationCheck.enabled' = 'false'\n",
    ");\n",
    "\n",
    "INSERT INTO delta_v2 VALUES (1, 'first'), (2, 'second');\n",
    "\n",
    "-- wait 1 minute\n",
    "OPTIMIZE delta_v2;\n",
    "CHECKPOINT delta_v2;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec244c5-175d-4e69-b4be-414dfd66053c",
   "metadata": {},
   "source": [
    "* 임의의 테이블을 log retention 지정하지 않고 생성하는 경우 30일 log-retention 이므로 이 때에 11건을 저장한다\n",
    "* 그리고 log-retention 1일로 변경하고, 다시 11건을 집어 넣고 익일 오전 까지 대기\n",
    "* 익일 오전 8시에 다시 11건을 입력하고 모니터링 했을 때에 로그 상태를 보고\n",
    "* 다시 오전 9시에 11건을 입력하고 모니터링 했을 때에 로그 상태를 보면 모든 테스트가 완료된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8598e90-e160-43e3-9204-5c3650543e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(1)</th></tr>\n",
       "<tr><td>11</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "|      11|\n",
       "+--------+"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbName=\"default\"\n",
    "tableName=\"delta_v2\"\n",
    "dropAndRemoveTable(dbName, tableName)\n",
    "\n",
    "sql(\"\"\"\n",
    "CREATE TABLE {}.{} (id INT, data STRING) \n",
    "USING delta \n",
    "\"\"\".format(dbName, tableName))\n",
    "\n",
    "for id in range(0, 11):\n",
    "    sql(\"INSERT INTO {}.{} VALUES ({}, 'data-{}')\".format(dbName, tableName, id, id))\n",
    "sql(\"select count(1) from {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c11521f-eb86-4d5e-8559-db43bee039a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 64\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 29 09:20 .\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 29 09:20 ..\n",
      "-rwxrwxrwx 1 jovyan 1000   771 Oct 29 09:20 00000000000000000000.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000000.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000001.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000001.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000002.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000002.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000003.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000003.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000004.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000004.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000005.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000005.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000006.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000006.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000007.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000007.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000008.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000008.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000009.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000009.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 11639 Oct 29 09:20 00000000000000000010.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   100 Oct 29 09:20 .00000000000000000010.checkpoint.parquet.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000010.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000010.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:20 00000000000000000011.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000011.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000  3023 Oct 29 09:20 _last_checkpoint\n",
      "-rwxrwxrwx 1 jovyan 1000    32 Oct 29 09:20 ._last_checkpoint.crc\n"
     ]
    }
   ],
   "source": [
    "ls(\"./spark-warehouse/{}/_delta_log\".format(tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87e24d54-4850-49ce-81e9-8d048f65f010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation   |operationParameters                                                          |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                          |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------+------------+-----------------------------------+\n",
      "|11     |2024-10-29 18:20:19.877|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |10         |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 708}|null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "|10     |2024-10-29 18:20:17.998|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |9          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 701}|null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "|9      |2024-10-29 18:20:16.846|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |8          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 701}|null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "|8      |2024-10-29 18:20:15.783|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |7          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 701}|null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "|7      |2024-10-29 18:20:14.777|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |6          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 701}|null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "|6      |2024-10-29 18:20:13.769|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |5          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 701}|null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "|5      |2024-10-29 18:20:12.816|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |4          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 701}|null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "|4      |2024-10-29 18:20:11.952|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |3          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 701}|null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "|3      |2024-10-29 18:20:11.058|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |2          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 701}|null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "|2      |2024-10-29 18:20:10.193|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |1          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 701}|null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "|1      |2024-10-29 18:20:09.419|null  |null    |WRITE       |{mode -> Append, partitionBy -> []}                                          |null|null    |null     |0          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 701}|null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "|0      |2024-10-29 18:20:08.58 |null  |null    |CREATE TABLE|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|null    |null     |null       |Serializable  |true         |{}                                                        |null        |Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------+------------+-----------------------------------+\n",
      "\n",
      "+----------------------+-------+\n",
      "|key                   |value  |\n",
      "+----------------------+-------+\n",
      "|Type                  |MANAGED|\n",
      "|delta.minReaderVersion|1      |\n",
      "|delta.minWriterVersion|2      |\n",
      "+----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show(\"describe history {}.{}\".format(dbName, tableName))\n",
    "show(\"show tblproperties {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18c7f66b-73cc-49fa-b5bb-882036bb36f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(1)</th></tr>\n",
       "<tr><td>22</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "|      22|\n",
       "+--------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"delta.logRetentionDuration\", \"1 days\")\n",
    "sql(\"\"\"\n",
    "ALTER TABLE {}.{}\n",
    "SET TBLPROPERTIES (\n",
    "  'delta.logRetentionDuration' = '1 minutes'\n",
    ")\n",
    "\"\"\".format(dbName, tableName))\n",
    "\n",
    "for id in range(11, 22):\n",
    "    sql(\"INSERT INTO {}.{} VALUES ({}, 'data-{}')\".format(dbName, tableName, id, id))\n",
    "\n",
    "sql(\"select count(1) from {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8576552c-b574-4e85-a2c0-f398b9c37b8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 180\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 29  2024 .\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 29  2024 ..\n",
      "-rwxrwxrwx 1 jovyan 1000   771 Oct 29 09:20 00000000000000000000.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000000.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000001.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000001.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000002.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000002.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000003.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000003.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000004.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000004.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000005.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000005.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000006.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000006.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000007.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000007.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000008.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000008.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000009.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000009.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 11639 Oct 29 09:20 00000000000000000010.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   100 Oct 29 09:20 .00000000000000000010.checkpoint.parquet.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000010.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000010.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:20 00000000000000000011.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000011.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000012.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:25 .00000000000000000012.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000013.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:25 .00000000000000000013.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000014.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:25 .00000000000000000014.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000015.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:25 .00000000000000000015.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000016.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:25 .00000000000000000016.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000017.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:25 .00000000000000000017.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000018.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:25 .00000000000000000018.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000019.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:25 .00000000000000000019.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 12281 Oct 29 09:25 00000000000000000020.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   104 Oct 29 09:25 .00000000000000000020.checkpoint.parquet.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000020.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:25 .00000000000000000020.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000021.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:25 .00000000000000000021.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000022.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:25 .00000000000000000022.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:46 00000000000000000023.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 22:46 .00000000000000000023.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:46 00000000000000000024.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 22:46 .00000000000000000024.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:46 00000000000000000025.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 22:46 .00000000000000000025.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:46 00000000000000000026.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 22:46 .00000000000000000026.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:46 00000000000000000027.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 22:46 .00000000000000000027.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:47 00000000000000000028.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 22:47 .00000000000000000028.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:47 00000000000000000029.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 22:47 .00000000000000000029.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 12896 Oct 29 22:47 00000000000000000030.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   112 Oct 29 22:47 .00000000000000000030.checkpoint.parquet.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:47 00000000000000000030.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 22:47 .00000000000000000030.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:47 00000000000000000031.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 22:47 .00000000000000000031.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:47 00000000000000000032.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 22:47 .00000000000000000032.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29  2024 00000000000000000033.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29  2024 .00000000000000000033.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000  3023 Oct 29 22:47 _last_checkpoint\n",
      "-rwxrwxrwx 1 jovyan 1000    32 Oct 29 22:47 ._last_checkpoint.crc\n"
     ]
    }
   ],
   "source": [
    "# 2024/10/30 오전 9시 이전에 아래의 명령어 수행\n",
    "\n",
    "for id in range(22, 33):\n",
    "    sql(\"INSERT INTO {}.{} VALUES ({}, 'data-{}')\".format(dbName, tableName, id, id))\n",
    "\n",
    "sql(\"select count(1) from {}.{}\".format(dbName, tableName))\n",
    "\n",
    "# 아래의 경로에 log 가 삭제되지 않았음을 확인\n",
    "ls(\"./spark-warehouse/{}/_delta_log\".format(tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "160c4cdb-41c0-47df-b582-bbe747ff8b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed 30 Oct 2024 12:04:10 AM UTC\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f80eec3-f006-4723-8a73-35fa0c6e8117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 240\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 30 00:08 .\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 30 00:08 ..\n",
      "-rwxrwxrwx 1 jovyan 1000   771 Oct 29 09:20 00000000000000000000.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000000.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000001.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000001.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000002.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 09:20 .00000000000000000002.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000003.json\n"
     ]
    }
   ],
   "source": [
    "# 2024/10/30 오전 9시 이후에 아래의 명령어 수행\n",
    "\n",
    "for id in range(33, 44):\n",
    "    sql(\"INSERT INTO {}.{} VALUES ({}, 'data-{}')\".format(dbName, tableName, id, id))\n",
    "\n",
    "sql(\"select count(1) from {}.{}\".format(dbName, tableName))\n",
    "\n",
    "# 처음 생성한 로그는 삭제되면 안 될 듯 ...\n",
    "ls_and_head(\"./spark-warehouse/{}/_delta_log\".format(tableName), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7b112fd-b7f7-4431-afd7-044b8e6fa6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 244\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 30 00:21 .\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 30 00:08 ..\n",
      "-rwxrwxrwx 1 jovyan 1000   771 Oct 29 09:20 00000000000000000000.json\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000001.json\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000002.json\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000003.json\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000004.json\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000005.json\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000006.json\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000007.json\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000008.json\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000009.json\n",
      "-rwxrwxrwx 1 jovyan 1000 11639 Oct 29 09:20 00000000000000000010.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 09:20 00000000000000000010.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:20 00000000000000000011.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000012.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000013.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000014.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000015.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000016.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000017.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000018.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000019.json\n",
      "-rwxrwxrwx 1 jovyan 1000 12281 Oct 29 09:25 00000000000000000020.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000020.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000021.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 09:25 00000000000000000022.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:46 00000000000000000023.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:46 00000000000000000024.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:46 00000000000000000025.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:46 00000000000000000026.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:46 00000000000000000027.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:47 00000000000000000028.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:47 00000000000000000029.json\n",
      "-rwxrwxrwx 1 jovyan 1000 12896 Oct 29 22:47 00000000000000000030.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:47 00000000000000000030.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:47 00000000000000000031.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:47 00000000000000000032.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 22:47 00000000000000000033.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000034.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000035.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000036.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000037.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000038.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000039.json\n",
      "-rwxrwxrwx 1 jovyan 1000 13518 Oct 30 00:08 00000000000000000040.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000040.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000041.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000042.json\n"
     ]
    }
   ],
   "source": [
    "ls_and_head(\"./spark-warehouse/{}/_delta_log\".format(tableName), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "788d9b78-050c-4452-8e0e-707497b4bb12",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- txn: struct (nullable = true)\n",
      " |    |-- appId: string (nullable = true)\n",
      " |    |-- version: long (nullable = true)\n",
      " |    |-- lastUpdated: long (nullable = true)\n",
      " |-- add: struct (nullable = true)\n",
      " |    |-- path: string (nullable = true)\n",
      " |    |-- partitionValues: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- modificationTime: long (nullable = true)\n",
      " |    |-- dataChange: boolean (nullable = true)\n",
      " |    |-- tags: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- stats: string (nullable = true)\n",
      " |-- remove: struct (nullable = true)\n",
      " |    |-- path: string (nullable = true)\n",
      " |    |-- deletionTimestamp: long (nullable = true)\n",
      " |    |-- dataChange: boolean (nullable = true)\n",
      " |    |-- extendedFileMetadata: boolean (nullable = true)\n",
      " |    |-- partitionValues: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- tags: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |-- metaData: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- format: struct (nullable = true)\n",
      " |    |    |-- provider: string (nullable = true)\n",
      " |    |    |-- options: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- schemaString: string (nullable = true)\n",
      " |    |-- partitionColumns: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- configuration: map (nullable = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |-- createdTime: long (nullable = true)\n",
      " |-- protocol: struct (nullable = true)\n",
      " |    |-- minReaderVersion: integer (nullable = true)\n",
      " |    |-- minWriterVersion: integer (nullable = true)\n",
      "\n",
      "+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|txn |add                                                                                                                                                                                                                                 |remove|metaData                                                                                                                                                                                                                                         |protocol|\n",
      "+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|null|{part-00000-623f9e1b-dce1-442a-9402-928571a81abb-c000.snappy.parquet, {}, 701, 1730193611910, false, null, {\"numRecords\":1,\"minValues\":{\"id\":3,\"data\":\"data-3\"},\"maxValues\":{\"id\":3,\"data\":\"data-3\"},\"nullCount\":{\"id\":0,\"data\":0}}}|null  |null                                                                                                                                                                                                                                             |null    |\n",
      "|null|{part-00000-81390712-ff72-44fc-9021-02981aa1020b-c000.snappy.parquet, {}, 701, 1730193611017, false, null, {\"numRecords\":1,\"minValues\":{\"id\":2,\"data\":\"data-2\"},\"maxValues\":{\"id\":2,\"data\":\"data-2\"},\"nullCount\":{\"id\":0,\"data\":0}}}|null  |null                                                                                                                                                                                                                                             |null    |\n",
      "|null|{part-00000-57baff27-b3c4-4dbf-96ca-ad142914b539-c000.snappy.parquet, {}, 701, 1730193614736, false, null, {\"numRecords\":1,\"minValues\":{\"id\":6,\"data\":\"data-6\"},\"maxValues\":{\"id\":6,\"data\":\"data-6\"},\"nullCount\":{\"id\":0,\"data\":0}}}|null  |null                                                                                                                                                                                                                                             |null    |\n",
      "|null|{part-00000-ecc90c18-caa8-4443-b9a7-7f4b3685de1c-c000.snappy.parquet, {}, 701, 1730193613728, false, null, {\"numRecords\":1,\"minValues\":{\"id\":5,\"data\":\"data-5\"},\"maxValues\":{\"id\":5,\"data\":\"data-5\"},\"nullCount\":{\"id\":0,\"data\":0}}}|null  |null                                                                                                                                                                                                                                             |null    |\n",
      "|null|{part-00000-df7f578a-bfd5-4883-aff1-b00b8babd9b8-c000.snappy.parquet, {}, 701, 1730193612773, false, null, {\"numRecords\":1,\"minValues\":{\"id\":4,\"data\":\"data-4\"},\"maxValues\":{\"id\":4,\"data\":\"data-4\"},\"nullCount\":{\"id\":0,\"data\":0}}}|null  |null                                                                                                                                                                                                                                             |null    |\n",
      "|null|{part-00000-c22ccee8-ec05-4b3c-810d-8578c5d287cd-c000.snappy.parquet, {}, 701, 1730193617951, false, null, {\"numRecords\":1,\"minValues\":{\"id\":9,\"data\":\"data-9\"},\"maxValues\":{\"id\":9,\"data\":\"data-9\"},\"nullCount\":{\"id\":0,\"data\":0}}}|null  |null                                                                                                                                                                                                                                             |null    |\n",
      "|null|{part-00000-16fcd67c-9f51-403e-ac18-224ba842bc08-c000.snappy.parquet, {}, 701, 1730193615742, false, null, {\"numRecords\":1,\"minValues\":{\"id\":7,\"data\":\"data-7\"},\"maxValues\":{\"id\":7,\"data\":\"data-7\"},\"nullCount\":{\"id\":0,\"data\":0}}}|null  |null                                                                                                                                                                                                                                             |null    |\n",
      "|null|{part-00000-3016df76-1f24-4d60-9069-f66c07b0cdd3-c000.snappy.parquet, {}, 701, 1730193616802, false, null, {\"numRecords\":1,\"minValues\":{\"id\":8,\"data\":\"data-8\"},\"maxValues\":{\"id\":8,\"data\":\"data-8\"},\"nullCount\":{\"id\":0,\"data\":0}}}|null  |null                                                                                                                                                                                                                                             |null    |\n",
      "|null|{part-00000-233b0c51-044b-486b-a6bc-c176c7f410ea-c000.snappy.parquet, {}, 701, 1730193610151, false, null, {\"numRecords\":1,\"minValues\":{\"id\":1,\"data\":\"data-1\"},\"maxValues\":{\"id\":1,\"data\":\"data-1\"},\"nullCount\":{\"id\":0,\"data\":0}}}|null  |null                                                                                                                                                                                                                                             |null    |\n",
      "|null|{part-00000-861c3791-1e2a-4471-a70f-670b002b31e0-c000.snappy.parquet, {}, 701, 1730193609376, false, null, {\"numRecords\":1,\"minValues\":{\"id\":0,\"data\":\"data-0\"},\"maxValues\":{\"id\":0,\"data\":\"data-0\"},\"nullCount\":{\"id\":0,\"data\":0}}}|null  |null                                                                                                                                                                                                                                             |null    |\n",
      "|null|null                                                                                                                                                                                                                                |null  |null                                                                                                                                                                                                                                             |{1, 2}  |\n",
      "|null|null                                                                                                                                                                                                                                |null  |{bbe524a7-376b-4797-8283-565c9ed03e20, null, null, {parquet, {}}, {\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"data\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}, [], {}, 1730193606688}|null    |\n",
      "+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"./spark-warehouse/{}/_delta_log/00000000000000000010.checkpoint.parquet\".format(tableName))\n",
    "df.printSchema()\n",
    "df.orderBy(asc(\"txn.version\")).show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54292b0e-9d65-44d0-ae9b-5608e11b9277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+---------+\n",
      "|key                       |value    |\n",
      "+--------------------------+---------+\n",
      "|Type                      |MANAGED  |\n",
      "|delta.logRetentionDuration|1 minutes|\n",
      "|delta.minReaderVersion    |1        |\n",
      "|delta.minWriterVersion    |2        |\n",
      "+--------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show(\"describe history {}.{}\".format(dbName, tableName))\n",
    "show(\"show tblproperties {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfee8eb2-38d5-48b4-8323-129cbda0bbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 128\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 30  2024 .\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 30  2024 ..\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000034.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000035.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000036.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000037.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000038.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000039.json\n",
      "-rwxrwxrwx 1 jovyan 1000 13518 Oct 30 00:08 00000000000000000040.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000040.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000041.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000042.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000043.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:08 00000000000000000044.json\n",
      "-rwxrwxrwx 1 jovyan 1000   763 Oct 30 00:21 00000000000000000045.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:22 00000000000000000046.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:22 00000000000000000047.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:22 00000000000000000048.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:22 00000000000000000049.json\n",
      "-rwxrwxrwx 1 jovyan 1000 14345 Oct 30 00:22 00000000000000000050.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:22 00000000000000000050.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:22 00000000000000000051.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:22 00000000000000000052.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:22 00000000000000000053.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:22 00000000000000000054.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30 00:22 00000000000000000055.json\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 30  2024 00000000000000000056.json\n",
      "-rwxrwxrwx 1 jovyan 1000  3023 Oct 30 00:22 _last_checkpoint\n"
     ]
    }
   ],
   "source": [
    "# 2024/10/30 오전 9시 retry\n",
    "\n",
    "for id in range(44, 55):\n",
    "    sql(\"INSERT INTO {}.{} VALUES ({}, 'data-{}')\".format(dbName, tableName, id, id))\n",
    "\n",
    "sql(\"select count(1) from {}.{}\".format(dbName, tableName))\n",
    "\n",
    "ls_and_head(\"./spark-warehouse/{}/_delta_log\".format(tableName), 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42953a3-01c8-4dd4-aa9e-7d315df8d78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143294a-45bb-48fb-b7ee-d650b51ff590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62cde40-d143-4770-826d-d44356dc5532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(1)</th></tr>\n",
       "<tr><td>22</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "|      22|\n",
       "+--------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- wait 1 minute\n",
    "import time\n",
    "time.sleep(60)\n",
    "\n",
    "for id in range(11, 22):\n",
    "    sql(\"INSERT INTO {}.{} VALUES ({}, 'data-{}')\".format(dbName, tableName, id, id))\n",
    "sql(\"select count(1) from {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc8156dd-59fa-416f-9d85-2a2d4653c8ab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 132\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 29 06:31 .\n",
      "drwxrwxrwx 1 jovyan 1000   512 Oct 29  2024 ..\n",
      "-rwxrwxrwx 1 jovyan 1000   855 Oct 29 06:29 00000000000000000000.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000000.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 06:29 00000000000000000001.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000001.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 06:29 00000000000000000002.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000002.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 06:29 00000000000000000003.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000003.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 06:29 00000000000000000004.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000004.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 06:29 00000000000000000005.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000005.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 06:29 00000000000000000006.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000006.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 06:29 00000000000000000007.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000007.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 06:29 00000000000000000008.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000008.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 06:29 00000000000000000009.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000009.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 11907 Oct 29 06:29 00000000000000000010.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   104 Oct 29 06:29 .00000000000000000010.checkpoint.parquet.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   693 Oct 29 06:29 00000000000000000010.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000010.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:29 00000000000000000011.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:29 .00000000000000000011.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:30 00000000000000000012.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:30 .00000000000000000012.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:30 00000000000000000013.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:30 .00000000000000000013.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:30 00000000000000000014.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:30 .00000000000000000014.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:30 00000000000000000015.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:30 .00000000000000000015.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:31 00000000000000000016.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:31 .00000000000000000016.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:31 00000000000000000017.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:31 .00000000000000000017.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:31 00000000000000000018.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:31 .00000000000000000018.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:31 00000000000000000019.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:31 .00000000000000000019.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 12553 Oct 29 06:31 00000000000000000020.checkpoint.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000   108 Oct 29 06:31 .00000000000000000020.checkpoint.parquet.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:31 00000000000000000020.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:31 .00000000000000000020.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:31 00000000000000000021.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:31 .00000000000000000021.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000   698 Oct 29 06:31 00000000000000000022.json\n",
      "-rwxrwxrwx 1 jovyan 1000    16 Oct 29 06:31 .00000000000000000022.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000  5303 Oct 29 06:31 00000000000000000023.json\n",
      "-rwxrwxrwx 1 jovyan 1000    52 Oct 29 06:31 .00000000000000000023.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000  3023 Oct 29 06:31 _last_checkpoint\n",
      "-rwxrwxrwx 1 jovyan 1000    32 Oct 29 06:31 ._last_checkpoint.crc\n"
     ]
    }
   ],
   "source": [
    "sql(\"OPTIMIZE {}.{}\".format(dbName, tableName))\n",
    "sql(\"VACUUM {}.{} RETAIN 0 HOURS\".format(dbName, tableName))\n",
    "ls(\"./spark-warehouse/{}/_delta_log\".format(tableName))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b207c-d0b4-4a11-97f4-07d1bc6131b6",
   "metadata": {},
   "source": [
    "### Q3. 특정 값을 가진 레코드 전체 및 히스토리 정보까지 완전히 삭제하려면?\n",
    "* 테이블 생성 시에 스파크 세션 수준에서 'retentionDurationCheck.enabled' false 설정\n",
    "* 대상 테이블에서 임의의 로우를 가진 데이터를 삭제\n",
    "* `retain 0 hours` 옵션으로 `vacuum` 수행\n",
    "* 과거 데이터 및 히스토리 정보 확인\n",
    "\n",
    "```sql\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"GDPR Retention Session\")\n",
    "  .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "  .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "spark.sql(\"DELETE FROM delta_v3 WHERE phone IS NOT NULL\")\n",
    "spark.sql(\"VACUUM delta_v3 RETAIN 0 HOURS\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ca80ae3-d617-4c43-bc17-940520381bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|id                          |int                                                             |       |\n",
      "|firstName                   |string                                                          |       |\n",
      "|lastName                    |string                                                          |       |\n",
      "|middleName                  |string                                                          |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Not partitioned             |                                                                |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.users                                                   |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/users                    |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_v2.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", True).saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "sql(\"show tables\")\n",
    "show(\"describe extended {}.{}\".format(dbName, tableName), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bcda87-8dad-4737-9805-aab02337739b",
   "metadata": {},
   "source": [
    "### Q4. 특정 값을 가진 컬럼을 제거하고 히스토리 정보까지 완전히 삭제하려면?\n",
    "* 대상 테이블에서 임의의 로우를 가진 컬럼을 제거한 데이터 프레임 생성\n",
    "* 기존 경로에 `overwrite` 모드와 `overwriteSchema` 옵션으로 데이터를 덮어쓴다\n",
    "* 기존 데이터 및 히스토리 정보 확인\n",
    "\n",
    "```sql\n",
    "val originalData = spark.read.format(\"delta\").load(\"/path/to/delta_v4\")\n",
    "val delta_v4 = originalData.drop(\"phone\")\n",
    "delta_v4.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"/path/to/delta_v4\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "200fb277-647b-4dc2-94b5-5de52b373240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>lastName</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>park</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kim</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+--------+\n",
       "| id|firstName|lastName|\n",
       "+---+---------+--------+\n",
       "|  1|   suhyuk|    park|\n",
       "|  2|  youngmi|     kim|\n",
       "+---+---------+--------+"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"select * from {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b93f4eae-d02a-4e28-9d2d-e34c1591068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|id                          |int                                                             |       |\n",
      "|firstName                   |string                                                          |       |\n",
      "|middleName                  |string                                                          |       |\n",
      "|lastName                    |string                                                          |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Not partitioned             |                                                                |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.users                                                   |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/users                    |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1 예제를 다시 실행하고\n",
    "\n",
    "schema_v3 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"middleName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True)\n",
    "])\n",
    "rows_v3 = []\n",
    "rows_v3.append(Row(3, \"sowon\", \"eva\", \"park\"))\n",
    "rows_v3.append(Row(4, \"sihun\", \"sean\", \"park\"))\n",
    "df_v3 = spark.createDataFrame(rows_v3, schema_v3)\n",
    "df_v3.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "sql(\"show tables\")\n",
    "show(\"describe extended {}.{}\".format(dbName, tableName), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96e00dc3-1c78-45c3-bbdf-65645d2f3902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th></tr>\n",
       "<tr><td>4</td><td>sihun</td><td>sean</td><td>park</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+\n",
       "| id|firstName|middleName|lastName|\n",
       "+---+---------+----------+--------+\n",
       "|  4|    sihun|      sean|    park|\n",
       "|  3|    sowon|       eva|    park|\n",
       "+---+---------+----------+--------+"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"select * from {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c284f-07a2-4984-87d7-d00df42271f1",
   "metadata": {},
   "source": [
    "### Q5. `VACUUM` 실행 시에도 `RETAIN 100 HOURS` 와 같이 적용할 수 있는데 원 테이블 설정과 런타임 시의 설정 중에 어떤 것 기준으로 적용이 되는가?\n",
    "* 테이블 설정보다 높은 값이 아닌 경우 오류를 반환하게 됩니다\n",
    "* `spark.databricks.delta.retentionDurationCheck.enabled = false` 설정 시에는 지정이 가능하며 반드시 `RETAIN` 설정 값을 지정해야 합니다\n",
    "* 조회되는 가장 긴 기간 혹은 업데이트 가능성이 있는 시간 보다 긴 시간을 지정해야 데이터 유실을 막을 수 있습니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1c4fed4-b514-4ef7-ad7d-fcefcdaa07a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|id                          |int                                                             |       |\n",
      "|firstName                   |string                                                          |       |\n",
      "|lastName                    |string                                                          |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Part 0                      |lastName                                                        |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.users                                                   |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/users                    |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>lastName</th></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kim</td></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>park</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+--------+\n",
       "| id|firstName|lastName|\n",
       "+---+---------+--------+\n",
       "|  2|  youngmi|     kim|\n",
       "|  1|   suhyuk|    park|\n",
       "+---+---------+--------+"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropAndRemoveTable(dbName, tableName)\n",
    "\n",
    "schema_v4 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True)\n",
    "])\n",
    "rows_v4 = []\n",
    "rows_v4.append(Row(1, \"suhyuk\", \"park\"))\n",
    "rows_v4.append(Row(3, \"sowon\", \"park\"))\n",
    "rows_v4.append(Row(4, \"sean\", \"park\"))\n",
    "\n",
    "df_v4 = spark.createDataFrame(rows_v1, schema_v1)\n",
    "df_v4.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"lastName\").saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "show(\"describe extended {}.{}\".format(dbName, tableName), 100)\n",
    "sql(\"select * from {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13a31e23-b4df-432d-842c-77c46b0ce743",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_v4a = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"middleName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True)\n",
    "])\n",
    "rows_v4a = []\n",
    "rows_v4a.append(Row(2, \"youngmi\", \"kiki\", \"kim\"))\n",
    "df_v4a = spark.createDataFrame(rows_v4a, schema_v4a)\n",
    "df_v4a.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", True).partitionBy(\"lastName\").saveAsTable(\"{}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0027ea19-96a4-4a7e-9f6d-39d5f92d3a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|id                          |int                                                             |       |\n",
      "|firstName                   |string                                                          |       |\n",
      "|lastName                    |string                                                          |       |\n",
      "|middleName                  |string                                                          |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Part 0                      |lastName                                                        |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.users                                                   |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/users                    |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show(\"describe extended {}.{}\".format(dbName, tableName), 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
