{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa727ab-1c39-48f7-b13c-582deb136463",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6-2. Change Data Fedd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9879ddf1-1276-49d0-89b8-eec2b8df8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-11-openjdk-amd64\n",
      "/usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986ad10a-b2b6-4543-812e-b0f09f85b72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "from delta import *\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "warehouse_dir = f\"{work_dir}/spark-warehouse\"\n",
    "\n",
    "# Create spark session with hive enabled\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"pyspark-notebook\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_dir)\n",
    "    .enableHiveSupport()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5b931d-1ee5-4394-960f-33f131d4d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 델타 레이크 생성시에 반드시 `configure_spark_with_delta_pip` 구성을 통해 실행되어야 정상적인 델타 의존성이 로딩됩니다\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39abe029-89d4-473d-9f8c-e44146492879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.31.95.65:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5310fe43d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark.conf.set(\"spark.sql.decimalOperations.allowPrecisionLoss\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57b61f7-1ee2-4a9a-bb31-3663bca1953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(queries, num_rows = 20):\n",
    "    for query in queries.split(\";\"):\n",
    "        spark.sql(query).show(num_rows, truncate=False)\n",
    "\n",
    "def sql(query):\n",
    "    return spark.sql(query)\n",
    "\n",
    "def history(dbName, tableName):\n",
    "    return spark.sql(\"describe history {}.{}\".format(dbName, tableName))\n",
    "\n",
    "def table(dbName, tableName):\n",
    "    return spark.read.format(\"delta\").table(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "def describe(dbName, tableName, extended = True, num_rows = 20):\n",
    "    if extended:\n",
    "        show(\"describe extended {}.{}\".format(dbName, tableName), num_rows)\n",
    "    else:\n",
    "        show(\"describe {}.{}\".format(dbName, tableName), num_rows)\n",
    "\n",
    "def ls(target):\n",
    "    !ls -al {target}\n",
    "\n",
    "def ls_and_head(target, lineno):\n",
    "    !ls -al {target} | grep -v 'crc' | head -{lineno}\n",
    "\n",
    "def cat(filename):\n",
    "    !cat {filename}\n",
    "\n",
    "def grep(keyword, filename):\n",
    "    !grep -i {keyword} {filename}\n",
    "\n",
    "def grep_and_json(keyword, filename):\n",
    "    !grep {keyword} {filename} | python -m json.tool\n",
    "\n",
    "def grep_sed_json(keyword, lineno, filename):\n",
    "    !grep {keyword} {filename} | sed -n {lineno}p | python -m json.tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1b09c1-b493-4566-9705-46147a81c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "def dropAndRemoveTable(dbName, tableName):\n",
    "    location=\"/home/jovyan/work/spark-warehouse/{}\".format(tableName)\n",
    "    !rm -rf {location}\n",
    "    sql(\"DROP TABLE IF EXISTS {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e28e9b69-edc2-4936-9131-2a6952722f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>delta_v1</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>delta_v2</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>family</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>pusan_popular_trip</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>pusan_popular_zorder</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>users</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+--------------------+-----------+\n",
       "|namespace|           tableName|isTemporary|\n",
       "+---------+--------------------+-----------+\n",
       "|  default|            delta_v1|      false|\n",
       "|  default|            delta_v2|      false|\n",
       "|  default|              family|      false|\n",
       "|  default|  pusan_popular_trip|      false|\n",
       "|  default|pusan_popular_zorder|      false|\n",
       "|  default|               users|      false|\n",
       "+---------+--------------------+-----------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d91b1684-723f-4a56-9ee0-c5a1b0e210bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbName = \"taxidb\"\n",
    "tableName = \"tripAggregates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c208f9a7-5757-4e48-9b72-59b6e835fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropAndRemoveTable(dbName, tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae4df287-9162-4182-b143-1c101ae924a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {}.{} \n",
    "(VendorId INT, PassengerCount INT, FareAmount INT)\n",
    "USING DELTA\n",
    "TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99eb094a-7a81-4696-a760-5fc14e0dc0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorId</th><th>PassengerCount</th><th>FareAmount</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------+----------+\n",
       "|VendorId|PassengerCount|FareAmount|\n",
       "+--------+--------------+----------+\n",
       "+--------+--------------+----------+"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(f\"use {dbName}\")\n",
    "sql(\"show tables\")\n",
    "sql(f\"select * from {dbName}.{tableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d587e006-ba3c-449c-b6f2-6fbfa7e65870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(f\"ALTER TABLE {dbName}.{tableName} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7d48d7b-608a-47fa-ab8a-310e161a412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxrwxrwx 1 jovyan 1000 512 Nov  5 08:00 .\n",
      "drwxrwxrwx 1 jovyan 1000 512 Nov  5 08:00 ..\n",
      "drwxrwxrwx 1 jovyan 1000 512 Nov  5  2024 _delta_log\n"
     ]
    }
   ],
   "source": [
    "ls(f\"spark-warehouse/{dbName}.db/{tableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb06afb5-5f55-4ed2-93ce-9905143ec120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5  2024 .\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:00 ..\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5  2024 _change_data\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5  2024 _delta_log\n"
     ]
    }
   ],
   "source": [
    "sql(f\"INSERT INTO {dbName}.{tableName} VALUES (1, 500, 1000)\")\n",
    "sql(f\"UPDATE {dbName}.{tableName} SET FareAmount = 2500 WHERE VendorId = 1\")\n",
    "sql(f\"INSERT INTO {dbName}.{tableName} VALUES (4, 500, 1000)\")\n",
    "sql(f\"DELETE FROM {dbName}.{tableName} WHERE VendorId = 4\")\n",
    "ls_and_head(f\"spark-warehouse/{dbName}.db/{tableName}\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27438edc-2529-4b9e-a47a-1e90e05dce16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:05 .\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:05 ..\n",
      "-rwxrwxrwx 1 jovyan 1000 1354 Nov  5 08:05 cdc-00000-26b8c438-25d0-4d2b-9d3c-8e04a73c7b1f.c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000 1239 Nov  5 08:02 cdc-00000-b4814949-a19c-4a4c-b12f-3ad1a35421c3.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "ls_and_head(f\"spark-warehouse/{dbName}.db/{tableName}/_change_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3633a4aa-5b73-4c43-8c93-1eef2b1cfc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:05 .\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:05 ..\n",
      "-rwxrwxrwx 1 jovyan 1000  943 Nov  5 08:00 00000000000000000000.json\n",
      "-rwxrwxrwx 1 jovyan 1000  850 Nov  5 08:00 00000000000000000001.json\n"
     ]
    }
   ],
   "source": [
    "ls_and_head(f\"spark-warehouse/{dbName}.db/{tableName}/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f388675c-6cd2-4316-b294-f71ed2a1bfda",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "could not resolve `table_changes` to a table-valued function; line 3 pos 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mSELECT *\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43mFROM table_changes(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, 1, 4)\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43mORDER BY _commit_timestamp\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36msql\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(query):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: could not resolve `table_changes` to a table-valued function; line 3 pos 5"
     ]
    }
   ],
   "source": [
    "sql(\"\"\"\n",
    "SELECT *\n",
    "FROM table_changes('{}.{}', 1, 4)\n",
    "ORDER BY _commit_timestamp\n",
    "\"\"\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "421959bf-45a5-4742-b311-93df92c74ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "changeFeed = spark.read.format(\"delta\").option(\"readChangeFeed\", True).option(\"startingVersion\", 1).option(\"endingVersion\", 4).table(f\"{dbName}.{tableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d840b079-efa3-4808-bf58-07b55b2af063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorId: integer (nullable = true)\n",
      " |-- PassengerCount: integer (nullable = true)\n",
      " |-- FareAmount: integer (nullable = true)\n",
      " |-- _change_type: string (nullable = true)\n",
      " |-- _commit_version: long (nullable = true)\n",
      " |-- _commit_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "changeFeed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e67b9518-f440-4e01-9dac-96b3b6bc563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------+------------+---------------+-----------------------+\n",
      "|VendorId|PassengerCount|FareAmount|_change_type|_commit_version|_commit_timestamp      |\n",
      "+--------+--------------+----------+------------+---------------+-----------------------+\n",
      "|4       |500           |1000      |delete      |4              |2024-11-05 08:02:37.781|\n",
      "|4       |500           |1000      |delete      |4              |2024-11-05 08:02:37.781|\n",
      "|4       |500           |1000      |insert      |2              |2024-11-05 08:00:37.562|\n",
      "|4       |500           |1000      |insert      |3              |2024-11-05 08:02:35.48 |\n",
      "+--------+--------------+----------+------------+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "changeFeed.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff4cda89-bd81-43d2-8ef8-c26718d4c5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>8</td><td>2024-11-05 17:05:46.717</td><td>null</td><td>null</td><td>DELETE</td><td>{predicate -&gt; [&quot;(spark_catalog.taxidb.tripAggregates.VendorId = 4)&quot;]}</td><td>null</td><td>null</td><td>null</td><td>7</td><td>Serializable</td><td>false</td><td>{numRemovedFiles -&gt; 1, numCopiedRows -&gt; 0, numAddedChangeFiles -&gt; 1, executionTimeMs -&gt; 536, numD...</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>7</td><td>2024-11-05 17:05:44.988</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>6</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 968}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>6</td><td>2024-11-05 17:05:43.713</td><td>null</td><td>null</td><td>UPDATE</td><td>{predicate -&gt; (VendorId#3889 = 1)}</td><td>null</td><td>null</td><td>null</td><td>5</td><td>Serializable</td><td>false</td><td>{numRemovedFiles -&gt; 1, numCopiedRows -&gt; 0, numAddedChangeFiles -&gt; 1, executionTimeMs -&gt; 699, scan...</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>5</td><td>2024-11-05 17:05:41.841</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>4</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 968}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>4</td><td>2024-11-05 17:02:37.781</td><td>null</td><td>null</td><td>DELETE</td><td>{predicate -&gt; [&quot;(spark_catalog.taxidb.tripAggregates.VendorId = 4)&quot;]}</td><td>null</td><td>null</td><td>null</td><td>3</td><td>Serializable</td><td>false</td><td>{numRemovedFiles -&gt; 2, numCopiedRows -&gt; 0, numAddedChangeFiles -&gt; 2, executionTimeMs -&gt; 1169, num...</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>3</td><td>2024-11-05 17:02:35.48</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 968}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>2</td><td>2024-11-05 17:00:37.562</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>1</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 1, numOutputRows -&gt; 1, numOutputBytes -&gt; 968}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>1</td><td>2024-11-05 17:00:32.661</td><td>null</td><td>null</td><td>SET TBLPROPERTIES</td><td>{properties -&gt; {&quot;delta.enableChangeDataFeed&quot;:&quot;true&quot;}}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>true</td><td>{}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>0</td><td>2024-11-05 17:00:25.519</td><td>null</td><td>null</td><td>CREATE TABLE</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {&quot;delta.enableChangeDat...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>true</td><td>{}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------------------+------+--------+-----------------+----------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|              timestamp|userId|userName|        operation|                                                                                 operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                                                                    operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-----------------------+------+--------+-----------------+----------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
       "|      8|2024-11-05 17:05:46.717|  null|    null|           DELETE|                               {predicate -> [\"(spark_catalog.taxidb.tripAggregates.VendorId = 4)\"]}|null|    null|     null|          7|  Serializable|        false|{numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 1, executionTimeMs -> 536, numD...|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      7|2024-11-05 17:05:44.988|  null|    null|            WRITE|                                                                 {mode -> Append, partitionBy -> []}|null|    null|     null|          6|  Serializable|         true|                                          {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 968}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      6|2024-11-05 17:05:43.713|  null|    null|           UPDATE|                                                                  {predicate -> (VendorId#3889 = 1)}|null|    null|     null|          5|  Serializable|        false|{numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 1, executionTimeMs -> 699, scan...|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      5|2024-11-05 17:05:41.841|  null|    null|            WRITE|                                                                 {mode -> Append, partitionBy -> []}|null|    null|     null|          4|  Serializable|         true|                                          {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 968}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      4|2024-11-05 17:02:37.781|  null|    null|           DELETE|                               {predicate -> [\"(spark_catalog.taxidb.tripAggregates.VendorId = 4)\"]}|null|    null|     null|          3|  Serializable|        false|{numRemovedFiles -> 2, numCopiedRows -> 0, numAddedChangeFiles -> 2, executionTimeMs -> 1169, num...|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      3| 2024-11-05 17:02:35.48|  null|    null|            WRITE|                                                                 {mode -> Append, partitionBy -> []}|null|    null|     null|          2|  Serializable|         true|                                          {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 968}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      2|2024-11-05 17:00:37.562|  null|    null|            WRITE|                                                                 {mode -> Append, partitionBy -> []}|null|    null|     null|          1|  Serializable|         true|                                          {numFiles -> 1, numOutputRows -> 1, numOutputBytes -> 968}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      1|2024-11-05 17:00:32.661|  null|    null|SET TBLPROPERTIES|                                               {properties -> {\"delta.enableChangeDataFeed\":\"true\"}}|null|    null|     null|          0|  Serializable|         true|                                                                                                  {}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      0|2024-11-05 17:00:25.519|  null|    null|     CREATE TABLE|{isManaged -> true, description -> null, partitionBy -> [], properties -> {\"delta.enableChangeDat...|null|    null|     null|       null|  Serializable|         true|                                                                                                  {}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-----------------------+------+--------+-----------------+----------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history(dbName, tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d2dd3eb-9da1-48e0-9b68-86397f35abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------+----------------+---------------+-----------------------+\n",
      "|VendorId|PassengerCount|FareAmount|_change_type    |_commit_version|_commit_timestamp      |\n",
      "+--------+--------------+----------+----------------+---------------+-----------------------+\n",
      "|4       |500           |1000      |insert          |2              |2024-11-05 08:00:37.562|\n",
      "|4       |500           |1000      |insert          |3              |2024-11-05 08:02:35.48 |\n",
      "|4       |500           |1000      |delete          |4              |2024-11-05 08:02:37.781|\n",
      "|4       |500           |1000      |delete          |4              |2024-11-05 08:02:37.781|\n",
      "|1       |500           |1000      |insert          |5              |2024-11-05 08:05:41.841|\n",
      "|1       |500           |1000      |update_preimage |6              |2024-11-05 08:05:43.713|\n",
      "|1       |500           |2500      |update_postimage|6              |2024-11-05 08:05:43.713|\n",
      "|4       |500           |1000      |insert          |7              |2024-11-05 08:05:44.988|\n",
      "|4       |500           |1000      |delete          |8              |2024-11-05 08:05:46.717|\n",
      "+--------+--------------+----------+----------------+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "changes = spark.read.format(\"delta\").option(\"readChangeFeed\", True).option(\"startingVersion\", 0).option(\"endingVersion\", 100).table(f\"{dbName}.{tableName}\")\n",
    "changes.orderBy(asc(\"_commit_timestamp\")).show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "324455ee-2b62-449a-94fb-73bb522b92bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:05 .\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:00 ..\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:05 _change_data\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:05 _delta_log\n",
      "-rwxrwxrwx 1 jovyan 1000 1185 Nov  5 08:05 part-00000-0b48e892-6353-4d8d-b156-d4aa897125e4.c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  968 Nov  5 08:05 part-00000-17099711-3cc6-4848-a0dc-00b47f5c4d8d-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  968 Nov  5 08:05 part-00000-465ec638-994d-42a1-831e-3cd9b2d37642-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  968 Nov  5 08:02 part-00000-6330f8a7-eed5-4186-903b-639d06fe7cec-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  968 Nov  5 08:00 part-00000-7cd2c96a-b44a-431b-8691-11d38d341a59-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "source=f\"spark-warehouse/{dbName}.db/{tableName}\"\n",
    "ls_and_head(source, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7963c400-b2a4-477b-90cc-dbaef76e072e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------+\n",
      "|VendorId|PassengerCount|FareAmount|\n",
      "+--------+--------------+----------+\n",
      "|1       |500           |2500      |\n",
      "+--------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbName = \"taxidb\"\n",
    "tableName = \"tripAggregates\"\n",
    "\n",
    "show(f\"select * from {dbName}.{tableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a741f9d-8adb-40d7-8cac-81d5ce22b3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(f\"INSERT INTO {dbName}.{tableName} VALUES (2, 1000, 2000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb744d5b-2e1f-4bac-a370-71912ddc476f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(f\"INSERT INTO {dbName}.{tableName} VALUES (3, 3000, 3000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1812146d-1dff-48bd-b715-63cc367da18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorId</th><th>PassengerCount</th><th>FareAmount</th><th>RecordStreamTime</th></tr>\n",
       "<tr><td>3</td><td>3000</td><td>3000</td><td>2024-11-05 18:16:13.737</td></tr>\n",
       "<tr><td>2</td><td>1000</td><td>2000</td><td>2024-11-05 18:15:54.323</td></tr>\n",
       "<tr><td>1</td><td>500</td><td>2500</td><td>2024-11-05 17:34:00.304</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------+----------+-----------------------+\n",
       "|VendorId|PassengerCount|FareAmount|       RecordStreamTime|\n",
       "+--------+--------------+----------+-----------------------+\n",
       "|       3|          3000|      3000|2024-11-05 18:16:13.737|\n",
       "|       2|          1000|      2000|2024-11-05 18:15:54.323|\n",
       "|       1|           500|      2500|2024-11-05 17:34:00.304|\n",
       "+--------+--------------+----------+-----------------------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbName = \"taxidb\"\n",
    "targetTable = \"streamTarget\"\n",
    "target=f\"spark-warehouse/{dbName}.db/{targetTable}\"\n",
    "spark.sql(f\"select * from delta.`/home/jovyan/work/{target}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87fa218-1160-4a20-bc44-2fbd16788f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
