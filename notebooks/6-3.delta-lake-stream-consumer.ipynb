{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5a3cd9-0120-455f-8d4b-26e67d2739e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-11-openjdk-amd64\n",
      "/usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a90e4f3-a9e2-4afd-83d0-efa79bc0fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "from delta import *\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "warehouse_dir = f\"{work_dir}/spark-warehouse\"\n",
    "\n",
    "# Create spark session with hive enabled\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"pyspark-notebook\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_dir)\n",
    "    .enableHiveSupport()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d379cd-3129-4c1f-ad8a-8ef6c0e22ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 델타 레이크 생성시에 반드시 `configure_spark_with_delta_pip` 구성을 통해 실행되어야 정상적인 델타 의존성이 로딩됩니다\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823b8796-ba08-4a2c-83d1-3f0e3b4463d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.31.95.65:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f49e8bc93d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark.conf.set(\"spark.sql.decimalOperations.allowPrecisionLoss\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1032d9-1d90-4807-b8c9-c0e42e3e763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(queries, num_rows = 20):\n",
    "    for query in queries.split(\";\"):\n",
    "        spark.sql(query).show(num_rows, truncate=False)\n",
    "\n",
    "def sql(query):\n",
    "    return spark.sql(query)\n",
    "\n",
    "def history(dbName, tableName):\n",
    "    return spark.sql(\"describe history {}.{}\".format(dbName, tableName))\n",
    "\n",
    "def table(dbName, tableName):\n",
    "    return spark.read.format(\"delta\").table(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "def describe(dbName, tableName, extended = True, num_rows = 20):\n",
    "    if extended:\n",
    "        show(\"describe extended {}.{}\".format(dbName, tableName), num_rows)\n",
    "    else:\n",
    "        show(\"describe {}.{}\".format(dbName, tableName), num_rows)\n",
    "\n",
    "def ls(target):\n",
    "    !ls -al {target}\n",
    "\n",
    "def ls_and_head(target, lineno):\n",
    "    !ls -al {target} | grep -v 'crc' | head -{lineno}\n",
    "\n",
    "def cat(filename):\n",
    "    !cat {filename}\n",
    "\n",
    "def grep(keyword, filename):\n",
    "    !grep -i {keyword} {filename}\n",
    "\n",
    "def grep_and_json(keyword, filename):\n",
    "    !grep {keyword} {filename} | python -m json.tool\n",
    "\n",
    "def grep_sed_json(keyword, lineno, filename):\n",
    "    !grep {keyword} {filename} | sed -n {lineno}p | python -m json.tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba2acef-ef0f-426d-9c0d-f09ef6125615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "def dropAndRemoveTable(dbName, tableName):\n",
    "    location=\"/home/jovyan/work/spark-warehouse/{}\".format(tableName)\n",
    "    !rm -rf {location}\n",
    "    sql(\"DROP TABLE IF EXISTS {}.{}\".format(dbName, tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284a578a-1535-4850-8cb4-e025d5f06bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbName = \"taxidb\"\n",
    "sourceTable = \"tripAggregates\"\n",
    "targetTable = \"streamTarget\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e0a65f-0c00-489d-b4e7-3a074b22d84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:05 .\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:34 ..\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:05 _change_data\n",
      "drwxrwxrwx 1 jovyan 1000  512 Nov  5 08:05 _delta_log\n",
      "-rwxrwxrwx 1 jovyan 1000 1185 Nov  5 08:05 part-00000-0b48e892-6353-4d8d-b156-d4aa897125e4.c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  968 Nov  5 08:05 part-00000-17099711-3cc6-4848-a0dc-00b47f5c4d8d-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  968 Nov  5 08:05 part-00000-465ec638-994d-42a1-831e-3cd9b2d37642-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  968 Nov  5 08:02 part-00000-6330f8a7-eed5-4186-903b-639d06fe7cec-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 jovyan 1000  968 Nov  5 08:00 part-00000-7cd2c96a-b44a-431b-8691-11d38d341a59-c000.snappy.parquet\n",
      "root\n",
      " |-- VendorId: integer (nullable = true)\n",
      " |-- PassengerCount: integer (nullable = true)\n",
      " |-- FareAmount: integer (nullable = true)\n",
      " |-- RecordStreamTime: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source=f\"spark-warehouse/{dbName}.db/{sourceTable}\"\n",
    "target=f\"spark-warehouse/{dbName}.db/{targetTable}\"\n",
    "chkpnt=f\"{target}/_checkpoint\"\n",
    "ls_and_head(source, 10)\n",
    "\n",
    "stream_df = spark.readStream.format(\"delta\").load(source)\n",
    "stream_df = stream_df.withColumn(\"RecordStreamTime\", current_timestamp())\n",
    "stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad4c118d-385d-4e96-b25c-edeaaf58b297",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StreamingQuery' object has no attribute 'awaitTerminaion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m stream_df \u001b[38;5;241m=\u001b[39m stream_df\u001b[38;5;241m.\u001b[39mselect(select_columns)\n\u001b[1;32m      5\u001b[0m stream_query \u001b[38;5;241m=\u001b[39m stream_df\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, chkpnt)\u001b[38;5;241m.\u001b[39mstart(target)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mstream_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTerminaion\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StreamingQuery' object has no attribute 'awaitTerminaion'"
     ]
    }
   ],
   "source": [
    "select_columns = [\n",
    "'VendorId', 'PassengerCount', 'FareAmount', 'RecordStreamTime'\n",
    "]\n",
    "stream_df = stream_df.select(select_columns)\n",
    "stream_query = stream_df.writeStream.format(\"delta\").option(\"checkpointLocation\", chkpnt).start(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbd61221-86df-4fdd-82ab-803d5fb41c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstream_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py:101\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:475\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    476\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stream_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "978371fe-df45-49b9-af7e-45a5fd8114d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>VendorId</td><td>int</td><td></td></tr>\n",
       "<tr><td>PassengerCount</td><td>int</td><td></td></tr>\n",
       "<tr><td>FareAmount</td><td>int</td><td></td></tr>\n",
       "<tr><td>RecordStreamTime</td><td>timestamp</td><td></td></tr>\n",
       "<tr><td></td><td></td><td></td></tr>\n",
       "<tr><td># Partitioning</td><td></td><td></td></tr>\n",
       "<tr><td>Not partitioned</td><td></td><td></td></tr>\n",
       "<tr><td></td><td></td><td></td></tr>\n",
       "<tr><td># Detailed Table Information</td><td></td><td></td></tr>\n",
       "<tr><td>Name</td><td>delta.`file:/home/jovyan/work/spark-warehouse/taxidb.db/streamTarget`</td><td></td></tr>\n",
       "<tr><td>Location</td><td>/home/jovyan/work/spark-warehouse/taxidb.db/streamTarget</td><td></td></tr>\n",
       "<tr><td>Provider</td><td>delta</td><td></td></tr>\n",
       "<tr><td>Table Properties</td><td>[delta.minReaderVersion=1,delta.minWriterVersion=2]</td><td></td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------------+---------------------------------------------------------------------+-------+\n",
       "|                    col_name|                                                            data_type|comment|\n",
       "+----------------------------+---------------------------------------------------------------------+-------+\n",
       "|                    VendorId|                                                                  int|       |\n",
       "|              PassengerCount|                                                                  int|       |\n",
       "|                  FareAmount|                                                                  int|       |\n",
       "|            RecordStreamTime|                                                            timestamp|       |\n",
       "|                            |                                                                     |       |\n",
       "|              # Partitioning|                                                                     |       |\n",
       "|             Not partitioned|                                                                     |       |\n",
       "|                            |                                                                     |       |\n",
       "|# Detailed Table Information|                                                                     |       |\n",
       "|                        Name|delta.`file:/home/jovyan/work/spark-warehouse/taxidb.db/streamTarget`|       |\n",
       "|                    Location|             /home/jovyan/work/spark-warehouse/taxidb.db/streamTarget|       |\n",
       "|                    Provider|                                                                delta|       |\n",
       "|            Table Properties|                  [delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
       "+----------------------------+---------------------------------------------------------------------+-------+"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"describe extended delta.`/home/jovyan/work/{target}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e139f-715f-42ea-be86-2f6b13c380ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
