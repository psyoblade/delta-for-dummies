{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9879ddf1-1276-49d0-89b8-eec2b8df8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-11-openjdk-amd64\n",
      "/usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986ad10a-b2b6-4543-812e-b0f09f85b72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "from delta import *\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "warehouse_dir = f\"{work_dir}/spark-warehouse\"\n",
    "\n",
    "# Create spark session with hive enabled\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"pyspark-notebook\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_dir)\n",
    "    .enableHiveSupport()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5b931d-1ee5-4394-960f-33f131d4d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 델타 레이크 생성시에 반드시 `configure_spark_with_delta_pip` 구성을 통해 실행되어야 정상적인 델타 의존성이 로딩됩니다\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39abe029-89d4-473d-9f8c-e44146492879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c7524b0ae2cc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f86d9e9d410>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark.conf.set(\"spark.sql.decimalOperations.allowPrecisionLoss\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57b61f7-1ee2-4a9a-bb31-3663bca1953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(queries, num_rows = 20):\n",
    "    for query in queries.split(\";\"):\n",
    "        spark.sql(query).show(num_rows, truncate=False)\n",
    "\n",
    "def sql(query):\n",
    "    return spark.sql(query)\n",
    "\n",
    "def ls(command):\n",
    "    !ls -al {command}\n",
    "\n",
    "def cat(filename):\n",
    "    !cat {filename}\n",
    "\n",
    "def grep(keyword, filename):\n",
    "    !grep -i {keyword} {filename}\n",
    "\n",
    "def grep_and_json(keyword, filename):\n",
    "    !grep {keyword} {filename} | python -m json.tool\n",
    "\n",
    "def grep_sed_json(keyword, lineno, filename):\n",
    "    !grep {keyword} {filename} | sed -n {lineno}p | python -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320ec102-2df5-48a2-9be5-581dd11cddfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicated_rate_card = spark.read.csv(\"data/duplicatedRateCard.csv\")\n",
    "duplicated_rate_card.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e28e9b69-edc2-4936-9131-2a6952722f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = duplicated_rate_card.where(expr(\"_c0 < 7\"))\n",
    "not_dup = duplicated_rate_card.where(expr(\"_c0 >= 7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fef70ac2-a72d-476c-bbb6-8204ec7cd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_dup.write.mode(\"overwrite\").partitionBy(\"_c0\").saveAsTable(\"default.rate_card_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2be36a30-f17c-47c8-8a18-a698b7f4d22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|  _c1|_c0|\n",
      "+-----+---+\n",
      "|Seoul|  7|\n",
      "|Pusan|  8|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>rate_card_v1</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>users</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>users_cluster_by</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+----------------+-----------+\n",
       "|namespace|       tableName|isTemporary|\n",
       "+---------+----------------+-----------+\n",
       "|  default|    rate_card_v1|      false|\n",
       "|  default|           users|      false|\n",
       "|  default|users_cluster_by|      false|\n",
       "+---------+----------------+-----------+"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from default.rate_card_v1\").show()\n",
    "spark.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5c71ec5-b9ab-404d-90c8-928ee469ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup.distinct().write.mode(\"append\").partitionBy(\"_c0\").saveAsTable(\"default.rate_card_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eaa7c4d8-22fd-47fd-84ed-64c2a233a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                 _c1|_c0|\n",
      "+--------------------+---+\n",
      "|Nassau or Westche...|  4|\n",
      "|     Negotiated fare|  5|\n",
      "|       Standard Rate|  1|\n",
      "|          Group ride|  6|\n",
      "|              Newark|  3|\n",
      "|               Seoul|  7|\n",
      "|               Pusan|  8|\n",
      "|                 JFK|  2|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>rate_card_v1</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>users</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>users_cluster_by</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+----------------+-----------+\n",
       "|namespace|       tableName|isTemporary|\n",
       "+---------+----------------+-----------+\n",
       "|  default|    rate_card_v1|      false|\n",
       "|  default|           users|      false|\n",
       "|  default|users_cluster_by|      false|\n",
       "+---------+----------------+-----------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from default.rate_card_v1\").show()\n",
    "spark.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e567b043-90d3-4067-8bd8-f138ca14a3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c1</th><th>_c0</th></tr>\n",
       "<tr><td>Standard Rate</td><td>1</td></tr>\n",
       "<tr><td>JFK</td><td>2</td></tr>\n",
       "<tr><td>Newark</td><td>3</td></tr>\n",
       "<tr><td>Nassau or Westchester</td><td>4</td></tr>\n",
       "<tr><td>Negotiated fare</td><td>5</td></tr>\n",
       "<tr><td>Group ride</td><td>6</td></tr>\n",
       "<tr><td>Seoul</td><td>7</td></tr>\n",
       "<tr><td>Pusan</td><td>8</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------------------+---+\n",
       "|                  _c1|_c0|\n",
       "+---------------------+---+\n",
       "|        Standard Rate|  1|\n",
       "|                  JFK|  2|\n",
       "|               Newark|  3|\n",
       "|Nassau or Westchester|  4|\n",
       "|      Negotiated fare|  5|\n",
       "|           Group ride|  6|\n",
       "|                Seoul|  7|\n",
       "|                Pusan|  8|\n",
       "+---------------------+---+"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from default.rate_card_v1 order by _c0 asc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d66ac54-60d7-41ec-bed8-8304d71985e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "def dropAndRemoveTable(dbName, tableName):\n",
    "    location=\"/home/jovyan/work/spark-warehouse/{}\".format(tableName)\n",
    "    !rm -rf {location}\n",
    "    query(\"DROP TABLE IF EXISTS {}.{}\".format(dbName, tableName))\n",
    "\n",
    "    \n",
    "def createFamilyMembers(dbName, tableName):\n",
    "    # 예제 데이터 생성 및 히스토리 테스트\n",
    "    tableSchema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"firstName\", StringType(), True),\n",
    "        StructField(\"middleName\", StringType(), True),\n",
    "        StructField(\"lastName\", StringType(), True),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"birthDate\", StringType(), True),\n",
    "        StructField(\"ssn\", StringType(), True),\n",
    "        StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    tableRows = []\n",
    "    tableRows.append(Row(1, \"suhyuk\", \"psyoblade\", \"park\", \"male\", \"2000/10/30\", \"741030\", 1000))\n",
    "    tableRows.append(Row(2, \"youngmi\", \"kiki\", \"kim\", \"female\", \"2004/08/08\", \"770808\", 2000))\n",
    "    tableRows.append(Row(3, \"sowon\", \"eva\", \"park\", \"female\", \"2005/05/20\", \"040520\", 3000))\n",
    "    tableRows.append(Row(4, \"sihun\", \"sean\", \"park\", \"male\", \"2006/01/14\", \"080114\", 4000))\n",
    "\n",
    "    df = spark.createDataFrame(tableRows, tableSchema)\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"salary\").saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "\n",
    "dropAndRemoveTable(\"default\", \"users\")\n",
    "createFamilyMembers(\"default\", \"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e286553f-b673-476a-b5cf-6f0f93a26b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td><td>female</td><td>2005/05/20</td><td>040520</td><td>3000</td></tr>\n",
       "<tr><td>4</td><td>sihun</td><td>sean</td><td>park</td><td>male</td><td>2006/01/14</td><td>080114</td><td>4000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
       "|  4|    sihun|      sean|    park|  male|2006/01/14|080114|  4000|\n",
       "+---+---------+----------+--------+------+----------+------+------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from default.users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "282f7630-40d8-4e60-b094-501680c4591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNewFamilyMembers(dbName, tableName):\n",
    "    # 예제 데이터 생성 및 히스토리 테스트\n",
    "    nameSchema = StructType([\n",
    "        StructField(\"firstName\", StringType(), True),\n",
    "        StructField(\"middleName\", StringType(), True),\n",
    "        StructField(\"lastName\", StringType(), True)\n",
    "    ])\n",
    "    tableSchema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", nameSchema, True),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"birthDate\", StringType(), True),\n",
    "        StructField(\"ssn\", StringType(), True),\n",
    "        StructField(\"salary\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    tableRows = []\n",
    "    tableRows.append(Row(1, (\"suhyuk\", \"psyoblade\", \"park\"), \"male\", \"2000/10/30\", \"741030\", 1000))\n",
    "    tableRows.append(Row(2, (\"youngmi\", \"kiki\", \"kim\"), \"female\", \"2004/08/08\", \"770808\", 2000))\n",
    "    tableRows.append(Row(3, (\"sowon\", \"eva\", \"park\"), \"female\", \"2005/05/20\", \"040520\", 3000))\n",
    "    tableRows.append(Row(4, (\"sihun\", \"sean\", \"park\"), \"male\", \"2006/01/14\", \"080114\", 4000))\n",
    "\n",
    "    df = spark.createDataFrame(tableRows, tableSchema)\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"salary\").saveAsTable(\"{}.{}\".format(dbName, tableName))\n",
    "\n",
    "\n",
    "dropAndRemoveTable(\"default\", \"family\")\n",
    "createNewFamilyMembers(\"default\", \"family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa47704-c86b-4775-8d51-fb701d25bbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>firstName</th><th>lastName</th></tr>\n",
       "<tr><td>suhyuk</td><td>park</td></tr>\n",
       "<tr><td>youngmi</td><td>kim</td></tr>\n",
       "<tr><td>sowon</td><td>park</td></tr>\n",
       "<tr><td>sihun</td><td>park</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+--------+\n",
       "|firstName|lastName|\n",
       "+---------+--------+\n",
       "|   suhyuk|    park|\n",
       "|  youngmi|     kim|\n",
       "|    sowon|    park|\n",
       "|    sihun|    park|\n",
       "+---------+--------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select name.firstName, name.lastName from default.family\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10bf1a-3033-4425-a5fc-1b85cf8a9488",
   "metadata": {},
   "source": [
    "## Update table schema\n",
    "> 구문 중에 표준 `Spark SQL` 구문이 아닌 `Delta lake` 구문의 경우는 동작하지 않기 때문에 `--` 와 같이 `SQL` 코멘트로 표현했습니\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4771e71-a6d5-40fb-aa24-f7fc5014949f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add columns\n",
    "```sql\n",
    "-- 컬럼 추가는 잘 동작함\n",
    "ALTER TABLE table_name ADD COLUMNS (col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name], ...)\n",
    "ALTER TABLE table_name ADD COLUMNS (col_name.nested_col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name], ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb8e8cc4-9f31-4837-8850-89e38a31b177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALTER TABLE table_name ADD COLUMNS (col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name], ...)\n",
    "query(\"\"\"\n",
    "alter table users add columns (description string comment 'description_of_member')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "493735f6-13f4-475a-8971-ecc3e3326ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>id</td><td>int</td><td></td></tr>\n",
       "<tr><td>firstName</td><td>string</td><td></td></tr>\n",
       "<tr><td>middleName</td><td>string</td><td></td></tr>\n",
       "<tr><td>lastName</td><td>string</td><td></td></tr>\n",
       "<tr><td>gender</td><td>string</td><td></td></tr>\n",
       "<tr><td>birthDate</td><td>string</td><td></td></tr>\n",
       "<tr><td>ssn</td><td>string</td><td></td></tr>\n",
       "<tr><td>salary</td><td>int</td><td></td></tr>\n",
       "<tr><td>description</td><td>string</td><td>description_of_member</td></tr>\n",
       "<tr><td></td><td></td><td></td></tr>\n",
       "<tr><td># Partitioning</td><td></td><td></td></tr>\n",
       "<tr><td>Part 0</td><td>salary</td><td></td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------+---------+---------------------+\n",
       "|      col_name|data_type|              comment|\n",
       "+--------------+---------+---------------------+\n",
       "|            id|      int|                     |\n",
       "|     firstName|   string|                     |\n",
       "|    middleName|   string|                     |\n",
       "|      lastName|   string|                     |\n",
       "|        gender|   string|                     |\n",
       "|     birthDate|   string|                     |\n",
       "|           ssn|   string|                     |\n",
       "|        salary|      int|                     |\n",
       "|   description|   string|description_of_member|\n",
       "|              |         |                     |\n",
       "|# Partitioning|         |                     |\n",
       "|        Part 0|   salary|                     |\n",
       "+--------------+---------+---------------------+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"describe users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "952289db-b02a-4609-8719-2eab1f858f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALTER TABLE table_name ADD COLUMNS (col_name.nested_col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name], ...)\n",
    "query(\"\"\"\n",
    "alter table family add columns (name.description string comment 'description_of_name')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41fb768b-9820-4b76-8430-2bc704efd5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>id</td><td>int</td><td></td></tr>\n",
       "<tr><td>name</td><td>struct&lt;firstName:string,middleName:string,lastName:string,description:string&gt;</td><td></td></tr>\n",
       "<tr><td>gender</td><td>string</td><td></td></tr>\n",
       "<tr><td>birthDate</td><td>string</td><td></td></tr>\n",
       "<tr><td>ssn</td><td>string</td><td></td></tr>\n",
       "<tr><td>salary</td><td>int</td><td></td></tr>\n",
       "<tr><td></td><td></td><td></td></tr>\n",
       "<tr><td># Partitioning</td><td></td><td></td></tr>\n",
       "<tr><td>Part 0</td><td>salary</td><td></td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------+-----------------------------------------------------------------------------+-------+\n",
       "|      col_name|                                                                    data_type|comment|\n",
       "+--------------+-----------------------------------------------------------------------------+-------+\n",
       "|            id|                                                                          int|       |\n",
       "|          name|struct<firstName:string,middleName:string,lastName:string,description:string>|       |\n",
       "|        gender|                                                                       string|       |\n",
       "|     birthDate|                                                                       string|       |\n",
       "|           ssn|                                                                       string|       |\n",
       "|        salary|                                                                          int|       |\n",
       "|              |                                                                             |       |\n",
       "|# Partitioning|                                                                             |       |\n",
       "|        Part 0|                                                                       salary|       |\n",
       "+--------------+-----------------------------------------------------------------------------+-------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"describe family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b62a5359-13f4-4a9c-a212-6727b1ddcfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>info_name</th><th>info_value</th></tr>\n",
       "<tr><td>col_name</td><td>name</td></tr>\n",
       "<tr><td>data_type</td><td>struct&lt;firstName:string,middleName:string,lastName:string,description:string&gt;</td></tr>\n",
       "<tr><td>comment</td><td>NULL</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+-----------------------------------------------------------------------------+\n",
       "|info_name|                                                                   info_value|\n",
       "+---------+-----------------------------------------------------------------------------+\n",
       "| col_name|                                                                         name|\n",
       "|data_type|struct<firstName:string,middleName:string,lastName:string,description:string>|\n",
       "|  comment|                                                                         NULL|\n",
       "+---------+-----------------------------------------------------------------------------+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-ref-syntax-aux-describe-table.html\n",
    "query(\"describe extended family default.family.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85a22e01-dbc3-412c-8c31-5b3161f7fbaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|  default|       family|      false|\n",
      "|  default|      student|      false|\n",
      "|  default|student_delta|      false|\n",
      "|  default|        users|      false|\n",
      "+---------+-------------+-----------+\n",
      "\n",
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                 |comment|\n",
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "|id                          |int                                                       |null   |\n",
      "|name                        |string                                                    |null   |\n",
      "|age                         |int                                                       |null   |\n",
      "|                            |                                                          |       |\n",
      "|# Detailed Table Information|                                                          |       |\n",
      "|Database                    |default                                                   |       |\n",
      "|Table                       |student                                                   |       |\n",
      "|Owner                       |root                                                      |       |\n",
      "|Created Time                |Sat Feb 04 06:16:00 UTC 2023                              |       |\n",
      "|Last Access                 |UNKNOWN                                                   |       |\n",
      "|Created By                  |Spark 3.2.1                                               |       |\n",
      "|Type                        |MANAGED                                                   |       |\n",
      "|Provider                    |hive                                                      |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1675491360]                        |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/student            |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe        |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                  |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat|       |\n",
      "|Storage Properties          |[serialization.format=1]                                  |       |\n",
      "|Partition Provider          |Catalog                                                   |       |\n",
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|id                          |int                                                             |       |\n",
      "|name                        |string                                                          |       |\n",
      "|age                         |int                                                             |       |\n",
      "|                            |                                                                |       |\n",
      "|# Partitioning              |                                                                |       |\n",
      "|Not partitioned             |                                                                |       |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Name                        |default.student_delta                                           |       |\n",
      "|Location                    |file:/home/jovyan/work/spark-warehouse/student_delta            |       |\n",
      "|Provider                    |delta                                                           |       |\n",
      "|Owner                       |root                                                            |       |\n",
      "|Table Properties            |[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(\"show tables\").show()\n",
    "query(\"desc extended student\").show(truncate=False)\n",
    "query(\"desc extended student_delta\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ddecae-6a5a-43fa-9832-dd43d9a80600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "898e766b-b171-4378-a78a-41e48d8e92f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Change column comment or ordering\n",
    "```sql\n",
    "-- 컬럼 타입 변경구분 경우에도 여러 컬럼 드랍이 필요하여 수행이 안 되는 것으로 추정되며 동작하지 않음\n",
    "-- ALTER TABLE table_name ALTER [COLUMN] col_name col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name]\n",
    "-- ALTER TABLE table_name ALTER [COLUMN] col_name.nested_col_name nested_col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name]\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aebba9-e92d-4c47-b935-1ee7d7bf4fa9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Replace columns\n",
    "> [Support Spark’s column drop and rename commands #732](https://github.com/delta-io/delta/issues/732) 에 따르면 현재 스파크가 지원하는 컬럼 드랍을 델타 SQL 구문에서 지원하지 않음\n",
    "```sql\n",
    "-- 여러 컬럼을 한 번에 드랍하는 기능이 델타 SQL 구문에서 동작하지 않음\n",
    "ALTER TABLE table_name REPLACE COLUMNS (col_name1 col_type1 [COMMENT col_comment1], ...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6bdaac2-279c-428c-9a7b-97a9cba386ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "REPLACE COLUMNS is only supported with v2 tables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malter table student replace columns (user_id int, user_name string, user_age long)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: REPLACE COLUMNS is only supported with v2 tables."
     ]
    }
   ],
   "source": [
    "# 일반 테이블의 경우 AnalysisException: REPLACE COLUMNS is only supported with v2 tables. 와 같은 오류 발생\n",
    "query(\"alter table student replace columns (user_id int, user_name string, user_age long)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55146bc5-5676-4063-b5a1-265e0a55f56a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "DROP COLUMN is not supported for your Delta table. \nPlease upgrade your Delta table to reader version 2 and writer version 5\n and change the column mapping mode to 'name' mapping. You can use the following command:\n\n ALTER TABLE <table_name> SET TBLPROPERTIES (\n   'delta.columnMapping.mode' = 'name',\n   'delta.minReaderVersion' = '2',\n   'delta.minWriterVersion' = '5')\n\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 델타 테이블의 경우 AnalysisException: DROP COLUMN is not supported for your Delta table. 와 같은 오류 발생\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malter table student_delta replace columns (user_id int, user_name string, user_age long)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mAnalysisException: DROP COLUMN is not supported for your Delta table. \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mPlease upgrade your Delta table to reader version 2 and writer version 5\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m   'delta.minWriterVersion' = '5')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: DROP COLUMN is not supported for your Delta table. \nPlease upgrade your Delta table to reader version 2 and writer version 5\n and change the column mapping mode to 'name' mapping. You can use the following command:\n\n ALTER TABLE <table_name> SET TBLPROPERTIES (\n   'delta.columnMapping.mode' = 'name',\n   'delta.minReaderVersion' = '2',\n   'delta.minWriterVersion' = '5')\n\n    "
     ]
    }
   ],
   "source": [
    "# 델타 테이블의 경우 AnalysisException: DROP COLUMN is not supported for your Delta table. 와 같은 오류 발생\n",
    "query(\"alter table student_delta replace columns (user_id int, user_name string, user_age long)\")\n",
    "\n",
    "\"\"\"\n",
    "AnalysisException: DROP COLUMN is not supported for your Delta table. \n",
    "Please upgrade your Delta table to reader version 2 and writer version 5\n",
    " and change the column mapping mode to 'name' mapping. You can use the following command:\n",
    "\n",
    "ALTER TABLE <table_name> SET TBLPROPERTIES (\n",
    "   'delta.columnMapping.mode' = 'name',\n",
    "   'delta.minReaderVersion' = '2',\n",
    "   'delta.minWriterVersion' = '5')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b52cc39-a6d2-4f79-a77a-552b77806426",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot drop column from a struct type with a single field: StructType(StructField(age,IntegerType,true))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 아래와 같이 버전을 변경하면 AnalysisException: Cannot drop column from a struct type with a single field: StructType(StructField(age,IntegerType,true)) 오류 발생\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# [Support Spark’s column drop and rename commands #732](https://github.com/delta-io/delta/issues/732)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mALTER TABLE student_delta SET TBLPROPERTIES (\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta.columnMapping.mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta.minReaderVersion\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta.minWriterVersion\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malter table student_delta replace columns (user_id int, user_name string, user_age long)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot drop column from a struct type with a single field: StructType(StructField(age,IntegerType,true))"
     ]
    }
   ],
   "source": [
    "# 아래와 같이 버전을 변경하면 AnalysisException: Cannot drop column from a struct type with a single field: StructType(StructField(age,IntegerType,true)) 오류 발생\n",
    "query(\"\"\"\n",
    "ALTER TABLE student_delta SET TBLPROPERTIES (\n",
    "   'delta.columnMapping.mode' = 'name',\n",
    "   'delta.minReaderVersion' = '2',\n",
    "   'delta.minWriterVersion' = '5')\n",
    "\"\"\")\n",
    "\n",
    "query(\"alter table student_delta replace columns (user_id int, user_name string, user_age long)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ae372-c41b-46ae-8043-d3eadd589f91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Rename columns\n",
    "```sql\n",
    "-- 일반 컬럼은 낮은 버전에서도 사용 가능 (minReaderVersion=1, minWriterVersion=2)\n",
    "ALTER TABLE table_name RENAME COLUMN old_col_name TO new_col_name\n",
    "\n",
    "-- 중첩된 컬럼은 델타레이크 버전을 올려야 사용가능\n",
    "ALTER TABLE <table_name> SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5')\n",
    "ALTER TABLE table_name RENAME COLUMN col_name.old_nested_field TO new_nested_field\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ec5cb61-4f2d-4c32-bef3-820e69efc2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"ALTER TABLE student_delta RENAME COLUMN name TO user_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71b39ddb-2794-479b-8d83-ea3303077f8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column rename is not supported for your Delta table. \nPlease upgrade your Delta table to reader version 2 and writer version 5\n and change the column mapping mode to 'name' mapping. You can use the following command:\n\n ALTER TABLE <table_name> SET TBLPROPERTIES (\n   'delta.columnMapping.mode' = 'name',\n   'delta.minReaderVersion' = '2',\n   'delta.minWriterVersion' = '5')\n\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALTER TABLE family RENAME COLUMN name.middleName TO midName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column rename is not supported for your Delta table. \nPlease upgrade your Delta table to reader version 2 and writer version 5\n and change the column mapping mode to 'name' mapping. You can use the following command:\n\n ALTER TABLE <table_name> SET TBLPROPERTIES (\n   'delta.columnMapping.mode' = 'name',\n   'delta.minReaderVersion' = '2',\n   'delta.minWriterVersion' = '5')\n\n    "
     ]
    }
   ],
   "source": [
    "# 기본 버전(minRead:1, minWrite:2) 버전에서 이름 변경 시에 AnalysisException: Column rename is not supported for your Delta table. 오류 발생\n",
    "query(\"ALTER TABLE family RENAME COLUMN name.middleName TO midName\")\n",
    "\n",
    "\"\"\"Please upgrade your Delta table to reader version 2 and writer version 5\n",
    " and change the column mapping mode to 'name' mapping. You can use the following command:\n",
    "\n",
    " ALTER TABLE <table_name> SET TBLPROPERTIES (\n",
    "   'delta.columnMapping.mode' = 'name',\n",
    "   'delta.minReaderVersion' = '2',\n",
    "   'delta.minWriterVersion' = '5')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3344f383-397b-4277-8ce3-5e2836b74ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"\"\"\n",
    "ALTER TABLE family SET TBLPROPERTIES (\n",
    "   'delta.columnMapping.mode' = 'name',\n",
    "   'delta.minReaderVersion' = '2',\n",
    "   'delta.minWriterVersion' = '5')\n",
    "\"\"\")\n",
    "query(\"ALTER TABLE family RENAME COLUMN name.middleName TO midName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba2e66-8cd5-4085-81f1-762180be463b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Drop columns\n",
    "```sql\n",
    "ALTER TABLE table_name DROP COLUMN col_name\n",
    "-- 하나의 컬럼은 드랍 되지만, 2개 이상 컬럼 드랍을 지원하지 않음\n",
    "-- ALTER TABLE table_name DROP COLUMNS (col_name_1, col_name_2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc49670a-06b7-4839-8a8b-42b8bd223627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"alter table student_delta drop column age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb25be16-0952-4a80-9b9d-a816b6abed46",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot drop column from a struct type with a single field: StructType(StructField(name,StringType,true))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malter table student_delta drop columns (id, name)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot drop column from a struct type with a single field: StructType(StructField(name,StringType,true))"
     ]
    }
   ],
   "source": [
    "query(\"alter table student_delta drop columns (id, name)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264576ff-9c36-422e-bd12-3d1c541c9459",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Change column type or name\n",
    "```python\n",
    "# 컬럼 타입의 변경은 API 활용하여 withColumn 및 cast 함수를 활용해야만 합니다\n",
    "spark.read.table(...) \\\n",
    "  .withColumn(\"birthDate\", col(\"birthDate\").cast(\"date\")) \\\n",
    "  .write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\", \"true\") \\\n",
    "  .saveAsTable(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39618223-1ba0-40d0-aa31-837cb4f21fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>id</td><td>int</td><td></td></tr>\n",
       "<tr><td>firstName</td><td>string</td><td></td></tr>\n",
       "<tr><td>middleName</td><td>string</td><td></td></tr>\n",
       "<tr><td>lastName</td><td>string</td><td></td></tr>\n",
       "<tr><td>gender</td><td>string</td><td></td></tr>\n",
       "<tr><td>birthDate</td><td>string</td><td></td></tr>\n",
       "<tr><td>ssn</td><td>string</td><td></td></tr>\n",
       "<tr><td>salary</td><td>int</td><td></td></tr>\n",
       "<tr><td>description</td><td>string</td><td>description_of_member</td></tr>\n",
       "<tr><td></td><td></td><td></td></tr>\n",
       "<tr><td># Partitioning</td><td></td><td></td></tr>\n",
       "<tr><td>Part 0</td><td>salary</td><td></td></tr>\n",
       "<tr><td></td><td></td><td></td></tr>\n",
       "<tr><td># Detailed Table Information</td><td></td><td></td></tr>\n",
       "<tr><td>Name</td><td>default.users</td><td></td></tr>\n",
       "<tr><td>Location</td><td>file:/home/jovyan/work/spark-warehouse/users</td><td></td></tr>\n",
       "<tr><td>Provider</td><td>delta</td><td></td></tr>\n",
       "<tr><td>Owner</td><td>root</td><td></td></tr>\n",
       "<tr><td>Table Properties</td><td>[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]</td><td></td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------------+----------------------------------------------------------------+---------------------+\n",
       "|                    col_name|                                                       data_type|              comment|\n",
       "+----------------------------+----------------------------------------------------------------+---------------------+\n",
       "|                          id|                                                             int|                     |\n",
       "|                   firstName|                                                          string|                     |\n",
       "|                  middleName|                                                          string|                     |\n",
       "|                    lastName|                                                          string|                     |\n",
       "|                      gender|                                                          string|                     |\n",
       "|                   birthDate|                                                          string|                     |\n",
       "|                         ssn|                                                          string|                     |\n",
       "|                      salary|                                                             int|                     |\n",
       "|                 description|                                                          string|description_of_member|\n",
       "|                            |                                                                |                     |\n",
       "|              # Partitioning|                                                                |                     |\n",
       "|                      Part 0|                                                          salary|                     |\n",
       "|                            |                                                                |                     |\n",
       "|# Detailed Table Information|                                                                |                     |\n",
       "|                        Name|                                                   default.users|                     |\n",
       "|                    Location|                    file:/home/jovyan/work/spark-warehouse/users|                     |\n",
       "|                    Provider|                                                           delta|                     |\n",
       "|                       Owner|                                                            root|                     |\n",
       "|            Table Properties|[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=2]|                     |\n",
       "+----------------------------+----------------------------------------------------------------+---------------------+"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"describe extended users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f852959b-d4de-4641-9e5f-73fc18063947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- middleName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthDate: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- middleName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthDate: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "tableName = \"users\"\n",
    "deltaUsers = DeltaTable.forName(spark, tableName).toDF()\n",
    "deltaUsers.printSchema()\n",
    "users = deltaUsers.withColumn(\"uid\", expr(\"cast(id as long)\")).drop(\"id\").withColumnRenamed(\"uid\", \"id\")\n",
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ebd4f-328a-4541-b490-6c1d98cea5ab",
   "metadata": {},
   "source": [
    "### 스파크 테이블 관리\n",
    "> 결국 스파크에서 어떻게 테이블을 관리할 지는 생성 시점의 `USING` 구문의 데이터소스의 종류에 의해 결정나며, 기본 데이터소스는 `Parquet` 입니다.\n",
    "\n",
    "\n",
    "### 1. 데이터소스 지정을 통해 생성하는 방법\n",
    "> `USING` 구문이 필수이며, 하이브가 아니라 스파크 독립적인 메타정보를 로컬 환경의 `derby` 엔진을 통해 `metastore` 정보를 관리하는 방식\n",
    "```sql\n",
    "CREATE TABLE [ IF NOT EXISTS ] table_identifier\n",
    "    [ ( col_name1 col_type1 [ COMMENT col_comment1 ], ... ) ]\n",
    "    USING data_source\n",
    "    [ OPTIONS ( key1=val1, key2=val2, ... ) ]\n",
    "    [ PARTITIONED BY ( col_name1, col_name2, ... ) ]\n",
    "    [ CLUSTERED BY ( col_name3, col_name4, ... ) \n",
    "        [ SORTED BY ( col_name [ ASC | DESC ], ... ) ] \n",
    "        INTO num_buckets BUCKETS ]\n",
    "    [ LOCATION path ]\n",
    "    [ COMMENT table_comment ]\n",
    "    [ TBLPROPERTIES ( key1=val1, key2=val2, ... ) ]\n",
    "    [ AS select_statement ]\n",
    "```\n",
    "\n",
    "### 2. 하이브 DDL 구문을 통해 생성하는 방법\n",
    "> 설치 및 기동 시에 hive 관련 정보를 로딩하거나 `SparkSession` 객체생성 시에 `config(\"spark.sql.catalogImplementation\", \"hive\")` 설정을 하여 `Hive DDL/DML` 구문으로 인식하게 하는 방법\n",
    "```sql\n",
    "CREATE [ EXTERNAL ] TABLE [ IF NOT EXISTS ] table_identifier\n",
    "    [ ( col_name1[:] col_type1 [ COMMENT col_comment1 ], ... ) ]\n",
    "    [ COMMENT table_comment ]\n",
    "    [ PARTITIONED BY ( col_name2[:] col_type2 [ COMMENT col_comment2 ], ... ) \n",
    "        | ( col_name1, col_name2, ... ) ]\n",
    "    [ CLUSTERED BY ( col_name1, col_name2, ...) \n",
    "        [ SORTED BY ( col_name1 [ ASC | DESC ], col_name2 [ ASC | DESC ], ... ) ] \n",
    "        INTO num_buckets BUCKETS ]\n",
    "    [ ROW FORMAT row_format ]\n",
    "    [ STORED AS file_format ]\n",
    "    [ LOCATION path ]\n",
    "    [ TBLPROPERTIES ( key1=val1, key2=val2, ... ) ]\n",
    "    [ AS select_statement ]\n",
    "```\n",
    "\n",
    "### 3. 이미 존재하는 테이블/뷰의 정의/메타를 활용하여 생성하는 방법\n",
    "> `LIKE` 구문과 `USING` 구문을 통해 지정한 데이터소스의 메타데이터 및 정의를 통해 테이블을 생성하는 방법\n",
    "```sql\n",
    "CREATE TABLE [IF NOT EXISTS] table_identifier LIKE source_table_identifier\n",
    "    USING data_source\n",
    "    [ ROW FORMAT row_format ]\n",
    "    [ STORED AS file_format ]\n",
    "    [ TBLPROPERTIES ( key1=val1, key2=val2, ... ) ]\n",
    "    [ LOCATION path ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd7c4716-f8d9-473b-acb5-eb1e6b144069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th></tr>\n",
       "<tr><td>id</td></tr>\n",
       "<tr><td>name</td></tr>\n",
       "<tr><td>age</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|col_name|\n",
       "+--------+\n",
       "|      id|\n",
       "|    name|\n",
       "|     age|\n",
       "+--------+"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"show columns in student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8bc608c7-7cc8-4410-97ba-ef3c2653f303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            col_name|           data_type|             comment|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|                 int|                    |\n",
      "|           firstName|              string|                    |\n",
      "|          middleName|              string|                    |\n",
      "|            lastName|              string|                    |\n",
      "|              gender|              string|                    |\n",
      "|           birthDate|              string|                    |\n",
      "|                 ssn|              string|                    |\n",
      "|              salary|                 int|                    |\n",
      "|         description|              string|description_of_me...|\n",
      "|                    |                    |                    |\n",
      "|      # Partitioning|                    |                    |\n",
      "|              Part 0|              salary|                    |\n",
      "|                    |                    |                    |\n",
      "|# Detailed Table ...|                    |                    |\n",
      "|                Name|       default.users|                    |\n",
      "|            Location|file:/home/jovyan...|                    |\n",
      "|            Provider|               delta|                    |\n",
      "|               Owner|                root|                    |\n",
      "|    Table Properties|[Type=MANAGED,del...|                    |\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "+------+--------------------+-------------+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "|format|                  id|         name|description|            location|           createdAt|        lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|\n",
      "+------+--------------------+-------------+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "| delta|95a07e78-d01a-4ea...|default.users|       null|file:/home/jovyan...|2023-02-04 18:14:...|2023-02-04 18:14:...|        [salary]|       4|       7980|        {}|               1|               2|\n",
      "+------+--------------------+-------------+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      1|2023-02-04 18:14:...|  null|    null|         ADD COLUMNS|{columns -> [{\"co...|null|    null|     null|          0|  Serializable|         true|                  {}|        null|Apache-Spark/3.2....|\n",
      "|      0|2023-02-04 18:14:...|  null|    null|CREATE OR REPLACE...|{isManaged -> tru...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 4, n...|        null|Apache-Spark/3.2....|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(\"describe extended users\").show()\n",
    "query(\"describe detail users\").show()\n",
    "query(\"describe history users\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f352c480-bf8b-4569-8ba7-fdcc79c218a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = spark.range(0,5)\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"./foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d151c6c1-1e33-49be-9442-79a91b84cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9d7327-0827-4785-89ce-2df0a7738b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE OR REPLACE TABLE default.people_using_sql (\n",
      " id INT,\n",
      " firstName STRING,\n",
      " middleName STRING,\n",
      " lastName STRING,\n",
      " gender STRING,\n",
      " birthDate TIMESTAMP,\n",
      " ssn STRING,\n",
      " salary INT\n",
      ")\n",
      "USING DELTA\n",
      "      PARTITIONED BY (gender)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "createTable=\"\"\"\n",
    "CREATE OR REPLACE TABLE default.people_using_sql (\n",
    " id INT,\n",
    " firstName STRING,\n",
    " middleName STRING,\n",
    " lastName STRING,\n",
    " gender STRING,\n",
    " birthDate TIMESTAMP,\n",
    " ssn STRING,\n",
    " salary INT\n",
    ")\n",
    "USING DELTA\n",
    "      PARTITIONED BY (gender)\n",
    "\"\"\"\n",
    "print(createTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abc2581-fa8c-47c7-96b1-2ac8912e937a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(createTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5162d9-0c41-47bb-b151-eb2a3d499f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>people_using_sql</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+----------------+-----------+\n",
       "|namespace|       tableName|isTemporary|\n",
       "+---------+----------------+-----------+\n",
       "|  default|people_using_sql|      false|\n",
       "+---------+----------------+-----------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45cefce-cd00-4aaf-bbd9-745c5d9ebbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /home/jovyan/work/spark-warehouse/foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00294975-c92d-4cea-8007-af068141eab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b124404e-bf44-463c-a925-40d4600e2758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createTable=\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS delta.`/home/jovyan/work/spark-warehouse/foo` (\n",
    " id INT,\n",
    " firstName STRING,\n",
    " middleName STRING,\n",
    " lastName STRING,\n",
    " gender STRING,\n",
    " birthDate TIMESTAMP,\n",
    " ssn STRING,\n",
    " salary INT\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (gender)\n",
    "\"\"\"\n",
    "query(createTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd9aec79-0820-407c-b74f-a8c4612e523e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>people_using_sql</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+----------------+-----------+\n",
       "|namespace|       tableName|isTemporary|\n",
       "+---------+----------------+-----------+\n",
       "|  default|people_using_sql|      false|\n",
       "+---------+----------------+-----------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a89248f-0dab-4f26-ba60-879f9fbc1450",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "`default.bad_style` is not a valid name for tables/databases. Valid names only contain alphabet characters, numbers and _.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE TABLE `default.bad_style` (id INT) USING DELTA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: `default.bad_style` is not a valid name for tables/databases. Valid names only contain alphabet characters, numbers and _."
     ]
    }
   ],
   "source": [
    "query(\"CREATE TABLE `default.bad_style` (id INT) USING DELTA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5940c33-d3cf-4595-8d40-2aa256f9664f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"CREATE TABLE `default`.`good_style` (id INT) USING DELTA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "890ada3f-a6aa-497e-8f49-bbfee90fb9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>good_style</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>people_using_sql</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+----------------+-----------+\n",
       "|namespace|       tableName|isTemporary|\n",
       "+---------+----------------+-----------+\n",
       "|  default|      good_style|      false|\n",
       "|  default|people_using_sql|      false|\n",
       "+---------+----------------+-----------+"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ca431a8-c352-4c77-8e74-8f7a814c5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    DeltaTable.create(spark)\n",
    "  .tableName(\"default.events\")\n",
    "  .addColumn(\"eventId\", \"BIGINT\")\n",
    "  .addColumn(\"data\", \"STRING\")\n",
    "  .addColumn(\"eventType\", \"STRING\")\n",
    "  .addColumn(\"eventTime\", \"TIMESTAMP\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "738b2874-f600-48bc-9e05-cd9d8b393315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x7f98002af580>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateTableBuilder = (\n",
    "    builder.addColumn(\"eventDate\", \"DATE\", generatedAlwaysAs=\"CAST(eventTime AS DATE)\")\n",
    "    .partitionedBy(\"eventType\", \"eventDate\")\n",
    ")\n",
    "dateTableBuilder.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e35c5dad-9c01-4e78-bcd2-2ebd55a2b71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>eventId</td><td>bigint</td><td></td></tr>\n",
       "<tr><td>data</td><td>string</td><td></td></tr>\n",
       "<tr><td>eventType</td><td>string</td><td></td></tr>\n",
       "<tr><td>eventTime</td><td>timestamp</td><td></td></tr>\n",
       "<tr><td>eventDate</td><td>date</td><td></td></tr>\n",
       "<tr><td></td><td></td><td></td></tr>\n",
       "<tr><td># Partitioning</td><td></td><td></td></tr>\n",
       "<tr><td>Part 0</td><td>eventType</td><td></td></tr>\n",
       "<tr><td>Part 1</td><td>eventDate</td><td></td></tr>\n",
       "<tr><td></td><td></td><td></td></tr>\n",
       "<tr><td># Detailed Table Information</td><td></td><td></td></tr>\n",
       "<tr><td>Name</td><td>default.events</td><td></td></tr>\n",
       "<tr><td>Location</td><td>file:/home/jovyan/work/spark-warehouse/events</td><td></td></tr>\n",
       "<tr><td>Provider</td><td>delta</td><td></td></tr>\n",
       "<tr><td>Table Properties</td><td>[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=4]</td><td></td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------------+----------------------------------------------------------------+-------+\n",
       "|                    col_name|                                                       data_type|comment|\n",
       "+----------------------------+----------------------------------------------------------------+-------+\n",
       "|                     eventId|                                                          bigint|       |\n",
       "|                        data|                                                          string|       |\n",
       "|                   eventType|                                                          string|       |\n",
       "|                   eventTime|                                                       timestamp|       |\n",
       "|                   eventDate|                                                            date|       |\n",
       "|                            |                                                                |       |\n",
       "|              # Partitioning|                                                                |       |\n",
       "|                      Part 0|                                                       eventType|       |\n",
       "|                      Part 1|                                                       eventDate|       |\n",
       "|                            |                                                                |       |\n",
       "|# Detailed Table Information|                                                                |       |\n",
       "|                        Name|                                                  default.events|       |\n",
       "|                    Location|                   file:/home/jovyan/work/spark-warehouse/events|       |\n",
       "|                    Provider|                                                           delta|       |\n",
       "|            Table Properties|[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=4]|       |\n",
       "+----------------------------+----------------------------------------------------------------+-------+"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"desc extended events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa6b1fbf-2e25-457d-95d5-267d8b05d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter (('eventTime >= 2020-10-01 00:00:00) AND ('eventTime <= 2020-10-01 12:00:00))\n",
      "   +- 'UnresolvedRelation [default, events], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "eventId: bigint, data: string, eventType: string, eventTime: timestamp, eventDate: date\n",
      "Project [eventId#4064L, data#4065, eventType#4066, eventTime#4067, eventDate#4068]\n",
      "+- Filter ((eventTime#4067 >= cast(2020-10-01 00:00:00 as timestamp)) AND (eventTime#4067 <= cast(2020-10-01 12:00:00 as timestamp)))\n",
      "   +- SubqueryAlias spark_catalog.default.events\n",
      "      +- Relation default.events[eventId#4064L,data#4065,eventType#4066,eventTime#4067,eventDate#4068] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter ((((eventDate#4068 >= cast(2020-10-01 00:00:00 as date)) OR isnull((eventDate#4068 >= cast(2020-10-01 00:00:00 as date)))) AND ((eventDate#4068 <= cast(2020-10-01 12:00:00 as date)) OR isnull((eventDate#4068 <= cast(2020-10-01 12:00:00 as date))))) AND ((isnotnull(eventTime#4067) AND (eventTime#4067 >= 2020-10-01 00:00:00)) AND (eventTime#4067 <= 2020-10-01 12:00:00)))\n",
      "+- Relation default.events[eventId#4064L,data#4065,eventType#4066,eventTime#4067,eventDate#4068] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [eventId#4064L, data#4065, eventType#4066, eventTime#4067, eventDate#4068]\n",
      "+- *(1) Filter ((isnotnull(eventTime#4067) AND (eventTime#4067 >= 2020-10-01 00:00:00)) AND (eventTime#4067 <= 2020-10-01 12:00:00))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet default.events[eventId#4064L,data#4065,eventTime#4067,eventType#4066,eventDate#4068] Batched: true, DataFilters: [isnotnull(eventTime#4067), (eventTime#4067 >= 2020-10-01 00:00:00), (eventTime#4067 <= 2020-10-0..., Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/home/jovyan/work/spark-warehouse/events], PartitionFilters: [((eventDate#4068 >= cast(2020-10-01 00:00:00 as date)) OR isnull((eventDate#4068 >= cast(2020-10..., PushedFilters: [IsNotNull(eventTime), GreaterThanOrEqual(eventTime,2020-09-30 15:00:00.0), LessThanOrEqual(event..., ReadSchema: struct<eventId:bigint,data:string,eventTime:timestamp>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query('SELECT * FROM default.events WHERE eventTime >= \"2020-10-01 00:00:00\" and eventTime <= \"2020-10-01 12:00:00\"').explain(\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec3d1bd5-7342-455f-bcbe-e91a8a911447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>events</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>good_style</td><td>false</td></tr>\n",
       "<tr><td>default</td><td>people_using_sql</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+----------------+-----------+\n",
       "|namespace|       tableName|isTemporary|\n",
       "+---------+----------------+-----------+\n",
       "|  default|          events|      false|\n",
       "|  default|      good_style|      false|\n",
       "|  default|people_using_sql|      false|\n",
       "+---------+----------------+-----------+"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25d4fbd2-6b56-47f1-978e-6bcd01b34d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>eventId</td><td>bigint</td><td></td></tr>\n",
       "<tr><td>data</td><td>string</td><td></td></tr>\n",
       "<tr><td>eventType</td><td>string</td><td></td></tr>\n",
       "<tr><td>eventTime</td><td>timestamp</td><td></td></tr>\n",
       "<tr><td>eventDate</td><td>date</td><td></td></tr>\n",
       "<tr><td></td><td></td><td></td></tr>\n",
       "<tr><td># Partitioning</td><td></td><td></td></tr>\n",
       "<tr><td>Part 0</td><td>eventType</td><td></td></tr>\n",
       "<tr><td>Part 1</td><td>eventDate</td><td></td></tr>\n",
       "<tr><td></td><td></td><td></td></tr>\n",
       "<tr><td># Detailed Table Information</td><td></td><td></td></tr>\n",
       "<tr><td>Name</td><td>default.events</td><td></td></tr>\n",
       "<tr><td>Location</td><td>file:/home/jovyan/work/spark-warehouse/events</td><td></td></tr>\n",
       "<tr><td>Provider</td><td>delta</td><td></td></tr>\n",
       "<tr><td>Table Properties</td><td>[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=4]</td><td></td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------------+----------------------------------------------------------------+-------+\n",
       "|                    col_name|                                                       data_type|comment|\n",
       "+----------------------------+----------------------------------------------------------------+-------+\n",
       "|                     eventId|                                                          bigint|       |\n",
       "|                        data|                                                          string|       |\n",
       "|                   eventType|                                                          string|       |\n",
       "|                   eventTime|                                                       timestamp|       |\n",
       "|                   eventDate|                                                            date|       |\n",
       "|                            |                                                                |       |\n",
       "|              # Partitioning|                                                                |       |\n",
       "|                      Part 0|                                                       eventType|       |\n",
       "|                      Part 1|                                                       eventDate|       |\n",
       "|                            |                                                                |       |\n",
       "|# Detailed Table Information|                                                                |       |\n",
       "|                        Name|                                                  default.events|       |\n",
       "|                    Location|                   file:/home/jovyan/work/spark-warehouse/events|       |\n",
       "|                    Provider|                                                           delta|       |\n",
       "|            Table Properties|[Type=MANAGED,delta.minReaderVersion=1,delta.minWriterVersion=4]|       |\n",
       "+----------------------------+----------------------------------------------------------------+-------+"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"desc formatted events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d4c67ab-7e24-45ce-8036-a7a450b023e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>key</th><th>value</th></tr>\n",
       "<tr><td>Type</td><td>MANAGED</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-------+\n",
       "| key|  value|\n",
       "+----+-------+\n",
       "|Type|MANAGED|\n",
       "+----+-------+"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"show tblproperties events ('Type')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8b30fe2-9695-42ff-a46b-b9db99f35980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>0</td><td>2022-12-29 12:18:56</td><td>null</td><td>null</td><td>CREATE TABLE</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;eventType&quot;,&quot;eventDate&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>true</td><td>{}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------------------+------+--------+------------+----------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+-----------------------------------+\n",
       "|version|          timestamp|userId|userName|   operation|                                                                                 operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-------------------+------+--------+------------+----------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+-----------------------------------+\n",
       "|      0|2022-12-29 12:18:56|  null|    null|CREATE TABLE|{isManaged -> true, description -> null, partitionBy -> [\"eventType\",\"eventDate\"], properties -> {}}|null|    null|     null|       null|  Serializable|         true|              {}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-------------------+------+--------+------------+----------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"describe history events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa9956f0-d0ae-48d4-8620-a0de4bc2d8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>eventId</th><th>data</th><th>eventType</th><th>eventTime</th><th>eventDate</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+----+---------+---------+---------+\n",
       "|eventId|data|eventType|eventTime|eventDate|\n",
       "+-------+----+---------+---------+---------+\n",
       "+-------+----+---------+---------+---------+"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from events VERSION AS of 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a7d8c9-9467-45e7-893d-2bb60050c729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.b\n"
     ]
    }
   ],
   "source": [
    "print(\"{}.{}\".format(\"a\", \"b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df905612-d904-43e5-a497-1505eabe37b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>0</td><td>2023-01-09 08:58:34</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;salary&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 4, numOutputRows -&gt; 4, numOutputBytes -&gt; 7980}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------------------+------+--------+---------------------------------+-------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|          timestamp|userId|userName|                        operation|                                                                  operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-------------------+------+--------+---------------------------------+-------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      0|2023-01-09 08:58:34|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"salary\"], properties -> {}}|null|    null|     null|       null|  Serializable|        false|{numFiles -> 4, numOutputRows -> 4, numOutputBytes -> 7980}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-------------------+------+--------+---------------------------------+-------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"describe history users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5089659-73c2-4be4-9726-0d41c5c4409f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td><td>female</td><td>2005/05/20</td><td>040520</td><td>3000</td></tr>\n",
       "<tr><td>4</td><td>sihun</td><td>sean</td><td>park</td><td>male</td><td>2006/01/14</td><td>080114</td><td>4000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
       "|  4|    sihun|      sean|    park|  male|2006/01/14|080114|  4000|\n",
       "+---+---------+----------+--------+------+----------+------+------+"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e0d2b01-63b3-47f4-94cd-cfe85508adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "appendRows = []\n",
    "appendRows.append(Row(3, \"sowon\", \"eva\", \"park\", \"female\", \"2005/05/20\", \"040520\", 3000))\n",
    "df1 = spark.createDataFrame(appendRows, tableSchema)\n",
    "df1.write.format(\"delta\").mode(\"append\").saveAsTable(\"default.users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "573b824e-23e7-4c4f-8dc7-0f9034b1f57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td><td>female</td><td>2005/05/20</td><td>040520</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
       "+---+---------+----------+--------+------+----------+------+------+"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6778e64-92cf-4cdb-8833-96e5f35509d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "appendRows = []\n",
    "appendRows.append(Row(4, \"sihun\", \"sean\", \"park\", \"male\", \"2006/01/14\", \"080114\", 2000))\n",
    "df2 = spark.createDataFrame(tableRows, tableSchema)\n",
    "df2.write.format(\"delta\").mode(\"append\").saveAsTable(\"default.users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "933e2e3d-94e7-4fff-9cc0-02badc97d5d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query(\"SELECT * FROM default.users TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "313789ab-eda6-4982-901c-29ba1df32e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(version)=3)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = query(\"DESCRIBE HISTORY users\")\n",
    "latest_version = history.selectExpr(\"max(version)\").collect()\n",
    "latest_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "27971c28-4478-4121-af32-43f8bca6062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = spark.read.format(\"delta\").option(\"versionAsOf\", latest_version[0][0]).load(\"./spark-warehouse/users\")\n",
    "users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c097b88-c372-469f-9c74-b2998e1015f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-12-31'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"SELECT CAST(date_sub(current_date(), 1) AS STRING)\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "642e89a4-c94b-4995-9a40-c3ea55508f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"./spark-warehouse/users\")\n",
    "users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0beee56e-84e2-4505-832c-8b20a60731cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"./spark-warehouse/users\")\n",
    "users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51834bbf-bb5f-41e4-b5ab-8edb9638d38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>3</td><td>2023-01-01 11:59:22</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>2</td><td>2023-01-01 11:59:16</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>1</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 2, numOutputRows -&gt; 1, numOutputBytes -&gt; 3130}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>1</td><td>2023-01-01 11:59:00</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>0</td><td>2022-12-30 11:42:08</td><td>null</td><td>null</td><td>CREATE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|          timestamp|userId|userName|                        operation|                                                          operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      3|2023-01-01 11:59:22|  null|    null|                            WRITE|                                          {mode -> Append, partitionBy -> []}|null|    null|     null|          2|  Serializable|         true|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      2|2023-01-01 11:59:16|  null|    null|                            WRITE|                                          {mode -> Append, partitionBy -> []}|null|    null|     null|          1|  Serializable|         true|{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 3130}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      1|2023-01-01 11:59:00|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|    null|     null|          0|  Serializable|        false|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      0|2022-12-30 11:42:08|  null|    null|           CREATE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|    null|     null|       null|  Serializable|         true|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"DESCRIBE HISTORY users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb2b7b96-d5a7-43f2-a726-ab3da3d40c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-12-31'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yesterday = query(\"SELECT CAST(date_add(current_date(), 0) AS STRING)\").collect()[0][0]\n",
    "yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "606e6660-ee7e-4225-adf0-79eb45250137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 원하는 데이터 버전과 버전 사이의 시간을 알고 있다면 조회가 가능하다\n",
    "df = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2022-12-31 14:40:00\").load(\"./spark-warehouse/users\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b658534-028b-4c5b-b7bc-8b716b40acba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 원하는 데이터 버전과 버전 사이의 시간을 알고 있다면 조회가 가능하다\n",
    "df2 = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2022-12-31 14:46:00\").load(\"./spark-warehouse/users\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a9a6744-fff5-4bf7-95a0-b8d60183ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 마지막 버전의 타임스템프보다 이후의 시간입력 시에는 오류가 발생, 이전 데이터는 사이의 시간을 입력해야만 한다\n",
    "df3 = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2022-12-31 14:54:59\").load(\"./spark-warehouse/users\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ca2e75c-9bc0-4ede-ad5f-fd8802c220bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최신 데이터는 조건 없이 조회하면 된다\n",
    "latest_data = spark.read.format(\"delta\").load(\"./spark-warehouse/users\")\n",
    "latest_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f3e24e9-6d8a-444c-93d1-a218d7bf7255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 초반에 넣었던 오류 데이터를 제외하고 신규 데이터만 가져오는 작업을 하려면 어떻게 해야할까?\n",
    "initial_data = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2022-12-31 14:40:00\").load(\"./spark-warehouse/users\")\n",
    "initial_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41054a5c-0e3c-44bc-9ddc-849574f49248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_added = latest_data.where(\"id > 2\")\n",
    "latest_added.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9bd7cb47-0ee3-4bf7-b727-c3ab9d0de7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_data.union(latest_added).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d328aa7-cae2-4652-9064-c2e89bf502ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초반에 넣었던 오류 데이터를 제외하고 신규 데이터만 가져오는 작업을 하려면 어떻게 해야할까?\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "# 실수로 2번 이하의 이용자를 모두 삭제하고\n",
    "deltaTable = DeltaTable.forPath(spark, \"./spark-warehouse/users\")\n",
    "deltaTable.delete(\"id < 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0fc4ba18-f043-4325-a35b-72890b00fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 초기에 추가된 이용자 2명을 다시 읽어와서\n",
    "initial_data = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2022-12-31 14:40:00\").load(\"./spark-warehouse/users\")\n",
    "initial_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45d4471f-d988-431c-962a-dbf08cfa7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_data = spark.read.format(\"delta\").load(\"./spark-warehouse/users\")\n",
    "latest_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a6a5b60-f106-40e0-977f-8c509ae28444",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 초기에 추가된 이용자 2명을 다시 추가하자\u001b[39;00m\n\u001b[1;32m      2\u001b[0m initial_data\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43msparks\u001b[49m\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from users\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparks' is not defined"
     ]
    }
   ],
   "source": [
    "# 초기에 추가된 이용자 2명을 다시 추가하자\n",
    "initial_data.write.format(\"delta\").mode(\"append\").saveAsTable(\"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "87ea011b-c4cc-40ba-bf61-25c4487ccb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td><td>female</td><td>2005/05/20</td><td>040520</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
       "+---+---------+----------+--------+------+----------+------+------+"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e49a8cde-9273-44ce-953c-9a2ba4c98c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>default</td><td>users</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---------+-----------+\n",
       "|namespace|tableName|isTemporary|\n",
       "+---------+---------+-----------+\n",
       "|  default|    users|      false|\n",
       "+---------+---------+-----------+"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91884101-b177-4877-be8d-bfa55edadef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "createTable=\"\"\"\n",
    "CREATE OR REPLACE TABLE default.people_using_sql (\n",
    " id INT,\n",
    " firstName STRING,\n",
    " middleName STRING,\n",
    " lastName STRING,\n",
    " gender STRING,\n",
    " birthDate TIMESTAMP,\n",
    " ssn STRING,\n",
    " salary INT\n",
    ")\n",
    "USING DELTA\n",
    "      PARTITIONED BY (gender)\n",
    "\"\"\"\n",
    "print(createTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "008d0476-f63f-4fab-bdff-219290edd133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"CREATE OR REPLACE TABLE default.users_clone USING DELTA AS SELECT * FROM users\")\n",
    "\n",
    "# 아래의 구문은 하이브 메타스토어를 통해 생성할 수 있는 구문\n",
    "# query(\"create table if not exists users_clone as select * from users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ff080c90-1d7d-4fe3-9fbc-6a7a40cc9973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"INSERT INTO users_clone SELECT * FROM users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71c8d21a-e1ca-4613-9ce6-b0ff7cb59d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"INSERT OVERWRITE TABLE users_clone SELECT * FROM users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "243b2a54-9bdd-4b57-98cb-d0744d7c87a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td><td>female</td><td>2005/05/20</td><td>040520</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
       "+---+---------+----------+--------+------+----------+------+------+"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"SELECT * FROM users_clone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98e90ec4-3de0-4069-bfa8-cbff747530b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVACUUM users RETAIN 24 HOURS DRY RUN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       "
     ]
    }
   ],
   "source": [
    "# 최대 vacuum 시간 미만으로 지정하는 경우 오류를 발생\n",
    "query(\"VACUUM users RETAIN 24 HOURS DRY RUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "53d0353b-53ec-424b-b1e5-8ac5d7a88a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>key</th><th>value</th></tr>\n",
       "<tr><td>spark.databricks.delta.retentionDurationCheck.enabled</td><td>false</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------------------------------------------+-----+\n",
       "|                                                  key|value|\n",
       "+-----------------------------------------------------+-----+\n",
       "|spark.databricks.delta.retentionDurationCheck.enabled|false|\n",
       "+-----------------------------------------------------+-----+"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "af5d0b99-7121-4597-a6b4-83b11ad48932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>key</th><th>value</th></tr>\n",
       "<tr><td>spark.databricks.delta.retentionDurationCheck.enabled</td><td>true</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------------------------------------------+-----+\n",
       "|                                                  key|value|\n",
       "+-----------------------------------------------------+-----+\n",
       "|spark.databricks.delta.retentionDurationCheck.enabled| true|\n",
       "+-----------------------------------------------------+-----+"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"SET spark.databricks.delta.retentionDurationCheck.enabled = true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8d9ef90c-26bc-40ac-a4fe-e21a138d14f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+\n",
       "|path|\n",
       "+----+\n",
       "+----+"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"VACUUM users RETAIN 24 HOURS DRY RUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "813820eb-cfb7-4006-b871-6553d526a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>3</td><td>2023-01-01 11:59:22</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>2</td><td>2023-01-01 11:59:16</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>1</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 2, numOutputRows -&gt; 1, numOutputBytes -&gt; 3130}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>1</td><td>2023-01-01 11:59:00</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>0</td><td>2022-12-30 11:42:08</td><td>null</td><td>null</td><td>CREATE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|          timestamp|userId|userName|                        operation|                                                          operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      3|2023-01-01 11:59:22|  null|    null|                            WRITE|                                          {mode -> Append, partitionBy -> []}|null|    null|     null|          2|  Serializable|         true|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      2|2023-01-01 11:59:16|  null|    null|                            WRITE|                                          {mode -> Append, partitionBy -> []}|null|    null|     null|          1|  Serializable|         true|{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 3130}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      1|2023-01-01 11:59:00|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|    null|     null|          0|  Serializable|        false|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      0|2022-12-30 11:42:08|  null|    null|           CREATE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|    null|     null|       null|  Serializable|         true|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"describe history users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "221e8360-8b06-4e78-8daf-ef4c5936b10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td><td>female</td><td>2005/05/20</td><td>040520</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
       "+---+---------+----------+--------+------+----------+------+------+"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd7bfb41-a5ae-4477-854d-e89864f2b342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------+\n",
      "|path                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------+\n",
      "|file:/home/jovyan/work/spark-warehouse/users/part-00002-b5e2db62-800d-404f-9750-ff24b9fbc75c-c000.snappy.parquet|\n",
      "|file:/home/jovyan/work/spark-warehouse/users/part-00000-8c43d864-9d91-4a46-8a5d-246e2899f237-c000.snappy.parquet|\n",
      "|file:/home/jovyan/work/spark-warehouse/users/part-00005-9e6a865b-ef24-419b-bf85-3998f26b4883-c000.snappy.parquet|\n",
      "+----------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(\"VACUUM users RETAIN 1 HOURS DRY RUN\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d76dd72b-0c80-426d-bc7a-5957f31030fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th></tr>\n",
       "<tr><td>file:/home/jovyan/work/spark-warehouse/users</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------------------------------+\n",
       "|                                        path|\n",
       "+--------------------------------------------+\n",
       "|file:/home/jovyan/work/spark-warehouse/users|\n",
       "+--------------------------------------------+"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"VACUUM users RETAIN 1 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d841e5c7-b9ab-48b1-9d74-9cfc2114a5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td><td>female</td><td>2005/05/20</td><td>040520</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
       "+---+---------+----------+--------+------+----------+------+------+"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f310b74d-92c1-4c2f-854c-029f81fb4973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th></tr>\n",
       "<tr><td>file:/home/jovyan/work/spark-warehouse/users</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------------------------------+\n",
       "|                                        path|\n",
       "+--------------------------------------------+\n",
       "|file:/home/jovyan/work/spark-warehouse/users|\n",
       "+--------------------------------------------+"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"VACUUM users RETAIN 0.1 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "627d6e6a-211b-476d-9532-522aae298149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td><td>female</td><td>2005/05/20</td><td>040520</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
       "+---+---------+----------+--------+------+----------+------+------+"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6b7c332b-fb2a-4fdb-8ccf-ba285d658639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>key</th><th>value</th></tr>\n",
       "<tr><td>delta.deletedFileRetentionDuration</td><td>&quot;interval 1 hour&quot;</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------------------+-----------------+\n",
       "|                               key|            value|\n",
       "+----------------------------------+-----------------+\n",
       "|delta.deletedFileRetentionDuration|\"interval 1 hour\"|\n",
       "+----------------------------------+-----------------+"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임의로 파일 삭제 리텐션을 수정\n",
    "\n",
    "query(\"\"\"\n",
    "SET delta.deletedFileRetentionDuration = \"interval 1 hour\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dd283485-8dd0-468c-a2e6-39c6d9931d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query(\"vacuum delta.`./spark-warehouse/users` retain 0 hours dry run\")\n",
    "from delta.tables import *\n",
    "\n",
    "pathToTable = \"./spark-warehouse/users\"\n",
    "deltaTable = DeltaTable.forPath(spark, pathToTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "793f9c39-9060-4f07-8bb0-b498e1c94689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.vacuum()        # vacuum files not required by versions older than the default retention period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "67ae8db4-f063-4333-b946-020e3af5ee4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td><td>female</td><td>2005/05/20</td><td>040520</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
       "+---+---------+----------+--------+------+----------+------+------+"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6fe8a0e3-2243-42e5-aca2-f40a4fbdf607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.vacuum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4fc7e5b1-2fc6-4c91-9c73-32c193b8b756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td><td>female</td><td>2005/05/20</td><td>040520</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
       "+---+---------+----------+--------+------+----------+------+------+"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "67169f0f-4cef-4e14-956b-0c874e4a471a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>3</td><td>2023-01-01 11:59:22</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>2</td><td>2023-01-01 11:59:16</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>1</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 2, numOutputRows -&gt; 1, numOutputBytes -&gt; 3130}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>1</td><td>2023-01-01 11:59:00</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>0</td><td>2022-12-30 11:42:08</td><td>null</td><td>null</td><td>CREATE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|          timestamp|userId|userName|                        operation|                                                          operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      3|2023-01-01 11:59:22|  null|    null|                            WRITE|                                          {mode -> Append, partitionBy -> []}|null|    null|     null|          2|  Serializable|         true|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      2|2023-01-01 11:59:16|  null|    null|                            WRITE|                                          {mode -> Append, partitionBy -> []}|null|    null|     null|          1|  Serializable|         true|{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 3130}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      1|2023-01-01 11:59:00|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|    null|     null|          0|  Serializable|        false|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      0|2022-12-30 11:42:08|  null|    null|           CREATE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|    null|     null|       null|  Serializable|         true|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"describe history users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "495f874f-baab-4786-beed-a39a79cc4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "pathToTable = \"/home/jovyan/work/spark-warehouse/users\"\n",
    "deltaTable = DeltaTable.forPath(spark, pathToTable)\n",
    "deltaTable.logRetentionDuration  = \"interval 7 days\"\n",
    "deltaTable.deletedFileRetentionDuration  = \"interval 1 days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2a0949b4-2ddd-4a8c-aa08-a9e2667694c3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [143]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVACUUM users RETAIN 48 HOURS DRY RUN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       "
     ]
    }
   ],
   "source": [
    "query(\"VACUUM users RETAIN 48 HOURS DRY RUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2ede413b-05ae-4f31-8887-b4dd11ef3941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th></tr>\n",
       "<tr><td>file:/home/jovyan/work/spark-warehouse/users</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------------------------------+\n",
       "|                                        path|\n",
       "+--------------------------------------------+\n",
       "|file:/home/jovyan/work/spark-warehouse/users|\n",
       "+--------------------------------------------+"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"VACUUM users RETAIN 0 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3ad9d160-01ea-4b6b-abe5-208aa14ade0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format('delta').load(pathToTable)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f503670e-47ae-49ff-84ab-5451b242951d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx = spark.read.parquet(pathToTable)\n",
    "dx.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7b269620-ef66-4147-8cb2-4d79271b1186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+\n",
       "|path|\n",
       "+----+\n",
       "+----+"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"VACUUM users DRY RUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5ee9ac09-9a96-421c-8107-f2c1a21ff5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a90aff55-44b7-462e-9842-17ac3d20f79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th></tr>\n",
       "<tr><td>file:/home/jovyan/work/spark-warehouse/users</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------------------------------+\n",
       "|                                        path|\n",
       "+--------------------------------------------+\n",
       "|file:/home/jovyan/work/spark-warehouse/users|\n",
       "+--------------------------------------------+"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"VACUUM delta.`/home/jovyan/work/spark-warehouse/users` RETAIN 100 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "739aad85-53ca-4406-89f9-df4a265973f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [141]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVACUUM delta.`/home/jovyan/work/spark-warehouse/users` RETAIN 1 HOURS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       "
     ]
    }
   ],
   "source": [
    "query(\"VACUUM delta.`/home/jovyan/work/spark-warehouse/users` RETAIN 1 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "54bd9782-6b08-4757-8c26-dc16907f171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "pathToTable = \"./spark-warehouse/users\"\n",
    "# users = DeltaTable.forName(spark, \"default.users\") # hive-metastore\n",
    "users = DeltaTable.forPath(spark, pathToTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e07f0574-9787-4769-b00f-b7b72b834b4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DeltaTable in module delta.tables object:\n",
      "\n",
      "class DeltaTable(builtins.object)\n",
      " |  DeltaTable(spark: pyspark.sql.session.SparkSession, jdt: 'JavaObject')\n",
      " |  \n",
      " |  Main class for programmatically interacting with Delta tables.\n",
      " |  You can create DeltaTable instances using the path of the Delta table.::\n",
      " |  \n",
      " |      deltaTable = DeltaTable.forPath(spark, \"/path/to/table\")\n",
      " |  \n",
      " |  In addition, you can convert an existing Parquet table in place into a Delta table.::\n",
      " |  \n",
      " |      deltaTable = DeltaTable.convertToDelta(spark, \"parquet.`/path/to/table`\")\n",
      " |  \n",
      " |  .. versionadded:: 0.4\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, spark: pyspark.sql.session.SparkSession, jdt: 'JavaObject')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  alias(self, aliasName: str) -> 'DeltaTable'\n",
      " |      Apply an alias to the Delta table.\n",
      " |      \n",
      " |      .. versionadded:: 0.4\n",
      " |  \n",
      " |  delete(self, condition: Union[str, pyspark.sql.column.Column, NoneType] = None) -> None\n",
      " |      Delete data from the table that match the given ``condition``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          deltaTable.delete(\"date < '2017-01-01'\")        # predicate using SQL formatted string\n",
      " |      \n",
      " |          deltaTable.delete(col(\"date\") < \"2017-01-01\")   # predicate using Spark SQL functions\n",
      " |      \n",
      " |      :param condition: condition of the update\n",
      " |      :type condition: str or pyspark.sql.Column\n",
      " |      \n",
      " |      .. versionadded:: 0.4\n",
      " |  \n",
      " |  generate(self, mode: str) -> None\n",
      " |      Generate manifest files for the given delta table.\n",
      " |      \n",
      " |      :param mode: mode for the type of manifest file to be generated\n",
      " |                   The valid modes are as follows (not case sensitive):\n",
      " |      \n",
      " |                   - \"symlink_format_manifest\": This will generate manifests in symlink format\n",
      " |                                                for Presto and Athena read support.\n",
      " |      \n",
      " |                   See the online documentation for more information.\n",
      " |      \n",
      " |      .. versionadded:: 0.5\n",
      " |  \n",
      " |  history(self, limit: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Get the information of the latest `limit` commits on this table as a Spark DataFrame.\n",
      " |      The information is in reverse chronological order.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
      " |      \n",
      " |          lastOperationDF = deltaTable.history(1) # get the last operation\n",
      " |      \n",
      " |      :param limit: Optional, number of latest commits to returns in the history.\n",
      " |      :return: Table's commit history. See the online Delta Lake documentation for more details.\n",
      " |      :rtype: pyspark.sql.DataFrame\n",
      " |      \n",
      " |      .. versionadded:: 0.4\n",
      " |  \n",
      " |  merge(self, source: pyspark.sql.dataframe.DataFrame, condition: Union[str, pyspark.sql.column.Column]) -> 'DeltaMergeBuilder'\n",
      " |      Merge data from the `source` DataFrame based on the given merge `condition`. This returns\n",
      " |      a :class:`DeltaMergeBuilder` object that can be used to specify the update, delete, or\n",
      " |      insert actions to be performed on rows based on whether the rows matched the condition or\n",
      " |      not. See :class:`DeltaMergeBuilder` for a full description of this operation and what\n",
      " |      combinations of update, delete and insert operations are allowed.\n",
      " |      \n",
      " |      Example 1 with conditions and update expressions as SQL formatted string::\n",
      " |      \n",
      " |          deltaTable.alias(\"events\").merge(\n",
      " |              source = updatesDF.alias(\"updates\"),\n",
      " |              condition = \"events.eventId = updates.eventId\"\n",
      " |            ).whenMatchedUpdate(set =\n",
      " |              {\n",
      " |                \"data\": \"updates.data\",\n",
      " |                \"count\": \"events.count + 1\"\n",
      " |              }\n",
      " |            ).whenNotMatchedInsert(values =\n",
      " |              {\n",
      " |                \"date\": \"updates.date\",\n",
      " |                \"eventId\": \"updates.eventId\",\n",
      " |                \"data\": \"updates.data\",\n",
      " |                \"count\": \"1\"\n",
      " |              }\n",
      " |            ).execute()\n",
      " |      \n",
      " |      Example 2 with conditions and update expressions as Spark SQL functions::\n",
      " |      \n",
      " |          from pyspark.sql.functions import *\n",
      " |      \n",
      " |          deltaTable.alias(\"events\").merge(\n",
      " |              source = updatesDF.alias(\"updates\"),\n",
      " |              condition = expr(\"events.eventId = updates.eventId\")\n",
      " |            ).whenMatchedUpdate(set =\n",
      " |              {\n",
      " |                \"data\" : col(\"updates.data\"),\n",
      " |                \"count\": col(\"events.count\") + 1\n",
      " |              }\n",
      " |            ).whenNotMatchedInsert(values =\n",
      " |              {\n",
      " |                \"date\": col(\"updates.date\"),\n",
      " |                \"eventId\": col(\"updates.eventId\"),\n",
      " |                \"data\": col(\"updates.data\"),\n",
      " |                \"count\": lit(\"1\")\n",
      " |              }\n",
      " |            ).execute()\n",
      " |      \n",
      " |      :param source: Source DataFrame\n",
      " |      :type source: pyspark.sql.DataFrame\n",
      " |      :param condition: Condition to match sources rows with the Delta table rows.\n",
      " |      :type condition: str or pyspark.sql.Column\n",
      " |      \n",
      " |      :return: builder object to specify whether to update, delete or insert rows based on\n",
      " |               whether the condition matched or not\n",
      " |      :rtype: :py:class:`delta.tables.DeltaMergeBuilder`\n",
      " |      \n",
      " |      .. versionadded:: 0.4\n",
      " |  \n",
      " |  optimize(self) -> 'DeltaOptimizeBuilder'\n",
      " |      Optimize the data layout of the table. This returns\n",
      " |      a :py:class:`~delta.tables.DeltaOptimizeBuilder` object that can\n",
      " |      be used to specify the partition filter to limit the scope of\n",
      " |      optimize and also execute different optimization techniques\n",
      " |      such as file compaction or order data using Z-Order curves.\n",
      " |      \n",
      " |      See the :py:class:`~delta.tables.DeltaOptimizeBuilder` for a\n",
      " |      full description of this operation.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          deltaTable.optimize().where(\"date='2021-11-18'\").executeCompaction()\n",
      " |      \n",
      " |      :return: an instance of DeltaOptimizeBuilder.\n",
      " |      :rtype: :py:class:`~delta.tables.DeltaOptimizeBuilder`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  restoreToTimestamp(self, timestamp: str) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Restore the DeltaTable to an older version of the table specified by a timestamp.\n",
      " |      Timestamp can be of the format yyyy-MM-dd or yyyy-MM-dd HH:mm:ss\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          io.delta.tables.DeltaTable.restoreToTimestamp('2021-01-01')\n",
      " |          io.delta.tables.DeltaTable.restoreToTimestamp('2021-01-01 01:01:01')\n",
      " |      \n",
      " |      :param timestamp: target timestamp of restored table\n",
      " |      :return: Dataframe with metrics of restore operation.\n",
      " |      :rtype: pyspark.sql.DataFrame\n",
      " |      \n",
      " |      .. versionadded:: 1.2\n",
      " |  \n",
      " |  restoreToVersion(self, version: int) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Restore the DeltaTable to an older version of the table specified by version number.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          io.delta.tables.DeltaTable.restoreToVersion(1)\n",
      " |      \n",
      " |      :param version: target version of restored table\n",
      " |      :return: Dataframe with metrics of restore operation.\n",
      " |      :rtype: pyspark.sql.DataFrame\n",
      " |      \n",
      " |      .. versionadded:: 1.2\n",
      " |  \n",
      " |  toDF(self) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Get a DataFrame representation of this Delta table.\n",
      " |      \n",
      " |      .. versionadded:: 0.4\n",
      " |  \n",
      " |  update(self, condition: Union[str, pyspark.sql.column.Column, NoneType] = None, set: Optional[Dict[str, Union[str, pyspark.sql.column.Column]]] = None) -> None\n",
      " |      Update data from the table on the rows that match the given ``condition``,\n",
      " |      which performs the rules defined by ``set``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          # condition using SQL formatted string\n",
      " |          deltaTable.update(\n",
      " |              condition = \"eventType = 'clck'\",\n",
      " |              set = { \"eventType\": \"'click'\" } )\n",
      " |      \n",
      " |          # condition using Spark SQL functions\n",
      " |          deltaTable.update(\n",
      " |              condition = col(\"eventType\") == \"clck\",\n",
      " |              set = { \"eventType\": lit(\"click\") } )\n",
      " |      \n",
      " |      :param condition: Optional condition of the update\n",
      " |      :type condition: str or pyspark.sql.Column\n",
      " |      :param set: Defines the rules of setting the values of columns that need to be updated.\n",
      " |                  *Note: This param is required.* Default value None is present to allow\n",
      " |                  positional args in same order across languages.\n",
      " |      :type set: dict with str as keys and str or pyspark.sql.Column as values\n",
      " |      \n",
      " |      .. versionadded:: 0.4\n",
      " |  \n",
      " |  upgradeTableProtocol(self, readerVersion: int, writerVersion: int) -> None\n",
      " |      Updates the protocol version of the table to leverage new features. Upgrading the reader\n",
      " |      version will prevent all clients that have an older version of Delta Lake from accessing\n",
      " |      this table. Upgrading the writer version will prevent older versions of Delta Lake to write\n",
      " |      to this table. The reader or writer version cannot be downgraded.\n",
      " |      \n",
      " |      See online documentation and Delta's protocol specification at PROTOCOL.md for more details.\n",
      " |      \n",
      " |      .. versionadded:: 0.8\n",
      " |  \n",
      " |  vacuum(self, retentionHours: Optional[float] = None) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Recursively delete files and directories in the table that are not needed by the table for\n",
      " |      maintaining older versions up to the given retention threshold. This method will return an\n",
      " |      empty DataFrame on successful completion.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          deltaTable.vacuum()     # vacuum files not required by versions more than 7 days old\n",
      " |      \n",
      " |          deltaTable.vacuum(100)  # vacuum files not required by versions more than 100 hours old\n",
      " |      \n",
      " |      :param retentionHours: Optional number of hours retain history. If not specified, then the\n",
      " |                             default retention period of 168 hours (7 days) will be used.\n",
      " |      \n",
      " |      .. versionadded:: 0.4\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  convertToDelta(sparkSession: pyspark.sql.session.SparkSession, identifier: str, partitionSchema: Union[str, pyspark.sql.types.StructType, NoneType] = None) -> 'DeltaTable' from builtins.type\n",
      " |      Create a DeltaTable from the given parquet table. Takes an existing parquet table and\n",
      " |      constructs a delta transaction log in the base path of the table.\n",
      " |      Note: Any changes to the table during the conversion process may not result in a consistent\n",
      " |      state at the end of the conversion. Users should stop any changes to the table before the\n",
      " |      conversion is started.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          # Convert unpartitioned parquet table at path 'path/to/table'\n",
      " |          deltaTable = DeltaTable.convertToDelta(\n",
      " |              spark, \"parquet.`path/to/table`\")\n",
      " |      \n",
      " |          # Convert partitioned parquet table at path 'path/to/table' and partitioned by\n",
      " |          # integer column named 'part'\n",
      " |          partitionedDeltaTable = DeltaTable.convertToDelta(\n",
      " |              spark, \"parquet.`path/to/table`\", \"part int\")\n",
      " |      \n",
      " |      :param sparkSession: SparkSession to use for the conversion\n",
      " |      :type sparkSession: pyspark.sql.SparkSession\n",
      " |      :param identifier: Parquet table identifier formatted as \"parquet.`path`\"\n",
      " |      :type identifier: str\n",
      " |      :param partitionSchema: Hive DDL formatted string, or pyspark.sql.types.StructType\n",
      " |      :return: DeltaTable representing the converted Delta table\n",
      " |      :rtype: :py:class:`~delta.tables.DeltaTable`\n",
      " |      \n",
      " |      .. versionadded:: 0.4\n",
      " |  \n",
      " |  create(sparkSession: Optional[pyspark.sql.session.SparkSession] = None) -> 'DeltaTableBuilder' from builtins.type\n",
      " |      Return :class:`DeltaTableBuilder` object that can be used to specify\n",
      " |      the table name, location, columns, partitioning columns, table comment,\n",
      " |      and table properties to create a Delta table, error if the table exists\n",
      " |      (the same as SQL `CREATE TABLE`).\n",
      " |      \n",
      " |      See :class:`DeltaTableBuilder` for a full description and examples\n",
      " |      of this operation.\n",
      " |      \n",
      " |      :param sparkSession: SparkSession to use for creating the table\n",
      " |      :return: an instance of DeltaTableBuilder\n",
      " |      :rtype: :py:class:`~delta.tables.DeltaTableBuilder`\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  createIfNotExists(sparkSession: Optional[pyspark.sql.session.SparkSession] = None) -> 'DeltaTableBuilder' from builtins.type\n",
      " |      Return :class:`DeltaTableBuilder` object that can be used to specify\n",
      " |      the table name, location, columns, partitioning columns, table comment,\n",
      " |      and table properties to create a Delta table,\n",
      " |      if it does not exists (the same as SQL `CREATE TABLE IF NOT EXISTS`).\n",
      " |      \n",
      " |      See :class:`DeltaTableBuilder` for a full description and examples\n",
      " |      of this operation.\n",
      " |      \n",
      " |      :param sparkSession: SparkSession to use for creating the table\n",
      " |      :return: an instance of DeltaTableBuilder\n",
      " |      :rtype: :py:class:`~delta.tables.DeltaTableBuilder`\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  createOrReplace(sparkSession: Optional[pyspark.sql.session.SparkSession] = None) -> 'DeltaTableBuilder' from builtins.type\n",
      " |      Return :class:`DeltaTableBuilder` object that can be used to specify\n",
      " |      the table name, location, columns, partitioning columns, table comment,\n",
      " |      and table properties replace a Delta table,\n",
      " |      error if the table doesn't exist (the same as SQL `REPLACE TABLE`).\n",
      " |      \n",
      " |      See :class:`DeltaTableBuilder` for a full description and examples\n",
      " |      of this operation.\n",
      " |      \n",
      " |      :param sparkSession: SparkSession to use for creating the table\n",
      " |      :return: an instance of DeltaTableBuilder\n",
      " |      :rtype: :py:class:`~delta.tables.DeltaTableBuilder`\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  forName(sparkSession: pyspark.sql.session.SparkSession, tableOrViewName: str) -> 'DeltaTable' from builtins.type\n",
      " |      Create a DeltaTable using the given table or view name using the given SparkSession.\n",
      " |      \n",
      " |      :param sparkSession: SparkSession to use for loading the table\n",
      " |      :param tableOrViewName: name of the table or view\n",
      " |      :return: loaded Delta table\n",
      " |      :rtype: :py:class:`~delta.tables.DeltaTable`\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          deltaTable = DeltaTable.forName(spark, \"tblName\")\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |  \n",
      " |  forPath(sparkSession: pyspark.sql.session.SparkSession, path: str) -> 'DeltaTable' from builtins.type\n",
      " |      Create a DeltaTable for the data at the given `path` using the given SparkSession.\n",
      " |      \n",
      " |      :param sparkSession: SparkSession to use for loading the table\n",
      " |      :type sparkSession: pyspark.sql.SparkSession\n",
      " |      :return: loaded Delta table\n",
      " |      :rtype: :py:class:`~delta.tables.DeltaTable`\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          deltaTable = DeltaTable.forPath(spark, \"/path/to/table\")\n",
      " |      \n",
      " |      .. versionadded:: 0.4\n",
      " |  \n",
      " |  isDeltaTable(sparkSession: pyspark.sql.session.SparkSession, identifier: str) -> bool from builtins.type\n",
      " |      Check if the provided `identifier` string, in this case a file path,\n",
      " |      is the root of a Delta table using the given SparkSession.\n",
      " |      \n",
      " |      :param sparkSession: SparkSession to use to perform the check\n",
      " |      :param path: location of the table\n",
      " |      :return: If the table is a delta table or not\n",
      " |      :rtype: bool\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          DeltaTable.isDeltaTable(spark, \"/path/to/table\")\n",
      " |      \n",
      " |      .. versionadded:: 0.4\n",
      " |  \n",
      " |  replace(sparkSession: Optional[pyspark.sql.session.SparkSession] = None) -> 'DeltaTableBuilder' from builtins.type\n",
      " |      Return :class:`DeltaTableBuilder` object that can be used to specify\n",
      " |      the table name, location, columns, partitioning columns, table comment,\n",
      " |      and table properties to replace a Delta table,\n",
      " |      error if the table doesn't exist (the same as SQL `REPLACE TABLE`).\n",
      " |      \n",
      " |      See :class:`DeltaTableBuilder` for a full description and examples\n",
      " |      of this operation.\n",
      " |      \n",
      " |      :param sparkSession: SparkSession to use for creating the table\n",
      " |      :return: an instance of DeltaTableBuilder\n",
      " |      :rtype: :py:class:`~delta.tables.DeltaTableBuilder`\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6bb153ad-f332-4dde-a6c5-8b408bc15884",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>3</td><td>2023-01-01 11:59:22</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>2</td><td>2023-01-01 11:59:16</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>1</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 2, numOutputRows -&gt; 1, numOutputBytes -&gt; 3130}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>1</td><td>2023-01-01 11:59:00</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>0</td><td>2022-12-30 11:42:08</td><td>null</td><td>null</td><td>CREATE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|          timestamp|userId|userName|                        operation|                                                          operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      3|2023-01-01 11:59:22|  null|    null|                            WRITE|                                          {mode -> Append, partitionBy -> []}|null|    null|     null|          2|  Serializable|         true|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      2|2023-01-01 11:59:16|  null|    null|                            WRITE|                                          {mode -> Append, partitionBy -> []}|null|    null|     null|          1|  Serializable|         true|{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 3130}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      1|2023-01-01 11:59:00|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|    null|     null|          0|  Serializable|        false|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      0|2022-12-30 11:42:08|  null|    null|           CREATE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|    null|     null|       null|  Serializable|         true|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3eb43d63-65d5-41a4-955f-b47ea1961322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.vacuum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c8561f1f-8407-41af-9de3-fffdf6df3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToTable = \"./spark-warehouse/users\"\n",
    "df1 = spark.read.format(\"delta\").load(pathToTable)\n",
    "df1.write.format(\"delta\").mode(\"overwrite\").save(\"./delta-warehouse/users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "938a14ca-174d-45bd-8825-77b27a0871a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>3</td><td>2023-01-01 11:59:22</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>2</td><td>2023-01-01 11:59:16</td><td>null</td><td>null</td><td>WRITE</td><td>{mode -&gt; Append, partitionBy -&gt; []}</td><td>null</td><td>null</td><td>null</td><td>1</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 2, numOutputRows -&gt; 1, numOutputBytes -&gt; 3130}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>1</td><td>2023-01-01 11:59:00</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>0</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "<tr><td>0</td><td>2022-12-30 11:42:08</td><td>null</td><td>null</td><td>CREATE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>true</td><td>{numFiles -&gt; 3, numOutputRows -&gt; 2, numOutputBytes -&gt; 5392}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|          timestamp|userId|userName|                        operation|                                                          operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      3|2023-01-01 11:59:22|  null|    null|                            WRITE|                                          {mode -> Append, partitionBy -> []}|null|    null|     null|          2|  Serializable|         true|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      2|2023-01-01 11:59:16|  null|    null|                            WRITE|                                          {mode -> Append, partitionBy -> []}|null|    null|     null|          1|  Serializable|         true|{numFiles -> 2, numOutputRows -> 1, numOutputBytes -> 3130}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      1|2023-01-01 11:59:00|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|    null|     null|          0|  Serializable|        false|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "|      0|2022-12-30 11:42:08|  null|    null|           CREATE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [], properties -> {}}|null|    null|     null|       null|  Serializable|         true|{numFiles -> 3, numOutputRows -> 2, numOutputBytes -> 5392}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl1 = DeltaTable.forPath(spark, pathToTable)\n",
    "tbl1.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ea240628-b908-4be2-91c5-bccf8ee81439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl1.vacuum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5908f415-9c51-4381-b93f-736a156c31c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tbl1.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7014add3-60c0-46e1-8ada-724faf9660ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl1.vacuum(169)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "598d541a-7e95-4fec-809c-ecdc01326ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>key</th><th>value</th></tr>\n",
       "<tr><td>Type</td><td>MANAGED</td></tr>\n",
       "<tr><td>delta.minReaderVersion</td><td>1</td></tr>\n",
       "<tr><td>delta.minWriterVersion</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------+-------+\n",
       "|                   key|  value|\n",
       "+----------------------+-------+\n",
       "|                  Type|MANAGED|\n",
       "|delta.minReaderVersion|      1|\n",
       "|delta.minWriterVersion|      2|\n",
       "+----------------------+-------+"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"SHOW TBLPROPERTIES default.users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f5c6286f-b6c4-475a-8c61-3ebdf0410414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th></tr>\n",
       "<tr><td>delta</td><td>39c271df-b1eb-48d2-ad36-c039e6d04a17</td><td>default.users</td><td>null</td><td>file:/home/jovyan/work/spark-warehouse/users</td><td>2022-12-30 11:42:07.508</td><td>2023-01-01 11:59:22</td><td>[]</td><td>8</td><td>13914</td><td>{}</td><td>1</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+------------------------------------+-------------+-----------+--------------------------------------------+-----------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
       "|format|                                  id|         name|description|                                    location|              createdAt|       lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|\n",
       "+------+------------------------------------+-------------+-----------+--------------------------------------------+-----------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
       "| delta|39c271df-b1eb-48d2-ad36-c039e6d04a17|default.users|       null|file:/home/jovyan/work/spark-warehouse/users|2022-12-30 11:42:07.508|2023-01-01 11:59:22|              []|       8|      13914|        {}|               1|               2|\n",
       "+------+------------------------------------+-------------+-----------+--------------------------------------------+-----------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"describe detail users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0a8cdebf-69ea-4144-be32-5043281cc5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"ALTER TABLE default.users SET TBLPROPERTIES ('delta.logRetentionDuration' = 'interval 7 days', 'delta.deletedFileRetentionDuration' = 'interval 1 days')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2fd0e240-e64d-49e8-b1b5-eca764e021ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>key</th><th>value</th></tr>\n",
       "<tr><td>Type</td><td>MANAGED</td></tr>\n",
       "<tr><td>delta.deletedFileRetentionDuration</td><td>interval 1 days</td></tr>\n",
       "<tr><td>delta.logRetentionDuration</td><td>interval 7 days</td></tr>\n",
       "<tr><td>delta.minReaderVersion</td><td>1</td></tr>\n",
       "<tr><td>delta.minWriterVersion</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------------------+---------------+\n",
       "|                               key|          value|\n",
       "+----------------------------------+---------------+\n",
       "|                              Type|        MANAGED|\n",
       "|delta.deletedFileRetentionDuration|interval 1 days|\n",
       "|        delta.logRetentionDuration|interval 7 days|\n",
       "|            delta.minReaderVersion|              1|\n",
       "|            delta.minWriterVersion|              2|\n",
       "+----------------------------------+---------------+"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"show tblproperties users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2889bed1-d2e9-44f4-89ce-81ebb05d6baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>path</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+\n",
       "|path|\n",
       "+----+\n",
       "+----+"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"VACUUM users DRY RUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3732c0a7-c9d6-4ef6-b2aa-5bb6a9ebeaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>1</td><td>suhyuk</td><td>psyoblade</td><td>park</td><td>male</td><td>2000/10/30</td><td>741030</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>2</td><td>youngmi</td><td>kiki</td><td>kim</td><td>female</td><td>2004/08/08</td><td>770808</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>sowon</td><td>eva</td><td>park</td><td>female</td><td>2005/05/20</td><td>040520</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
       "+---+---------+----------+--------+------+----------+------+------+\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
       "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
       "+---+---------+----------+--------+------+----------+------+------+"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"select * from users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59fa68aa-77ad-40dd-8ac9-ca0d51ee67d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr>\n",
       "<tr><td>0</td><td>2023-01-09 08:58:34</td><td>null</td><td>null</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>{isManaged -&gt; true, description -&gt; null, partitionBy -&gt; [&quot;salary&quot;], properties -&gt; {}}</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Serializable</td><td>false</td><td>{numFiles -&gt; 4, numOutputRows -&gt; 4, numOutputBytes -&gt; 7980}</td><td>null</td><td>Apache-Spark/3.2.1 Delta-Lake/2.0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------------------+------+--------+---------------------------------+-------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|version|          timestamp|userId|userName|                        operation|                                                                  operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|                                           operationMetrics|userMetadata|                         engineInfo|\n",
       "+-------+-------------------+------+--------+---------------------------------+-------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+\n",
       "|      0|2023-01-09 08:58:34|  null|    null|CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"salary\"], properties -> {}}|null|    null|     null|       null|  Serializable|        false|{numFiles -> 4, numOutputRows -> 4, numOutputBytes -> 7980}|        null|Apache-Spark/3.2.1 Delta-Lake/2.0.0|\n",
       "+-------+-------------------+------+--------+---------------------------------+-------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------+------------+-----------------------------------+"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = DeltaTable.forPath(spark, \"./spark-warehouse/users\")\n",
    "users.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "261199ec-475d-4bce-bb26-e6230be2a312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
      "|  4|    sihun|      sean|    park|  male|2006/01/14|080114|  4000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = spark.read.format(\"delta\").load(\"./spark-warehouse/users\")\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a2960ec-042d-47ca-ada7-bf903758611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연봉을 기준으로 파티셔닝해서\n",
    "table.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"salary\").save(\"./spark-warehouse/users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b67a632b-db92-4b2e-b564-cd8079d75e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
      "|  4|    sihun|      sean|    park|  male|2006/01/14|080114|  4000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 특정 연봉의 이용자만 replaceWhere 구문으로 변경해보자\n",
    "poor = spark.read.format(\"delta\").load(\"./spark-warehouse/users\")\n",
    "pure = poor.distinct()\n",
    "pure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "210e9c0d-307d-4adb-9171-fd8b7bd4a339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실제 데이터프레임에 저장되어 있는 데이터와, 저장하려는 데이터의 범위가 맞지 않으면 예외를 던진다\n",
    "# pure.write.format(\"delta\").mode(\"overwrite\").option(\"replaceWhere\", \"salary <= 2000\").save(\"./delta-warehouse/users\")\n",
    "# 변경하고 싶은 범위와 파티션 범위를 동일하게 만들고 replaceWhere 적용을 해야 하는것으로 보인다\n",
    "criteria = pure.where(\"salary <= 2000\")\n",
    "criteria.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6699a1e-f3cf-48a6-b674-a2f6ed8a7b54",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Data written out does not match replaceWhere 'salary = 1000'.\nCHECK constraint EXPRESSION(('salary = 1000)) (salary = 1000) violated by row with values:\n - salary : 2000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcriteria\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplaceWhere\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msalary = 1000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./spark-warehouse/users\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Data written out does not match replaceWhere 'salary = 1000'.\nCHECK constraint EXPRESSION(('salary = 1000)) (salary = 1000) violated by row with values:\n - salary : 2000"
     ]
    }
   ],
   "source": [
    "criteria.write.format(\"delta\").mode(\"overwrite\").option(\"replaceWhere\", \"salary = 1000\").save(\"./spark-warehouse/users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3f1d8ee-6af5-4f21-bb5f-670e264d7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria.write.format(\"delta\").mode(\"append\").option(\"replaceWhere\", \"salary = 1000\").saveAsTable(\"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2ea46594-51d6-44e8-a692-e3ba74098d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.read.format(\"delta\").load(\"./delta-warehouse/users\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d1bf66d7-7a93-400d-8c98-c5779ec588d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  2|  youngmi|      kiki|     kim|female|2004/08/08|770808|  2000|\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
      "|  3|    sowon|       eva|    park|female|2005/05/20|040520|  3000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrong = spark.read.parquet(\"./delta-warehouse/users\")\n",
    "wrong.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b8275be2-969f-46fa-8302-7bedd6869957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "| id|firstName|middleName|lastName|gender| birthDate|   ssn|salary|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "|  1|   suhyuk| psyoblade|    park|  male|2000/10/30|741030|  1000|\n",
      "+---+---------+----------+--------+------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replaceWhere 조건이 왜 필요한가? 어차피 범위를 확인해서 필터해야 하는거라면?\n",
    "# 아니지 overwrite 이므로 조건을 지정하지 않으면 모든 데이터가 삭제되므로, replaceWhere 절은 반드시 필요하다\n",
    "# 결국 replaceWhere 절은 overwrite 시에 필요한 만큼만 overwrite 한다는 의미이지, 데이터의 검증까지 하는 것은 아니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbe42f92-70dc-4aeb-a016-64d48730d767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "|taxidb   |\n",
      "+---------+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|default  |family   |false      |\n",
      "|default  |users    |false      |\n",
      "+---------+---------+-----------+\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+---------+------------------+-----------+\n",
      "|namespace|tableName         |isTemporary|\n",
      "+---------+------------------+-----------+\n",
      "|taxidb   |greentaxis        |false      |\n",
      "|taxidb   |yellowtaxis       |false      |\n",
      "|taxidb   |yellowtaxis_append|false      |\n",
      "+---------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql(\"show databases; show tables ; use taxidb ; show tables \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d2a0cfb-2c9c-418e-aa04-da227c73ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- version: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- userName: string (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- operationParameters: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- job: struct (nullable = true)\n",
      " |    |-- jobId: string (nullable = true)\n",
      " |    |-- jobName: string (nullable = true)\n",
      " |    |-- runId: string (nullable = true)\n",
      " |    |-- jobOwnerId: string (nullable = true)\n",
      " |    |-- triggerType: string (nullable = true)\n",
      " |-- notebook: struct (nullable = true)\n",
      " |    |-- notebookId: string (nullable = true)\n",
      " |-- clusterId: string (nullable = true)\n",
      " |-- readVersion: long (nullable = true)\n",
      " |-- isolationLevel: string (nullable = true)\n",
      " |-- isBlindAppend: boolean (nullable = true)\n",
      " |-- operationMetrics: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- userMetadata: string (nullable = true)\n",
      " |-- engineInfo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = sql(\"describe history yellowtaxis\")\n",
    "history.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8328c47c-9763-463d-b945-7269b59e60ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                      |comment|\n",
      "+----------------------------+-------------------------------------------------------------------------------+-------+\n",
      "|RideId                      |int                                                                            |       |\n",
      "|VendorId                    |int                                                                            |       |\n",
      "|PickupTime                  |timestamp                                                                      |       |\n",
      "|DropTime                    |timestamp                                                                      |       |\n",
      "|PickupLocationId            |int                                                                            |       |\n",
      "|DropLocationId              |int                                                                            |       |\n",
      "|CabNumber                   |string                                                                         |       |\n",
      "|DriverLicenseNumber         |string                                                                         |       |\n",
      "|PassengerCount              |int                                                                            |       |\n",
      "|TripDistance                |double                                                                         |       |\n",
      "|RatecodeId                  |int                                                                            |       |\n",
      "|PaymentType                 |int                                                                            |       |\n",
      "|TotalAmount                 |double                                                                         |       |\n",
      "|FareAmount                  |double                                                                         |       |\n",
      "|Extra                       |double                                                                         |       |\n",
      "|MtaTax                      |double                                                                         |       |\n",
      "|TipAmount                   |double                                                                         |       |\n",
      "|TollsAmount                 |double                                                                         |       |\n",
      "|ImprovementSurcharge        |double                                                                         |       |\n",
      "|                            |                                                                               |       |\n",
      "|# Partitioning              |                                                                               |       |\n",
      "|Not partitioned             |                                                                               |       |\n",
      "|                            |                                                                               |       |\n",
      "|# Detailed Table Information|                                                                               |       |\n",
      "|Name                        |taxidb.yellowtaxis                                                             |       |\n",
      "|Location                    |file:/home/jovyan/work/data/yellowTaxis.delta                                  |       |\n",
      "|Provider                    |delta                                                                          |       |\n",
      "|Owner                       |root                                                                           |       |\n",
      "|Table Properties            |[Type=EXTERNAL,delta.minReaderVersion=1,delta.minWriterVersion=2,external=true]|       |\n",
      "+----------------------------+-------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show(\"describe extended yellowtaxis\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e636ab8-5990-48fe-9831-19e2648efe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxrwxrwx 1 jovyan 1000  512 Aug 28 12:37 .\n",
      "drwxrwxrwx 1 jovyan 1000  512 Aug 28 12:37 ..\n",
      "-rwxrwxrwx 1 jovyan 1000 2177 Aug 14 10:52 00000000000000000000.json\n",
      "-rwxrwxrwx 1 jovyan 1000   28 Aug 14 10:52 .00000000000000000000.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 1861 Aug 14 10:52 00000000000000000001.json\n",
      "-rwxrwxrwx 1 jovyan 1000   24 Aug 14 10:52 .00000000000000000001.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 1861 Aug 28 06:03 00000000000000000002.json\n",
      "-rwxrwxrwx 1 jovyan 1000   24 Aug 28 06:03 .00000000000000000002.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 3354 Aug 28 06:08 00000000000000000003.json\n",
      "-rwxrwxrwx 1 jovyan 1000   36 Aug 28 06:08 .00000000000000000003.json.crc\n",
      "-rwxrwxrwx 1 jovyan 1000 4133 Aug 28 12:37 00000000000000000004.json\n",
      "-rwxrwxrwx 1 jovyan 1000   44 Aug 28 12:37 .00000000000000000004.json.crc\n",
      "drwxrwxrwx 1 jovyan 1000  512 Aug 28 04:41 .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "ls(\"/home/jovyan/work/data/yellowTaxis.delta/_delta_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87cb0d7c-fcd4-4e04-844f-69f50752660c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"add\": {\n",
      "        \"path\": \"part-00000-a2bcac59-0d4f-40f8-bcdb-57e66bbb00f2-c000.snappy.parquet\",\n",
      "        \"partitionValues\": {},\n",
      "        \"size\": 5252,\n",
      "        \"modificationTime\": 1724848669406,\n",
      "        \"dataChange\": true,\n",
      "        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"RideId\\\":9999995,\\\"VendorId\\\":1,\\\"PickupTime\\\":\\\"2019-11-01T09:00:00.000+09:00\\\",\\\"DropTime\\\":\\\"2019-11-01T09:02:23.573+09:00\\\",\\\"PickupLocationId\\\":65,\\\"DropLocationId\\\":71,\\\"CabNumber\\\":\\\"TAC304\\\",\\\"DriverLicenseNumber\\\":\\\"453987\\\",\\\"PassengerCount\\\":5,\\\"TripDistance\\\":4.5,\\\"RatecodeId\\\":1,\\\"PaymentType\\\":1,\\\"TotalAmount\\\":20.34,\\\"FareAmount\\\":15.0,\\\"Extra\\\":0.5,\\\"MtaTax\\\":0.4,\\\"TipAmount\\\":2.0,\\\"TollsAmount\\\":2.0,\\\"ImprovementSurcharge\\\":1.1},\\\"maxValues\\\":{\\\"RideId\\\":9999995,\\\"VendorId\\\":1,\\\"PickupTime\\\":\\\"2019-11-01T09:00:00.000+09:00\\\",\\\"DropTime\\\":\\\"2019-11-01T09:02:23.573+09:00\\\",\\\"PickupLocationId\\\":65,\\\"DropLocationId\\\":71,\\\"CabNumber\\\":\\\"TAC304\\\",\\\"DriverLicenseNumber\\\":\\\"453987\\\",\\\"PassengerCount\\\":5,\\\"TripDistance\\\":4.5,\\\"RatecodeId\\\":1,\\\"PaymentType\\\":1,\\\"TotalAmount\\\":20.34,\\\"FareAmount\\\":15.0,\\\"Extra\\\":0.5,\\\"MtaTax\\\":0.4,\\\"TipAmount\\\":2.0,\\\"TollsAmount\\\":2.0,\\\"ImprovementSurcharge\\\":1.1},\\\"nullCount\\\":{\\\"RideId\\\":0,\\\"VendorId\\\":0,\\\"PickupTime\\\":0,\\\"DropTime\\\":0,\\\"PickupLocationId\\\":0,\\\"DropLocationId\\\":0,\\\"CabNumber\\\":0,\\\"DriverLicenseNumber\\\":0,\\\"PassengerCount\\\":0,\\\"TripDistance\\\":0,\\\"RatecodeId\\\":0,\\\"PaymentType\\\":0,\\\"TotalAmount\\\":0,\\\"FareAmount\\\":0,\\\"Extra\\\":0,\\\"MtaTax\\\":0,\\\"TipAmount\\\":0,\\\"TollsAmount\\\":0,\\\"ImprovementSurcharge\\\":0}}\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "grep_sed_json(\"add\", 1, \"/home/jovyan/work/data/yellowTaxis.delta/_delta_log/00000000000000000004.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db0eb3a1-05fc-4bb9-8c5d-bd5295a47e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorId|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|2       |2019-12-05 07:43:41 |2019-12-05 07:50:30  |N                 |1         |97          |65          |1              |0.87         |6.0        |0.0  |0.5    |1.36      |0.0         |null     |0.3                  |10.11       |1           |1        |0.0                 |\n",
      "|2       |2019-12-05 07:02:27 |2019-12-05 07:10:21  |N                 |1         |74          |75          |2              |1.21         |7.0        |0.0  |0.5    |1.95      |0.0         |null     |0.3                  |9.75        |1           |1        |0.0                 |\n",
      "|2       |2019-12-05 07:15:52 |2019-12-05 07:43:32  |N                 |1         |74          |162         |2              |4.8          |20.0       |0.0  |0.5    |5.89      |0.0         |null     |0.3                  |29.44       |1           |1        |2.75                |\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"taxidb.greentaxis\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b9b6a-4169-408d-bab4-080c46fd277d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
