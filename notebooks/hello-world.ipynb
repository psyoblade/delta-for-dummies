{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9879ddf1-1276-49d0-89b8-eec2b8df8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-11-openjdk-amd64\n",
      "/usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986ad10a-b2b6-4543-812e-b0f09f85b72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "from delta import *\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "warehouse_dir = f\"{work_dir}/spark-warehouse\"\n",
    "\n",
    "# Create spark session with hive enabled\n",
    "builder = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"pyspark-notebook\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_dir)\n",
    "    .enableHiveSupport()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5b931d-1ee5-4394-960f-33f131d4d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 델타 레이크 생성시에 반드시 `configure_spark_with_delta_pip` 구성을 통해 실행되어야 정상적인 델타 의존성이 로딩됩니다\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39abe029-89d4-473d-9f8c-e44146492879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c7524b0ae2cc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3a1f622d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark.conf.set(\"spark.sql.decimalOperations.allowPrecisionLoss\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57b61f7-1ee2-4a9a-bb31-3663bca1953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(queries, num_rows = 20):\n",
    "    for query in queries.split(\";\"):\n",
    "        spark.sql(query).show(num_rows, truncate=False)\n",
    "\n",
    "def sql(query):\n",
    "    return spark.sql(query)\n",
    "\n",
    "def ls(command):\n",
    "    !ls -al {command}\n",
    "\n",
    "def cat(filename):\n",
    "    !cat {filename}\n",
    "\n",
    "def grep(keyword, filename):\n",
    "    !grep -i {keyword} {filename}\n",
    "\n",
    "def grep_and_json(keyword, filename):\n",
    "    !grep {keyword} {filename} | python -m json.tool\n",
    "\n",
    "def grep_sed_json(keyword, lineno, filename):\n",
    "    !grep {keyword} {filename} | sed -n {lineno}p | python -m json.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "320ec102-2df5-48a2-9be5-581dd11cddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "def transform(spark, params):\n",
    "    base_date = params[\"base_date\"]\n",
    "    p_date = datetime.strptime(base_date, '%Y%m%d')\n",
    "    bds_db = params[\"bds_db\"]\n",
    "    interim_db = params[\"interim_db\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e80c3c7f-875b-444e-a7a5-d64db78bcebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/sample\").createOrReplaceTempView(\"bs_gamelog\")\n",
    "# spark.sql(\"select * from bs_gamelog\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e28e9b69-edc2-4936-9131-2a6952722f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(f\"\"\"\n",
    "with base as ( \n",
    "    select \n",
    "        concat(target_object_id, \"_\", target_name) as monster_id\n",
    "        , sort_array(collect_list(concat(actor_id, \"_\", actor_group))) as player_group\n",
    "    from ( \n",
    "        select actor_group, target_object_id, target_name, actor_id\n",
    "        from bs_gamelog\n",
    "        group by actor_group, target_object_id, target_name, actor_id \n",
    "    ) a\n",
    "    group by concat(target_object_id, \"_\", target_name)\n",
    "    having size(sort_array(collect_list(concat(actor_id, \"_\", actor_group)))) between 0 and 100\n",
    ")\n",
    "\n",
    "select player_group, count(*) as hunt_count\n",
    "from base\n",
    "group by player_group\n",
    "\"\"\")\n",
    "# where plogdate = '{p_date}' and logid = 1208 and actor_group in (4501, 4502, ... 3504) and target_code = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fef70ac2-a72d-476c-bbb6-8204ec7cd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "def generate_combinations(group, min_size=4):\n",
    "    for size in range(min_size, len(group)+1):\n",
    "        for comb in itertools.combinations(group, size):\n",
    "            yield comb\n",
    "\n",
    "def count_combinations(player_group, hunt_count):\n",
    "    frequency = defaultdict(int)\n",
    "    for comb in generate_combinations(player_group):\n",
    "        comb_str = \",\".join(map(str, sorted(comb)))\n",
    "        frequency[comb_str] += hunt_count\n",
    "        return dict(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2be36a30-f17c-47c8-8a18-a698b7f4d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, StringType, IntegerType\n",
    "from pyspark.sql.functions import date_format, concat_ws, collect_list, size, explode, split, count\n",
    "\n",
    "@udf(MapType(StringType(), IntegerType()))\n",
    "def calculate_combinations(player_group, hunt_count):\n",
    "    return count_combinations(player_group, hunt_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a5c71ec5-b9ab-404d-90c8-928ee469ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_combinations = df.withColumn(\"combinations\", calculate_combinations(col(\"player_group\"), col(\"hunt_count\")))\n",
    "df_exploded = df_with_combinations.select(explode(col(\"combinations\")).alias(\"combination\", \"count\"))\n",
    "df_filtered = df_exploded.groupBy(\"combination\").agg({\"count\":\"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "587a7749-09d3-4132-8977-71eca4efd242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+\n",
      "|        player_group|hunt_count|combinations|\n",
      "+--------------------+----------+------------+\n",
      "|[actor_1_group_a,...|         1|        NULL|\n",
      "|[actor_2_group_a,...|         1|        NULL|\n",
      "|[actor_1_group_c,...|         1|        NULL|\n",
      "|[actor_3_group_b,...|         1|        NULL|\n",
      "|[actor_4_group_b,...|         1|        NULL|\n",
      "+--------------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"combinations\", calculate_combinations(col(\"player_group\"), col(\"hunt_count\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eaa7c4d8-22fd-47fd-84ed-64c2a233a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.withColumnRenamed(\"sum(count)\", \"total_hunts\")\n",
    "df_filtered = df_filtered.filter(col(\"total_hunts\") >= 1)\n",
    "df_players = df_filtered.withColumn(\"player\", explode(split(col(\"combination\"), \",\")))\n",
    "df_players = df_players.withColumn(\"char_id\", split(col(\"player\"), \"_\")[0]).withColumn(\"server_cd\", split(col(\"player\"), \"_\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e567b043-90d3-4067-8bd8-f138ca14a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_hunts = df_players.groupBy(\"char_id\", \"server_cd\").agg(max(\"total_hunts\").alias(\"max_hunt_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d66ac54-60d7-41ec-bed8-8304d71985e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|char_id|server_cd|\n",
      "+-------+---------+\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_max_hunts.select(\"char_id\", \"server_cd\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac54613-4f9c-4a6f-a5ec-a458e5bcfce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "having size (sort_array(collect_list(concat(actor_id, \"_\", actor_group))))\n",
    "between 4 and 20 -- 실패 (OOM)\n",
    "between 4 and 12 -- 성공"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
